{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import scale\n",
    "from nltk.corpus import wordnet\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiprocessing\n",
    "from keras.models import Sequential, Model \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install --yes --prefix {sys.prefix} gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from file\n",
    "def read_file(encode):\n",
    "    with open(r\"ra_data_classifier.csv\", encoding = encode, errors='ignore', newline='') as f:\n",
    "        df = pd.read_csv(f)\n",
    "        df = df.drop(columns=['hid'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    pre-processing on the given text.\n",
    "    \n",
    "    Steps:\n",
    "    - Removing punctuation\n",
    "    - Lowering text\n",
    "    \"\"\"\n",
    "    # remove leading and trailing whitespaces, newline and tab characters\n",
    "    # convert text to lowercase\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    # remove the chracters [\\], ['], [+], [\"]\n",
    "    text = re.sub(r\"\\+\", \"\", text)\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "    \n",
    "    # replace email address with \"emailadd\"\n",
    "    text = re.sub('\\S+@\\S+', \"emailadd \", text)\n",
    "    \n",
    "    # replace \"$numeric / month\", \"$numeric/month\", \"$numeric/m\", \"$numeric / m\" to rentfee \n",
    "    p = re.compile(r\"(?:\\$\\d+\\/)month|(?:\\$\\d+\\s+\\/\\s+)month|(?:\\$\\d+\\s+\\/\\s+)m|(?:\\$\\d+\\/)m|(?:\\S\\d+)|(?:\\$\\d+\\/)mo|(?:\\$\\d+\\s+\\/\\s+)mo\")\n",
    "    text = p.sub(\"rentfee \", text)\n",
    "                 \n",
    "    # replace phone number with \"phonenumber\"\n",
    "    text = re.sub(r\"\\(?\\d{3}\\)?[-.\\s]\\d{3}[-.\\s]\\d{4}\", \"phonenumber\", text)\n",
    "\n",
    "    # remove non-alphabet characters\n",
    "    text = re.sub('[^a-z]+', ' ', text)\n",
    "    \n",
    "    return text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize with POS Tag\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the sentence and lemmatize the words\n",
    "def tokenization(df):\n",
    "    # grab all cleaned_chunk\n",
    "    tokens = []\n",
    "    df['token'] = df['clean_chunk']\n",
    "    \n",
    "    # tokenize the string into words\n",
    "    for c in df['clean_chunk']:\n",
    "        token = word_tokenize(c)\n",
    "        tokens.append(token)\n",
    "        \n",
    "    # remove none-alphabetic tokens, such as punctuation\n",
    "    for token in tokens:\n",
    "        token = [word for word in token if word.isalpha()]\n",
    "    \n",
    "    # filter out stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    for token in tokens:\n",
    "        token = [word for word in token if not word in stop_words]\n",
    "        \n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(tokens)):\n",
    "        temp = []\n",
    "        for j in range(len(tokens[i])):\n",
    "            tokens[i][j] = lemmatizer.lemmatize(tokens[i][j], get_wordnet_pos(tokens[i][j]))\n",
    "            \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORD2VEC_CBOW()\n",
    "def word2vec(tokened_data, cbow):\n",
    "    w2v_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "                                                                # \n",
    "    cores = multiprocessing.cpu_count() # Count the number of cores in a computer, important for a parameter of the model\n",
    "    if cbow == 1: # use CBOW\n",
    "        w2v_model = Word2Vec(min_count=20,\n",
    "                             window=2,\n",
    "                             size=300,\n",
    "                             sample=6e-5, \n",
    "                             alpha=0.03, \n",
    "                             min_alpha=0.0007, \n",
    "                             negative=20,\n",
    "                             workers=cores-1)\n",
    "    else: # use skipgram\n",
    "        w2v_model = Word2Vec(min_count=20,\n",
    "                             window=2,\n",
    "                             size=300,\n",
    "                             sample=6e-5, \n",
    "                             alpha=0.03, \n",
    "                             min_alpha=0.0007, \n",
    "                             negative=20,\n",
    "                             workers=cores-1,\n",
    "                             sg=1)\n",
    "    # BUILD_VOCAB()\n",
    "    w2v_model.build_vocab(tokened_data, progress_per=1000)\n",
    "\n",
    "    # TRAIN()\n",
    "    w2v_model.train(tokened_data, total_examples=w2v_model.corpus_count, epochs=10000, report_delay=1)\n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_cloud(terms):\n",
    "    text = terms.index\n",
    "    text = ' '.join(list(text))\n",
    "    # lower max_font_size\n",
    "    wordcloud = WordCloud(max_font_size=40).generate(text)\n",
    "    plt.figure(figsize=(25, 25))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size, w2v_model, tfidf):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += w2v_model[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Input, Embedding\n",
    "def get_model(trainX, trainY):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128, activation='relu', input_dim=300))\n",
    "    model.add(Dropout(0.7)) # to prevent overfitting\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adadelta',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    model.fit(trainX, trainY, epochs=30, batch_size=50, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>has_space</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Landmark Center, 8th Fl</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contact: The C3 team at MakemeC3@cic.us -- Add...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A powerful tool for developers, the MySQL Data...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Easy access to T, Hubway, and parking</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Check out our Private Offices</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk  has_space\n",
       "0                            Landmark Center, 8th Fl          0\n",
       "1  Contact: The C3 team at MakemeC3@cic.us -- Add...          0\n",
       "2  A powerful tool for developers, the MySQL Data...          0\n",
       "3              Easy access to T, Hubway, and parking          0\n",
       "4                      Check out our Private Offices          1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ENCODING = \"latin-1\"\n",
    "# ENCODING = \"ISO-8859-1\"\n",
    "ENCODING = \"utf-8\"\n",
    "\n",
    "#read file\n",
    "data = read_file(ENCODING)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seongohryoo/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>has_space</th>\n",
       "      <th>clean_chunk</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Landmark Center, 8th Fl</td>\n",
       "      <td>0</td>\n",
       "      <td>landmark center th fl</td>\n",
       "      <td>[landmark, center, th, fl]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contact: The C3 team at MakemeC3@cic.us -- Add...</td>\n",
       "      <td>0</td>\n",
       "      <td>contact the rentfee team at emailadd additiona...</td>\n",
       "      <td>[contact, the, rentfee, team, at, emailadd, ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A powerful tool for developers, the MySQL Data...</td>\n",
       "      <td>0</td>\n",
       "      <td>a powerful tool for developers the mysql datab...</td>\n",
       "      <td>[a, powerful, tool, for, developer, the, mysql...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Easy access to T, Hubway, and parking</td>\n",
       "      <td>0</td>\n",
       "      <td>easy access to t hubway and parking</td>\n",
       "      <td>[easy, access, to, t, hubway, and, parking]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Check out our Private Offices</td>\n",
       "      <td>1</td>\n",
       "      <td>check out our private offices</td>\n",
       "      <td>[check, out, our, private, office]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\r\\r\\r\\r          \\r          By Michael Carne...</td>\n",
       "      <td>0</td>\n",
       "      <td>by michael carney written on june rentfee rent...</td>\n",
       "      <td>[by, michael, carney, write, on, june, rentfee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\r\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tWorkbar۪s coworking spac...</td>\n",
       "      <td>1</td>\n",
       "      <td>workbar s coworking spaces provide the right b...</td>\n",
       "      <td>[workbar, s, coworking, space, provide, the, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>۝We went from 3,000 sq ft to 13,000 sq ft. Tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>we went from rentfee sq ft to rentfee rentfee...</td>\n",
       "      <td>[we, go, from, rentfee, sq, ft, to, rentfee, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Common space / kitchen, available for use day ...</td>\n",
       "      <td>0</td>\n",
       "      <td>common space kitchen available for use day and...</td>\n",
       "      <td>[common, space, kitchen, available, for, use, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Workbar Union  $350 / month full-time open wor...</td>\n",
       "      <td>1</td>\n",
       "      <td>workbar union rentfee full time open workspace...</td>\n",
       "      <td>[workbar, union, rentfee, full, time, open, wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk  has_space  \\\n",
       "0                            Landmark Center, 8th Fl          0   \n",
       "1  Contact: The C3 team at MakemeC3@cic.us -- Add...          0   \n",
       "2  A powerful tool for developers, the MySQL Data...          0   \n",
       "3              Easy access to T, Hubway, and parking          0   \n",
       "4                      Check out our Private Offices          1   \n",
       "5  \\r\\r\\r\\r          \\r          By Michael Carne...          0   \n",
       "6  \\r\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tWorkbar۪s coworking spac...          1   \n",
       "7  ۝We went from 3,000 sq ft to 13,000 sq ft. Tha...          0   \n",
       "8  Common space / kitchen, available for use day ...          0   \n",
       "9  Workbar Union  $350 / month full-time open wor...          1   \n",
       "\n",
       "                                         clean_chunk  \\\n",
       "0                              landmark center th fl   \n",
       "1  contact the rentfee team at emailadd additiona...   \n",
       "2  a powerful tool for developers the mysql datab...   \n",
       "3                easy access to t hubway and parking   \n",
       "4                      check out our private offices   \n",
       "5  by michael carney written on june rentfee rent...   \n",
       "6  workbar s coworking spaces provide the right b...   \n",
       "7   we went from rentfee sq ft to rentfee rentfee...   \n",
       "8  common space kitchen available for use day and...   \n",
       "9  workbar union rentfee full time open workspace...   \n",
       "\n",
       "                                               token  \n",
       "0                         [landmark, center, th, fl]  \n",
       "1  [contact, the, rentfee, team, at, emailadd, ad...  \n",
       "2  [a, powerful, tool, for, developer, the, mysql...  \n",
       "3        [easy, access, to, t, hubway, and, parking]  \n",
       "4                 [check, out, our, private, office]  \n",
       "5  [by, michael, carney, write, on, june, rentfee...  \n",
       "6  [workbar, s, coworking, space, provide, the, r...  \n",
       "7  [we, go, from, rentfee, sq, ft, to, rentfee, r...  \n",
       "8  [common, space, kitchen, available, for, use, ...  \n",
       "9  [workbar, union, rentfee, full, time, open, wo...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data cleaning\n",
    "cleaned_chunk = []\n",
    "num_row = data.shape[0]\n",
    "for i in range(num_row):\n",
    "    cleaned_chunk.append(clean_text(data['chunk'][i]))\n",
    "\n",
    "# add clean_chunk column to dataframe        \n",
    "data['clean_chunk'] = pd.DataFrame(cleaned_chunk)\n",
    "\n",
    "# tokenize the cleaned chunk\n",
    "tokenized_chunk = tokenization(data)\n",
    "\n",
    "# add tokenized chunk column to the data\n",
    "for c in range(len(tokenized_chunk)):\n",
    "    data['token'][c] = tokenized_chunk[c]\n",
    "data[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the x (input), and the y (output)\n",
    "y = data['has_space'].values\n",
    "x = np.array(data[\"token\"])\n",
    "\n",
    "# split train(80%), test set(20%)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 29\n"
     ]
    }
   ],
   "source": [
    "# vectorize the data\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x for x in x_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print ('vocab size :', len(tfidf))\n",
    "# plot word cloud\n",
    "tfidf2 = pd.DataFrame(columns=['tfidf']).from_dict(dict(tfidf), orient='index')\n",
    "tfidf2.columns = ['tfidf']\n",
    "# print(\"The most frequent words in the file are below.\\n\")\n",
    "# plot_word_cloud(tfidf2.sort_values(by=['tfidf'], ascending=True).head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec model with CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word2vec model\n",
    "w2v_model_cbow = word2vec(data['token'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape for training set :  (80, 300) \n",
      "shape for test set :  (20, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seongohryoo/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/Users/seongohryoo/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# generate word2vec train and test data\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, 300, w2v_model_cbow, tfidf) for z in map(lambda x: x, x_train)])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, 300, w2v_model_cbow, tfidf) for z in map(lambda x: x, x_test)])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "print ('shape for training set : ',train_vecs_w2v.shape,\n",
    "      '\\nshape for test set : ', test_vecs_w2v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               38528     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 38,657\n",
      "Trainable params: 38,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "model = get_model(train_vecs_w2v, y_train)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for test set\n",
    "test_probs = model.predict(train_vecs_w2v, verbose=0)\n",
    "test_classes = model.predict_classes(test_vecs_w2v, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce to 1d array\n",
    "test_probs = test_probs[:,0]\n",
    "test_classes = test_classes[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.850000\n",
      "Precision: 0.750000\n",
      "Recall: 0.857143\n",
      "F1 score: 0.800000\n",
      "confusion matrix: \n",
      " [[11  2]\n",
      " [ 1  6]]\n",
      "true negative :  11 \n",
      "false positive :  2 \n",
      "false negative :  1 \n",
      "true positive :  6\n"
     ]
    }
   ],
   "source": [
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, test_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision: tp / (tp + fp)\n",
    "precision = precision_score(y_test, test_classes)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, test_classes)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 * (precision * recall) / (precision + recall)\n",
    "f1 = f1_score(y_test, test_classes)\n",
    "print('F1 score: %f' % f1)\n",
    "# confuson matrix\n",
    "matrix = confusion_matrix(y_test, test_classes)\n",
    "tn, fp, fn, tp = matrix.ravel()\n",
    "print('confusion matrix: \\n', matrix)\n",
    "print(\"true negative : \", tn, \"\\nfalse positive : \", fp ,\"\\nfalse negative : \",  fn, \"\\ntrue positive : \", tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec model with skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word2vec model\n",
    "w2v_model_sg = word2vec(data['token'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seongohryoo/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/Users/seongohryoo/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape for training set :  (80, 300) \n",
      "shape for test set :  (20, 300)\n"
     ]
    }
   ],
   "source": [
    "# generate word2vec train and test data\n",
    "train_vecs_w2v_sg = np.concatenate([buildWordVector(z, 300, w2v_model_sg, tfidf) for z in map(lambda x: x, x_train)])\n",
    "train_vecs_w2v_sg = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v_sg = np.concatenate([buildWordVector(z, 300, w2v_model_sg, tfidf) for z in map(lambda x: x, x_test)])\n",
    "test_vecs_w2v_sg = scale(test_vecs_w2v)\n",
    "\n",
    "print ('shape for training set : ',train_vecs_w2v_sg.shape,\n",
    "      '\\nshape for test set : ', test_vecs_w2v_sg.shape)\n",
    "\n",
    "# fit model\n",
    "model_sg = get_model(train_vecs_w2v_sg, y_train)\n",
    "\n",
    "# predict probabilities for test set\n",
    "test_probs_sg = model_sg.predict(train_vecs_w2v_sg, verbose=0)\n",
    "test_classes_sg = model_sg.predict_classes(test_vecs_w2v_sg, verbose=0)\n",
    "\n",
    "# reduce to 1d array\n",
    "test_probs_sg = test_probs_sg[:,0]\n",
    "test_classes_sg = test_classes_sg[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.850000\n",
      "Precision: 0.750000\n",
      "Recall: 0.857143\n",
      "F1 score: 0.800000\n",
      "confusion matrix: \n",
      " [[11  2]\n",
      " [ 1  6]]\n",
      "true negative :  11 \n",
      "false positive :  2 \n",
      "false negative :  1 \n",
      "true positive :  6\n"
     ]
    }
   ],
   "source": [
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy_sg = accuracy_score(y_test, test_classes_sg)\n",
    "print('Accuracy: %f' % accuracy_sg)\n",
    "# precision: tp / (tp + fp)\n",
    "precision_sg = precision_score(y_test, test_classes_sg)\n",
    "print('Precision: %f' % precision_sg)\n",
    "# recall: tp / (tp + fn)\n",
    "recall_sg = recall_score(y_test, test_classes_sg)\n",
    "print('Recall: %f' % recall_sg)\n",
    "# f1: 2 * (precision * recall) / (precision + recall)\n",
    "f1_sg = f1_score(y_test, test_classes_sg)\n",
    "print('F1 score: %f' % f1_sg)\n",
    "# confuson matrix\n",
    "matrix_sg = confusion_matrix(y_test, test_classes_sg)\n",
    "tn_sg, fp_sg, fn_sg, tp_sg = matrix_sg.ravel()\n",
    "print('confusion matrix: \\n', matrix_sg)\n",
    "print(\"true negative : \", tn_sg, \"\\nfalse positive : \", fp_sg ,\"\\nfalse negative : \",  fn_sg, \"\\ntrue positive : \", tp_sg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare the CBOW model and Skipgram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_result = pd.DataFrame({'CBOW': [accuracy, f1, precision, recall], 'Skipgram': [accuracy_sg, f1_sg, precision_sg, recall_sg]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CBOW</th>\n",
       "      <th>Skipgram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accurcy</th>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 score</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               CBOW  Skipgram\n",
       "accurcy    0.850000  0.850000\n",
       "F1 score   0.800000  0.800000\n",
       "precision  0.750000  0.750000\n",
       "recall     0.857143  0.857143"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_result.rename(index={0:'accurcy',1:'F1 score', 2:'precision', 3:'recall'}, inplace=True)\n",
    "compare_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized chunk</th>\n",
       "      <th>has_space</th>\n",
       "      <th>predicted has_space_CBOW</th>\n",
       "      <th>predicted has_space_skipgram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[prestigious, kendall, square, business, addre...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[let, s, grow, our, business, together, a, a, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[a, powerful, tool, for, developer, the, mysql...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[on, site, gym, come, soon]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[workshop, brookline, rentfee, full, time, mem...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[if, you, would, like, to, host, an, event, in...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[learnlaunch, campus, unlisted, pricing, for, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[the, majority, of, our, host, location, be, i...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[locate, in, a, historic, brick, and, beam, bu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[cambridge, coworking, community, rentfee]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[your, workbar, membership, be, a, flexible, a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[then, there, wework, the, co, work, juggernau...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[special, pricing, on, multi, month, menbershi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[photo, credit, ed, wonsek]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[we, go, from, rentfee, sq, ft, to, rentfee, r...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[unlimited, access, most, flexible]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[coworking, be, a, concept, create, to, satisf...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[cove, rentfee, night, weekend, access, rentfe...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[article, by, evona, w, niewiadomska]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[common, space, kitchen, available, for, use, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      tokenized chunk  has_space  \\\n",
       "0   [prestigious, kendall, square, business, addre...          0   \n",
       "1   [let, s, grow, our, business, together, a, a, ...          0   \n",
       "2   [a, powerful, tool, for, developer, the, mysql...          0   \n",
       "3                         [on, site, gym, come, soon]          0   \n",
       "4   [workshop, brookline, rentfee, full, time, mem...          1   \n",
       "5   [if, you, would, like, to, host, an, event, in...          0   \n",
       "6   [learnlaunch, campus, unlisted, pricing, for, ...          1   \n",
       "7   [the, majority, of, our, host, location, be, i...          1   \n",
       "8   [locate, in, a, historic, brick, and, beam, bu...          1   \n",
       "9          [cambridge, coworking, community, rentfee]          0   \n",
       "10  [your, workbar, membership, be, a, flexible, a...          1   \n",
       "11  [then, there, wework, the, co, work, juggernau...          1   \n",
       "12  [special, pricing, on, multi, month, menbershi...          0   \n",
       "13                        [photo, credit, ed, wonsek]          0   \n",
       "14  [we, go, from, rentfee, sq, ft, to, rentfee, r...          0   \n",
       "15                [unlimited, access, most, flexible]          0   \n",
       "16  [coworking, be, a, concept, create, to, satisf...          0   \n",
       "17  [cove, rentfee, night, weekend, access, rentfe...          1   \n",
       "18              [article, by, evona, w, niewiadomska]          0   \n",
       "19  [common, space, kitchen, available, for, use, ...          0   \n",
       "\n",
       "    predicted has_space_CBOW  predicted has_space_skipgram  \n",
       "0                          0                             0  \n",
       "1                          0                             0  \n",
       "2                          0                             0  \n",
       "3                          0                             0  \n",
       "4                          1                             1  \n",
       "5                          1                             1  \n",
       "6                          1                             1  \n",
       "7                          1                             1  \n",
       "8                          1                             1  \n",
       "9                          0                             0  \n",
       "10                         0                             0  \n",
       "11                         1                             1  \n",
       "12                         0                             0  \n",
       "13                         0                             0  \n",
       "14                         0                             0  \n",
       "15                         0                             0  \n",
       "16                         0                             0  \n",
       "17                         1                             1  \n",
       "18                         0                             0  \n",
       "19                         1                             1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dict = {'tokenized chunk': x_test, 'has_space': y_test, 'predicted has_space_CBOW': test_classes, 'predicted has_space_skipgram' : test_classes_sg}\n",
    "validation_data = pd.DataFrame(validation_dict)\n",
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
