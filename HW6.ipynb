{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, KFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Supervised, Semi-Supervised, and Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Download the Breast Cancer Wisconsin Data Set. Use the first 20% of the positive and negative classes in the file as the test set and the rest as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'https://raw.githubusercontent.com/seongohr/ML/master/wdbc.data'\n",
    "df = pd.read_csv(filepath, sep=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[0])\n",
    "df = df.replace({'B': 1, 'M': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17.990</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.71190</td>\n",
       "      <td>0.26540</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20.570</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.070170</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.24160</td>\n",
       "      <td>0.18600</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>19.690</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.45040</td>\n",
       "      <td>0.24300</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>11.420</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.68690</td>\n",
       "      <td>0.25750</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20.290</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>0.16250</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>12.450</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.080890</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>...</td>\n",
       "      <td>15.470</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>0.52490</td>\n",
       "      <td>0.53550</td>\n",
       "      <td>0.17410</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>18.250</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>...</td>\n",
       "      <td>22.880</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.14420</td>\n",
       "      <td>0.25760</td>\n",
       "      <td>0.37840</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>13.710</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.093660</td>\n",
       "      <td>0.059850</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>...</td>\n",
       "      <td>17.060</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.16540</td>\n",
       "      <td>0.36820</td>\n",
       "      <td>0.26780</td>\n",
       "      <td>0.15560</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>13.000</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>0.093530</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>...</td>\n",
       "      <td>15.490</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.17030</td>\n",
       "      <td>0.54010</td>\n",
       "      <td>0.53900</td>\n",
       "      <td>0.20600</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>12.460</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.085430</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>...</td>\n",
       "      <td>15.090</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.18530</td>\n",
       "      <td>1.05800</td>\n",
       "      <td>1.10500</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>16.020</td>\n",
       "      <td>23.24</td>\n",
       "      <td>102.70</td>\n",
       "      <td>797.8</td>\n",
       "      <td>0.08206</td>\n",
       "      <td>0.06669</td>\n",
       "      <td>0.032990</td>\n",
       "      <td>0.033230</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>...</td>\n",
       "      <td>19.190</td>\n",
       "      <td>33.88</td>\n",
       "      <td>123.80</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.11810</td>\n",
       "      <td>0.15510</td>\n",
       "      <td>0.14590</td>\n",
       "      <td>0.09975</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.08452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>15.780</td>\n",
       "      <td>17.89</td>\n",
       "      <td>103.60</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0.09710</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.099540</td>\n",
       "      <td>0.066060</td>\n",
       "      <td>0.1842</td>\n",
       "      <td>...</td>\n",
       "      <td>20.420</td>\n",
       "      <td>27.28</td>\n",
       "      <td>136.50</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>0.13960</td>\n",
       "      <td>0.56090</td>\n",
       "      <td>0.39650</td>\n",
       "      <td>0.18100</td>\n",
       "      <td>0.3792</td>\n",
       "      <td>0.10480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>19.170</td>\n",
       "      <td>24.80</td>\n",
       "      <td>132.40</td>\n",
       "      <td>1123.0</td>\n",
       "      <td>0.09740</td>\n",
       "      <td>0.24580</td>\n",
       "      <td>0.206500</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>20.960</td>\n",
       "      <td>29.94</td>\n",
       "      <td>151.70</td>\n",
       "      <td>1332.0</td>\n",
       "      <td>0.10370</td>\n",
       "      <td>0.39030</td>\n",
       "      <td>0.36390</td>\n",
       "      <td>0.17670</td>\n",
       "      <td>0.3176</td>\n",
       "      <td>0.10230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>15.850</td>\n",
       "      <td>23.95</td>\n",
       "      <td>103.70</td>\n",
       "      <td>782.7</td>\n",
       "      <td>0.08401</td>\n",
       "      <td>0.10020</td>\n",
       "      <td>0.099380</td>\n",
       "      <td>0.053640</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>...</td>\n",
       "      <td>16.840</td>\n",
       "      <td>27.66</td>\n",
       "      <td>112.00</td>\n",
       "      <td>876.5</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.19240</td>\n",
       "      <td>0.23220</td>\n",
       "      <td>0.11190</td>\n",
       "      <td>0.2809</td>\n",
       "      <td>0.06287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>13.730</td>\n",
       "      <td>22.61</td>\n",
       "      <td>93.60</td>\n",
       "      <td>578.3</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.22930</td>\n",
       "      <td>0.212800</td>\n",
       "      <td>0.080250</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>15.030</td>\n",
       "      <td>32.01</td>\n",
       "      <td>108.80</td>\n",
       "      <td>697.7</td>\n",
       "      <td>0.16510</td>\n",
       "      <td>0.77250</td>\n",
       "      <td>0.69430</td>\n",
       "      <td>0.22080</td>\n",
       "      <td>0.3596</td>\n",
       "      <td>0.14310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>14.540</td>\n",
       "      <td>27.54</td>\n",
       "      <td>96.73</td>\n",
       "      <td>658.8</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.15950</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.073640</td>\n",
       "      <td>0.2303</td>\n",
       "      <td>...</td>\n",
       "      <td>17.460</td>\n",
       "      <td>37.13</td>\n",
       "      <td>124.10</td>\n",
       "      <td>943.2</td>\n",
       "      <td>0.16780</td>\n",
       "      <td>0.65770</td>\n",
       "      <td>0.70260</td>\n",
       "      <td>0.17120</td>\n",
       "      <td>0.4218</td>\n",
       "      <td>0.13410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>14.680</td>\n",
       "      <td>20.13</td>\n",
       "      <td>94.74</td>\n",
       "      <td>684.5</td>\n",
       "      <td>0.09867</td>\n",
       "      <td>0.07200</td>\n",
       "      <td>0.073950</td>\n",
       "      <td>0.052590</td>\n",
       "      <td>0.1586</td>\n",
       "      <td>...</td>\n",
       "      <td>19.070</td>\n",
       "      <td>30.88</td>\n",
       "      <td>123.40</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.14640</td>\n",
       "      <td>0.18710</td>\n",
       "      <td>0.29140</td>\n",
       "      <td>0.16090</td>\n",
       "      <td>0.3029</td>\n",
       "      <td>0.08216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>16.130</td>\n",
       "      <td>20.68</td>\n",
       "      <td>108.10</td>\n",
       "      <td>798.8</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.20220</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>...</td>\n",
       "      <td>20.960</td>\n",
       "      <td>31.48</td>\n",
       "      <td>136.80</td>\n",
       "      <td>1315.0</td>\n",
       "      <td>0.17890</td>\n",
       "      <td>0.42330</td>\n",
       "      <td>0.47840</td>\n",
       "      <td>0.20730</td>\n",
       "      <td>0.3706</td>\n",
       "      <td>0.11420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>19.810</td>\n",
       "      <td>22.15</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>0.09831</td>\n",
       "      <td>0.10270</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.094980</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>...</td>\n",
       "      <td>27.320</td>\n",
       "      <td>30.88</td>\n",
       "      <td>186.80</td>\n",
       "      <td>2398.0</td>\n",
       "      <td>0.15120</td>\n",
       "      <td>0.31500</td>\n",
       "      <td>0.53720</td>\n",
       "      <td>0.23880</td>\n",
       "      <td>0.2768</td>\n",
       "      <td>0.07615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>13.540</td>\n",
       "      <td>14.36</td>\n",
       "      <td>87.46</td>\n",
       "      <td>566.3</td>\n",
       "      <td>0.09779</td>\n",
       "      <td>0.08129</td>\n",
       "      <td>0.066640</td>\n",
       "      <td>0.047810</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>...</td>\n",
       "      <td>15.110</td>\n",
       "      <td>19.26</td>\n",
       "      <td>99.70</td>\n",
       "      <td>711.2</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.17730</td>\n",
       "      <td>0.23900</td>\n",
       "      <td>0.12880</td>\n",
       "      <td>0.2977</td>\n",
       "      <td>0.07259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>13.080</td>\n",
       "      <td>15.71</td>\n",
       "      <td>85.63</td>\n",
       "      <td>520.0</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.12700</td>\n",
       "      <td>0.045680</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>...</td>\n",
       "      <td>14.500</td>\n",
       "      <td>20.49</td>\n",
       "      <td>96.09</td>\n",
       "      <td>630.5</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.18900</td>\n",
       "      <td>0.07283</td>\n",
       "      <td>0.3184</td>\n",
       "      <td>0.08183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>9.504</td>\n",
       "      <td>12.44</td>\n",
       "      <td>60.34</td>\n",
       "      <td>273.9</td>\n",
       "      <td>0.10240</td>\n",
       "      <td>0.06492</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.1815</td>\n",
       "      <td>...</td>\n",
       "      <td>10.230</td>\n",
       "      <td>15.66</td>\n",
       "      <td>65.13</td>\n",
       "      <td>314.9</td>\n",
       "      <td>0.13240</td>\n",
       "      <td>0.11480</td>\n",
       "      <td>0.08867</td>\n",
       "      <td>0.06227</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.07773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>15.340</td>\n",
       "      <td>14.26</td>\n",
       "      <td>102.50</td>\n",
       "      <td>704.4</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.21350</td>\n",
       "      <td>0.207700</td>\n",
       "      <td>0.097560</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>...</td>\n",
       "      <td>18.070</td>\n",
       "      <td>19.08</td>\n",
       "      <td>125.10</td>\n",
       "      <td>980.9</td>\n",
       "      <td>0.13900</td>\n",
       "      <td>0.59540</td>\n",
       "      <td>0.63050</td>\n",
       "      <td>0.23930</td>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.09946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>21.160</td>\n",
       "      <td>23.04</td>\n",
       "      <td>137.20</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>0.09428</td>\n",
       "      <td>0.10220</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>0.086320</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>...</td>\n",
       "      <td>29.170</td>\n",
       "      <td>35.59</td>\n",
       "      <td>188.00</td>\n",
       "      <td>2615.0</td>\n",
       "      <td>0.14010</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>0.31550</td>\n",
       "      <td>0.20090</td>\n",
       "      <td>0.2822</td>\n",
       "      <td>0.07526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>16.650</td>\n",
       "      <td>21.38</td>\n",
       "      <td>110.00</td>\n",
       "      <td>904.6</td>\n",
       "      <td>0.11210</td>\n",
       "      <td>0.14570</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.091700</td>\n",
       "      <td>0.1995</td>\n",
       "      <td>...</td>\n",
       "      <td>26.460</td>\n",
       "      <td>31.56</td>\n",
       "      <td>177.00</td>\n",
       "      <td>2215.0</td>\n",
       "      <td>0.18050</td>\n",
       "      <td>0.35780</td>\n",
       "      <td>0.46950</td>\n",
       "      <td>0.20950</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.09564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>17.140</td>\n",
       "      <td>16.40</td>\n",
       "      <td>116.00</td>\n",
       "      <td>912.7</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.22760</td>\n",
       "      <td>0.222900</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>...</td>\n",
       "      <td>22.250</td>\n",
       "      <td>21.40</td>\n",
       "      <td>152.40</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>0.15450</td>\n",
       "      <td>0.39490</td>\n",
       "      <td>0.38530</td>\n",
       "      <td>0.25500</td>\n",
       "      <td>0.4066</td>\n",
       "      <td>0.10590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>14.580</td>\n",
       "      <td>21.53</td>\n",
       "      <td>97.41</td>\n",
       "      <td>644.8</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.18680</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.087830</td>\n",
       "      <td>0.2252</td>\n",
       "      <td>...</td>\n",
       "      <td>17.620</td>\n",
       "      <td>33.21</td>\n",
       "      <td>122.40</td>\n",
       "      <td>896.9</td>\n",
       "      <td>0.15250</td>\n",
       "      <td>0.66430</td>\n",
       "      <td>0.55390</td>\n",
       "      <td>0.27010</td>\n",
       "      <td>0.4264</td>\n",
       "      <td>0.12750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>18.610</td>\n",
       "      <td>20.25</td>\n",
       "      <td>122.10</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>0.09440</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.077310</td>\n",
       "      <td>0.1697</td>\n",
       "      <td>...</td>\n",
       "      <td>21.310</td>\n",
       "      <td>27.26</td>\n",
       "      <td>139.90</td>\n",
       "      <td>1403.0</td>\n",
       "      <td>0.13380</td>\n",
       "      <td>0.21170</td>\n",
       "      <td>0.34460</td>\n",
       "      <td>0.14900</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.07421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>15.300</td>\n",
       "      <td>25.27</td>\n",
       "      <td>102.40</td>\n",
       "      <td>732.4</td>\n",
       "      <td>0.10820</td>\n",
       "      <td>0.16970</td>\n",
       "      <td>0.168300</td>\n",
       "      <td>0.087510</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>...</td>\n",
       "      <td>20.270</td>\n",
       "      <td>36.71</td>\n",
       "      <td>149.30</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>0.16410</td>\n",
       "      <td>0.61100</td>\n",
       "      <td>0.63350</td>\n",
       "      <td>0.20240</td>\n",
       "      <td>0.4027</td>\n",
       "      <td>0.09876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>17.570</td>\n",
       "      <td>15.05</td>\n",
       "      <td>115.00</td>\n",
       "      <td>955.1</td>\n",
       "      <td>0.09847</td>\n",
       "      <td>0.11570</td>\n",
       "      <td>0.098750</td>\n",
       "      <td>0.079530</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>...</td>\n",
       "      <td>20.010</td>\n",
       "      <td>19.52</td>\n",
       "      <td>134.90</td>\n",
       "      <td>1227.0</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.28120</td>\n",
       "      <td>0.24890</td>\n",
       "      <td>0.14560</td>\n",
       "      <td>0.2756</td>\n",
       "      <td>0.07919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>1</td>\n",
       "      <td>7.691</td>\n",
       "      <td>25.44</td>\n",
       "      <td>48.34</td>\n",
       "      <td>170.4</td>\n",
       "      <td>0.08668</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.092520</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>0.2037</td>\n",
       "      <td>...</td>\n",
       "      <td>8.678</td>\n",
       "      <td>31.89</td>\n",
       "      <td>54.49</td>\n",
       "      <td>223.6</td>\n",
       "      <td>0.15960</td>\n",
       "      <td>0.30640</td>\n",
       "      <td>0.33930</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.10660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>1</td>\n",
       "      <td>11.540</td>\n",
       "      <td>14.44</td>\n",
       "      <td>74.65</td>\n",
       "      <td>402.9</td>\n",
       "      <td>0.09984</td>\n",
       "      <td>0.11200</td>\n",
       "      <td>0.067370</td>\n",
       "      <td>0.025940</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>...</td>\n",
       "      <td>12.260</td>\n",
       "      <td>19.68</td>\n",
       "      <td>78.78</td>\n",
       "      <td>457.8</td>\n",
       "      <td>0.13450</td>\n",
       "      <td>0.21180</td>\n",
       "      <td>0.17970</td>\n",
       "      <td>0.06918</td>\n",
       "      <td>0.2329</td>\n",
       "      <td>0.08134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>1</td>\n",
       "      <td>14.470</td>\n",
       "      <td>24.99</td>\n",
       "      <td>95.81</td>\n",
       "      <td>656.4</td>\n",
       "      <td>0.08837</td>\n",
       "      <td>0.12300</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.1872</td>\n",
       "      <td>...</td>\n",
       "      <td>16.220</td>\n",
       "      <td>31.73</td>\n",
       "      <td>113.50</td>\n",
       "      <td>808.9</td>\n",
       "      <td>0.13400</td>\n",
       "      <td>0.42020</td>\n",
       "      <td>0.40400</td>\n",
       "      <td>0.12050</td>\n",
       "      <td>0.3187</td>\n",
       "      <td>0.10230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>1</td>\n",
       "      <td>14.740</td>\n",
       "      <td>25.42</td>\n",
       "      <td>94.70</td>\n",
       "      <td>668.6</td>\n",
       "      <td>0.08275</td>\n",
       "      <td>0.07214</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.030270</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>...</td>\n",
       "      <td>16.510</td>\n",
       "      <td>32.29</td>\n",
       "      <td>107.40</td>\n",
       "      <td>826.4</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.13760</td>\n",
       "      <td>0.16110</td>\n",
       "      <td>0.10950</td>\n",
       "      <td>0.2722</td>\n",
       "      <td>0.06956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>1</td>\n",
       "      <td>13.210</td>\n",
       "      <td>28.06</td>\n",
       "      <td>84.88</td>\n",
       "      <td>538.4</td>\n",
       "      <td>0.08671</td>\n",
       "      <td>0.06877</td>\n",
       "      <td>0.029870</td>\n",
       "      <td>0.032750</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>...</td>\n",
       "      <td>14.370</td>\n",
       "      <td>37.17</td>\n",
       "      <td>92.48</td>\n",
       "      <td>629.6</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0.13810</td>\n",
       "      <td>0.10620</td>\n",
       "      <td>0.07958</td>\n",
       "      <td>0.2473</td>\n",
       "      <td>0.06443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>1</td>\n",
       "      <td>13.870</td>\n",
       "      <td>20.70</td>\n",
       "      <td>89.77</td>\n",
       "      <td>584.8</td>\n",
       "      <td>0.09578</td>\n",
       "      <td>0.10180</td>\n",
       "      <td>0.036880</td>\n",
       "      <td>0.023690</td>\n",
       "      <td>0.1620</td>\n",
       "      <td>...</td>\n",
       "      <td>15.050</td>\n",
       "      <td>24.75</td>\n",
       "      <td>99.17</td>\n",
       "      <td>688.6</td>\n",
       "      <td>0.12640</td>\n",
       "      <td>0.20370</td>\n",
       "      <td>0.13770</td>\n",
       "      <td>0.06845</td>\n",
       "      <td>0.2249</td>\n",
       "      <td>0.08492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>1</td>\n",
       "      <td>13.620</td>\n",
       "      <td>23.23</td>\n",
       "      <td>87.19</td>\n",
       "      <td>573.2</td>\n",
       "      <td>0.09246</td>\n",
       "      <td>0.06747</td>\n",
       "      <td>0.029740</td>\n",
       "      <td>0.024430</td>\n",
       "      <td>0.1664</td>\n",
       "      <td>...</td>\n",
       "      <td>15.350</td>\n",
       "      <td>29.09</td>\n",
       "      <td>97.58</td>\n",
       "      <td>729.8</td>\n",
       "      <td>0.12160</td>\n",
       "      <td>0.15170</td>\n",
       "      <td>0.10490</td>\n",
       "      <td>0.07174</td>\n",
       "      <td>0.2642</td>\n",
       "      <td>0.06953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>1</td>\n",
       "      <td>10.320</td>\n",
       "      <td>16.35</td>\n",
       "      <td>65.31</td>\n",
       "      <td>324.9</td>\n",
       "      <td>0.09434</td>\n",
       "      <td>0.04994</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>...</td>\n",
       "      <td>11.250</td>\n",
       "      <td>21.77</td>\n",
       "      <td>71.12</td>\n",
       "      <td>384.9</td>\n",
       "      <td>0.12850</td>\n",
       "      <td>0.08842</td>\n",
       "      <td>0.04384</td>\n",
       "      <td>0.02381</td>\n",
       "      <td>0.2681</td>\n",
       "      <td>0.07399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>1</td>\n",
       "      <td>10.260</td>\n",
       "      <td>16.58</td>\n",
       "      <td>65.85</td>\n",
       "      <td>320.8</td>\n",
       "      <td>0.08877</td>\n",
       "      <td>0.08066</td>\n",
       "      <td>0.043580</td>\n",
       "      <td>0.024380</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>...</td>\n",
       "      <td>10.830</td>\n",
       "      <td>22.04</td>\n",
       "      <td>71.08</td>\n",
       "      <td>357.4</td>\n",
       "      <td>0.14610</td>\n",
       "      <td>0.22460</td>\n",
       "      <td>0.17830</td>\n",
       "      <td>0.08333</td>\n",
       "      <td>0.2691</td>\n",
       "      <td>0.09479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>1</td>\n",
       "      <td>9.683</td>\n",
       "      <td>19.34</td>\n",
       "      <td>61.05</td>\n",
       "      <td>285.7</td>\n",
       "      <td>0.08491</td>\n",
       "      <td>0.05030</td>\n",
       "      <td>0.023370</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>...</td>\n",
       "      <td>10.930</td>\n",
       "      <td>25.59</td>\n",
       "      <td>69.10</td>\n",
       "      <td>364.2</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.09546</td>\n",
       "      <td>0.09350</td>\n",
       "      <td>0.03846</td>\n",
       "      <td>0.2552</td>\n",
       "      <td>0.07920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>1</td>\n",
       "      <td>10.820</td>\n",
       "      <td>24.21</td>\n",
       "      <td>68.89</td>\n",
       "      <td>361.6</td>\n",
       "      <td>0.08192</td>\n",
       "      <td>0.06602</td>\n",
       "      <td>0.015480</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>0.1976</td>\n",
       "      <td>...</td>\n",
       "      <td>13.030</td>\n",
       "      <td>31.45</td>\n",
       "      <td>83.90</td>\n",
       "      <td>505.6</td>\n",
       "      <td>0.12040</td>\n",
       "      <td>0.16330</td>\n",
       "      <td>0.06194</td>\n",
       "      <td>0.03264</td>\n",
       "      <td>0.3059</td>\n",
       "      <td>0.07626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>1</td>\n",
       "      <td>10.860</td>\n",
       "      <td>21.48</td>\n",
       "      <td>68.51</td>\n",
       "      <td>360.5</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.04227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1661</td>\n",
       "      <td>...</td>\n",
       "      <td>11.660</td>\n",
       "      <td>24.77</td>\n",
       "      <td>74.08</td>\n",
       "      <td>412.3</td>\n",
       "      <td>0.10010</td>\n",
       "      <td>0.07348</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2458</td>\n",
       "      <td>0.06592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>1</td>\n",
       "      <td>11.130</td>\n",
       "      <td>22.44</td>\n",
       "      <td>71.49</td>\n",
       "      <td>378.4</td>\n",
       "      <td>0.09566</td>\n",
       "      <td>0.08194</td>\n",
       "      <td>0.048240</td>\n",
       "      <td>0.022570</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>...</td>\n",
       "      <td>12.020</td>\n",
       "      <td>28.26</td>\n",
       "      <td>77.80</td>\n",
       "      <td>436.6</td>\n",
       "      <td>0.10870</td>\n",
       "      <td>0.17820</td>\n",
       "      <td>0.15640</td>\n",
       "      <td>0.06413</td>\n",
       "      <td>0.3169</td>\n",
       "      <td>0.08032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>1</td>\n",
       "      <td>12.770</td>\n",
       "      <td>29.43</td>\n",
       "      <td>81.35</td>\n",
       "      <td>507.9</td>\n",
       "      <td>0.08276</td>\n",
       "      <td>0.04234</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>0.014990</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>...</td>\n",
       "      <td>13.870</td>\n",
       "      <td>36.00</td>\n",
       "      <td>88.10</td>\n",
       "      <td>594.7</td>\n",
       "      <td>0.12340</td>\n",
       "      <td>0.10640</td>\n",
       "      <td>0.08653</td>\n",
       "      <td>0.06498</td>\n",
       "      <td>0.2407</td>\n",
       "      <td>0.06484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>1</td>\n",
       "      <td>9.333</td>\n",
       "      <td>21.94</td>\n",
       "      <td>59.01</td>\n",
       "      <td>264.0</td>\n",
       "      <td>0.09240</td>\n",
       "      <td>0.05605</td>\n",
       "      <td>0.039960</td>\n",
       "      <td>0.012820</td>\n",
       "      <td>0.1692</td>\n",
       "      <td>...</td>\n",
       "      <td>9.845</td>\n",
       "      <td>25.05</td>\n",
       "      <td>62.86</td>\n",
       "      <td>295.8</td>\n",
       "      <td>0.11030</td>\n",
       "      <td>0.08298</td>\n",
       "      <td>0.07993</td>\n",
       "      <td>0.02564</td>\n",
       "      <td>0.2435</td>\n",
       "      <td>0.07393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>1</td>\n",
       "      <td>12.880</td>\n",
       "      <td>28.92</td>\n",
       "      <td>82.50</td>\n",
       "      <td>514.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.05824</td>\n",
       "      <td>0.061950</td>\n",
       "      <td>0.023430</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>...</td>\n",
       "      <td>13.890</td>\n",
       "      <td>35.74</td>\n",
       "      <td>88.84</td>\n",
       "      <td>595.7</td>\n",
       "      <td>0.12270</td>\n",
       "      <td>0.16200</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.06493</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>0.07242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>1</td>\n",
       "      <td>10.290</td>\n",
       "      <td>27.61</td>\n",
       "      <td>65.67</td>\n",
       "      <td>321.4</td>\n",
       "      <td>0.09030</td>\n",
       "      <td>0.07658</td>\n",
       "      <td>0.059990</td>\n",
       "      <td>0.027380</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>...</td>\n",
       "      <td>10.840</td>\n",
       "      <td>34.91</td>\n",
       "      <td>69.57</td>\n",
       "      <td>357.6</td>\n",
       "      <td>0.13840</td>\n",
       "      <td>0.17100</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.09127</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.08283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>1</td>\n",
       "      <td>10.160</td>\n",
       "      <td>19.59</td>\n",
       "      <td>64.73</td>\n",
       "      <td>311.7</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.07504</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.011160</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>...</td>\n",
       "      <td>10.650</td>\n",
       "      <td>22.88</td>\n",
       "      <td>67.88</td>\n",
       "      <td>347.3</td>\n",
       "      <td>0.12650</td>\n",
       "      <td>0.12000</td>\n",
       "      <td>0.01005</td>\n",
       "      <td>0.02232</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.06742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>1</td>\n",
       "      <td>9.423</td>\n",
       "      <td>27.88</td>\n",
       "      <td>59.26</td>\n",
       "      <td>271.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.04971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>...</td>\n",
       "      <td>10.490</td>\n",
       "      <td>34.24</td>\n",
       "      <td>66.50</td>\n",
       "      <td>330.6</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.07158</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>0.06969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>1</td>\n",
       "      <td>14.590</td>\n",
       "      <td>22.68</td>\n",
       "      <td>96.39</td>\n",
       "      <td>657.1</td>\n",
       "      <td>0.08473</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.037360</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>...</td>\n",
       "      <td>15.480</td>\n",
       "      <td>27.27</td>\n",
       "      <td>105.90</td>\n",
       "      <td>733.5</td>\n",
       "      <td>0.10260</td>\n",
       "      <td>0.31710</td>\n",
       "      <td>0.36620</td>\n",
       "      <td>0.11050</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.08004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>1</td>\n",
       "      <td>11.510</td>\n",
       "      <td>23.93</td>\n",
       "      <td>74.52</td>\n",
       "      <td>403.5</td>\n",
       "      <td>0.09261</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.111200</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>...</td>\n",
       "      <td>12.480</td>\n",
       "      <td>37.16</td>\n",
       "      <td>82.28</td>\n",
       "      <td>474.2</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.25170</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.09653</td>\n",
       "      <td>0.2112</td>\n",
       "      <td>0.08732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>1</td>\n",
       "      <td>14.050</td>\n",
       "      <td>27.15</td>\n",
       "      <td>91.38</td>\n",
       "      <td>600.4</td>\n",
       "      <td>0.09929</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.043040</td>\n",
       "      <td>0.1537</td>\n",
       "      <td>...</td>\n",
       "      <td>15.300</td>\n",
       "      <td>33.17</td>\n",
       "      <td>100.20</td>\n",
       "      <td>706.7</td>\n",
       "      <td>0.12410</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.13260</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.08321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>1</td>\n",
       "      <td>11.200</td>\n",
       "      <td>29.37</td>\n",
       "      <td>70.67</td>\n",
       "      <td>386.0</td>\n",
       "      <td>0.07449</td>\n",
       "      <td>0.03558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>...</td>\n",
       "      <td>11.920</td>\n",
       "      <td>38.30</td>\n",
       "      <td>75.19</td>\n",
       "      <td>439.6</td>\n",
       "      <td>0.09267</td>\n",
       "      <td>0.05494</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>0</td>\n",
       "      <td>15.220</td>\n",
       "      <td>30.62</td>\n",
       "      <td>103.40</td>\n",
       "      <td>716.9</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.20870</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.094290</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>...</td>\n",
       "      <td>17.520</td>\n",
       "      <td>42.79</td>\n",
       "      <td>128.70</td>\n",
       "      <td>915.0</td>\n",
       "      <td>0.14170</td>\n",
       "      <td>0.79170</td>\n",
       "      <td>1.17000</td>\n",
       "      <td>0.23560</td>\n",
       "      <td>0.4089</td>\n",
       "      <td>0.14090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0</td>\n",
       "      <td>20.920</td>\n",
       "      <td>25.09</td>\n",
       "      <td>143.00</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.22360</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>0.2149</td>\n",
       "      <td>...</td>\n",
       "      <td>24.290</td>\n",
       "      <td>29.41</td>\n",
       "      <td>179.10</td>\n",
       "      <td>1819.0</td>\n",
       "      <td>0.14070</td>\n",
       "      <td>0.41860</td>\n",
       "      <td>0.65990</td>\n",
       "      <td>0.25420</td>\n",
       "      <td>0.2929</td>\n",
       "      <td>0.09873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0</td>\n",
       "      <td>21.560</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.243900</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.41070</td>\n",
       "      <td>0.22160</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0</td>\n",
       "      <td>20.130</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.097910</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.32150</td>\n",
       "      <td>0.16280</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0</td>\n",
       "      <td>16.600</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.092510</td>\n",
       "      <td>0.053020</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.34030</td>\n",
       "      <td>0.14180</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0</td>\n",
       "      <td>20.600</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.93870</td>\n",
       "      <td>0.26500</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1</td>\n",
       "      <td>7.760</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows  31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1       2      3       4       5        6        7         8         9   \\\n",
       "0     0  17.990  10.38  122.80  1001.0  0.11840  0.27760  0.300100  0.147100   \n",
       "1     0  20.570  17.77  132.90  1326.0  0.08474  0.07864  0.086900  0.070170   \n",
       "2     0  19.690  21.25  130.00  1203.0  0.10960  0.15990  0.197400  0.127900   \n",
       "3     0  11.420  20.38   77.58   386.1  0.14250  0.28390  0.241400  0.105200   \n",
       "4     0  20.290  14.34  135.10  1297.0  0.10030  0.13280  0.198000  0.104300   \n",
       "5     0  12.450  15.70   82.57   477.1  0.12780  0.17000  0.157800  0.080890   \n",
       "6     0  18.250  19.98  119.60  1040.0  0.09463  0.10900  0.112700  0.074000   \n",
       "7     0  13.710  20.83   90.20   577.9  0.11890  0.16450  0.093660  0.059850   \n",
       "8     0  13.000  21.82   87.50   519.8  0.12730  0.19320  0.185900  0.093530   \n",
       "9     0  12.460  24.04   83.97   475.9  0.11860  0.23960  0.227300  0.085430   \n",
       "10    0  16.020  23.24  102.70   797.8  0.08206  0.06669  0.032990  0.033230   \n",
       "11    0  15.780  17.89  103.60   781.0  0.09710  0.12920  0.099540  0.066060   \n",
       "12    0  19.170  24.80  132.40  1123.0  0.09740  0.24580  0.206500  0.111800   \n",
       "13    0  15.850  23.95  103.70   782.7  0.08401  0.10020  0.099380  0.053640   \n",
       "14    0  13.730  22.61   93.60   578.3  0.11310  0.22930  0.212800  0.080250   \n",
       "15    0  14.540  27.54   96.73   658.8  0.11390  0.15950  0.163900  0.073640   \n",
       "16    0  14.680  20.13   94.74   684.5  0.09867  0.07200  0.073950  0.052590   \n",
       "17    0  16.130  20.68  108.10   798.8  0.11700  0.20220  0.172200  0.102800   \n",
       "18    0  19.810  22.15  130.00  1260.0  0.09831  0.10270  0.147900  0.094980   \n",
       "19    1  13.540  14.36   87.46   566.3  0.09779  0.08129  0.066640  0.047810   \n",
       "20    1  13.080  15.71   85.63   520.0  0.10750  0.12700  0.045680  0.031100   \n",
       "21    1   9.504  12.44   60.34   273.9  0.10240  0.06492  0.029560  0.020760   \n",
       "22    0  15.340  14.26  102.50   704.4  0.10730  0.21350  0.207700  0.097560   \n",
       "23    0  21.160  23.04  137.20  1404.0  0.09428  0.10220  0.109700  0.086320   \n",
       "24    0  16.650  21.38  110.00   904.6  0.11210  0.14570  0.152500  0.091700   \n",
       "25    0  17.140  16.40  116.00   912.7  0.11860  0.22760  0.222900  0.140100   \n",
       "26    0  14.580  21.53   97.41   644.8  0.10540  0.18680  0.142500  0.087830   \n",
       "27    0  18.610  20.25  122.10  1094.0  0.09440  0.10660  0.149000  0.077310   \n",
       "28    0  15.300  25.27  102.40   732.4  0.10820  0.16970  0.168300  0.087510   \n",
       "29    0  17.570  15.05  115.00   955.1  0.09847  0.11570  0.098750  0.079530   \n",
       "..   ..     ...    ...     ...     ...      ...      ...       ...       ...   \n",
       "539   1   7.691  25.44   48.34   170.4  0.08668  0.11990  0.092520  0.013640   \n",
       "540   1  11.540  14.44   74.65   402.9  0.09984  0.11200  0.067370  0.025940   \n",
       "541   1  14.470  24.99   95.81   656.4  0.08837  0.12300  0.100900  0.038900   \n",
       "542   1  14.740  25.42   94.70   668.6  0.08275  0.07214  0.041050  0.030270   \n",
       "543   1  13.210  28.06   84.88   538.4  0.08671  0.06877  0.029870  0.032750   \n",
       "544   1  13.870  20.70   89.77   584.8  0.09578  0.10180  0.036880  0.023690   \n",
       "545   1  13.620  23.23   87.19   573.2  0.09246  0.06747  0.029740  0.024430   \n",
       "546   1  10.320  16.35   65.31   324.9  0.09434  0.04994  0.010120  0.005495   \n",
       "547   1  10.260  16.58   65.85   320.8  0.08877  0.08066  0.043580  0.024380   \n",
       "548   1   9.683  19.34   61.05   285.7  0.08491  0.05030  0.023370  0.009615   \n",
       "549   1  10.820  24.21   68.89   361.6  0.08192  0.06602  0.015480  0.008160   \n",
       "550   1  10.860  21.48   68.51   360.5  0.07431  0.04227  0.000000  0.000000   \n",
       "551   1  11.130  22.44   71.49   378.4  0.09566  0.08194  0.048240  0.022570   \n",
       "552   1  12.770  29.43   81.35   507.9  0.08276  0.04234  0.019970  0.014990   \n",
       "553   1   9.333  21.94   59.01   264.0  0.09240  0.05605  0.039960  0.012820   \n",
       "554   1  12.880  28.92   82.50   514.3  0.08123  0.05824  0.061950  0.023430   \n",
       "555   1  10.290  27.61   65.67   321.4  0.09030  0.07658  0.059990  0.027380   \n",
       "556   1  10.160  19.59   64.73   311.7  0.10030  0.07504  0.005025  0.011160   \n",
       "557   1   9.423  27.88   59.26   271.3  0.08123  0.04971  0.000000  0.000000   \n",
       "558   1  14.590  22.68   96.39   657.1  0.08473  0.13300  0.102900  0.037360   \n",
       "559   1  11.510  23.93   74.52   403.5  0.09261  0.10210  0.111200  0.041050   \n",
       "560   1  14.050  27.15   91.38   600.4  0.09929  0.11260  0.044620  0.043040   \n",
       "561   1  11.200  29.37   70.67   386.0  0.07449  0.03558  0.000000  0.000000   \n",
       "562   0  15.220  30.62  103.40   716.9  0.10480  0.20870  0.255000  0.094290   \n",
       "563   0  20.920  25.09  143.00  1347.0  0.10990  0.22360  0.317400  0.147400   \n",
       "564   0  21.560  22.39  142.00  1479.0  0.11100  0.11590  0.243900  0.138900   \n",
       "565   0  20.130  28.25  131.20  1261.0  0.09780  0.10340  0.144000  0.097910   \n",
       "566   0  16.600  28.08  108.30   858.1  0.08455  0.10230  0.092510  0.053020   \n",
       "567   0  20.600  29.33  140.10  1265.0  0.11780  0.27700  0.351400  0.152000   \n",
       "568   1   7.760  24.54   47.92   181.0  0.05263  0.04362  0.000000  0.000000   \n",
       "\n",
       "         10  ...      22     23      24      25       26       27       28  \\\n",
       "0    0.2419  ...  25.380  17.33  184.60  2019.0  0.16220  0.66560  0.71190   \n",
       "1    0.1812  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660  0.24160   \n",
       "2    0.2069  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450  0.45040   \n",
       "3    0.2597  ...  14.910  26.50   98.87   567.7  0.20980  0.86630  0.68690   \n",
       "4    0.1809  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500  0.40000   \n",
       "5    0.2087  ...  15.470  23.75  103.40   741.6  0.17910  0.52490  0.53550   \n",
       "6    0.1794  ...  22.880  27.66  153.20  1606.0  0.14420  0.25760  0.37840   \n",
       "7    0.2196  ...  17.060  28.14  110.60   897.0  0.16540  0.36820  0.26780   \n",
       "8    0.2350  ...  15.490  30.73  106.20   739.3  0.17030  0.54010  0.53900   \n",
       "9    0.2030  ...  15.090  40.68   97.65   711.4  0.18530  1.05800  1.10500   \n",
       "10   0.1528  ...  19.190  33.88  123.80  1150.0  0.11810  0.15510  0.14590   \n",
       "11   0.1842  ...  20.420  27.28  136.50  1299.0  0.13960  0.56090  0.39650   \n",
       "12   0.2397  ...  20.960  29.94  151.70  1332.0  0.10370  0.39030  0.36390   \n",
       "13   0.1847  ...  16.840  27.66  112.00   876.5  0.11310  0.19240  0.23220   \n",
       "14   0.2069  ...  15.030  32.01  108.80   697.7  0.16510  0.77250  0.69430   \n",
       "15   0.2303  ...  17.460  37.13  124.10   943.2  0.16780  0.65770  0.70260   \n",
       "16   0.1586  ...  19.070  30.88  123.40  1138.0  0.14640  0.18710  0.29140   \n",
       "17   0.2164  ...  20.960  31.48  136.80  1315.0  0.17890  0.42330  0.47840   \n",
       "18   0.1582  ...  27.320  30.88  186.80  2398.0  0.15120  0.31500  0.53720   \n",
       "19   0.1885  ...  15.110  19.26   99.70   711.2  0.14400  0.17730  0.23900   \n",
       "20   0.1967  ...  14.500  20.49   96.09   630.5  0.13120  0.27760  0.18900   \n",
       "21   0.1815  ...  10.230  15.66   65.13   314.9  0.13240  0.11480  0.08867   \n",
       "22   0.2521  ...  18.070  19.08  125.10   980.9  0.13900  0.59540  0.63050   \n",
       "23   0.1769  ...  29.170  35.59  188.00  2615.0  0.14010  0.26000  0.31550   \n",
       "24   0.1995  ...  26.460  31.56  177.00  2215.0  0.18050  0.35780  0.46950   \n",
       "25   0.3040  ...  22.250  21.40  152.40  1461.0  0.15450  0.39490  0.38530   \n",
       "26   0.2252  ...  17.620  33.21  122.40   896.9  0.15250  0.66430  0.55390   \n",
       "27   0.1697  ...  21.310  27.26  139.90  1403.0  0.13380  0.21170  0.34460   \n",
       "28   0.1926  ...  20.270  36.71  149.30  1269.0  0.16410  0.61100  0.63350   \n",
       "29   0.1739  ...  20.010  19.52  134.90  1227.0  0.12550  0.28120  0.24890   \n",
       "..      ...  ...     ...    ...     ...     ...      ...      ...      ...   \n",
       "539  0.2037  ...   8.678  31.89   54.49   223.6  0.15960  0.30640  0.33930   \n",
       "540  0.1818  ...  12.260  19.68   78.78   457.8  0.13450  0.21180  0.17970   \n",
       "541  0.1872  ...  16.220  31.73  113.50   808.9  0.13400  0.42020  0.40400   \n",
       "542  0.1840  ...  16.510  32.29  107.40   826.4  0.10600  0.13760  0.16110   \n",
       "543  0.1628  ...  14.370  37.17   92.48   629.6  0.10720  0.13810  0.10620   \n",
       "544  0.1620  ...  15.050  24.75   99.17   688.6  0.12640  0.20370  0.13770   \n",
       "545  0.1664  ...  15.350  29.09   97.58   729.8  0.12160  0.15170  0.10490   \n",
       "546  0.1885  ...  11.250  21.77   71.12   384.9  0.12850  0.08842  0.04384   \n",
       "547  0.1669  ...  10.830  22.04   71.08   357.4  0.14610  0.22460  0.17830   \n",
       "548  0.1580  ...  10.930  25.59   69.10   364.2  0.11990  0.09546  0.09350   \n",
       "549  0.1976  ...  13.030  31.45   83.90   505.6  0.12040  0.16330  0.06194   \n",
       "550  0.1661  ...  11.660  24.77   74.08   412.3  0.10010  0.07348  0.00000   \n",
       "551  0.2030  ...  12.020  28.26   77.80   436.6  0.10870  0.17820  0.15640   \n",
       "552  0.1539  ...  13.870  36.00   88.10   594.7  0.12340  0.10640  0.08653   \n",
       "553  0.1692  ...   9.845  25.05   62.86   295.8  0.11030  0.08298  0.07993   \n",
       "554  0.1566  ...  13.890  35.74   88.84   595.7  0.12270  0.16200  0.24390   \n",
       "555  0.1593  ...  10.840  34.91   69.57   357.6  0.13840  0.17100  0.20000   \n",
       "556  0.1791  ...  10.650  22.88   67.88   347.3  0.12650  0.12000  0.01005   \n",
       "557  0.1742  ...  10.490  34.24   66.50   330.6  0.10730  0.07158  0.00000   \n",
       "558  0.1454  ...  15.480  27.27  105.90   733.5  0.10260  0.31710  0.36620   \n",
       "559  0.1388  ...  12.480  37.16   82.28   474.2  0.12980  0.25170  0.36300   \n",
       "560  0.1537  ...  15.300  33.17  100.20   706.7  0.12410  0.22640  0.13260   \n",
       "561  0.1060  ...  11.920  38.30   75.19   439.6  0.09267  0.05494  0.00000   \n",
       "562  0.2128  ...  17.520  42.79  128.70   915.0  0.14170  0.79170  1.17000   \n",
       "563  0.2149  ...  24.290  29.41  179.10  1819.0  0.14070  0.41860  0.65990   \n",
       "564  0.1726  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130  0.41070   \n",
       "565  0.1752  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220  0.32150   \n",
       "566  0.1590  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940  0.34030   \n",
       "567  0.2397  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810  0.93870   \n",
       "568  0.1587  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.00000   \n",
       "\n",
       "          29      30       31  \n",
       "0    0.26540  0.4601  0.11890  \n",
       "1    0.18600  0.2750  0.08902  \n",
       "2    0.24300  0.3613  0.08758  \n",
       "3    0.25750  0.6638  0.17300  \n",
       "4    0.16250  0.2364  0.07678  \n",
       "5    0.17410  0.3985  0.12440  \n",
       "6    0.19320  0.3063  0.08368  \n",
       "7    0.15560  0.3196  0.11510  \n",
       "8    0.20600  0.4378  0.10720  \n",
       "9    0.22100  0.4366  0.20750  \n",
       "10   0.09975  0.2948  0.08452  \n",
       "11   0.18100  0.3792  0.10480  \n",
       "12   0.17670  0.3176  0.10230  \n",
       "13   0.11190  0.2809  0.06287  \n",
       "14   0.22080  0.3596  0.14310  \n",
       "15   0.17120  0.4218  0.13410  \n",
       "16   0.16090  0.3029  0.08216  \n",
       "17   0.20730  0.3706  0.11420  \n",
       "18   0.23880  0.2768  0.07615  \n",
       "19   0.12880  0.2977  0.07259  \n",
       "20   0.07283  0.3184  0.08183  \n",
       "21   0.06227  0.2450  0.07773  \n",
       "22   0.23930  0.4667  0.09946  \n",
       "23   0.20090  0.2822  0.07526  \n",
       "24   0.20950  0.3613  0.09564  \n",
       "25   0.25500  0.4066  0.10590  \n",
       "26   0.27010  0.4264  0.12750  \n",
       "27   0.14900  0.2341  0.07421  \n",
       "28   0.20240  0.4027  0.09876  \n",
       "29   0.14560  0.2756  0.07919  \n",
       "..       ...     ...      ...  \n",
       "539  0.05000  0.2790  0.10660  \n",
       "540  0.06918  0.2329  0.08134  \n",
       "541  0.12050  0.3187  0.10230  \n",
       "542  0.10950  0.2722  0.06956  \n",
       "543  0.07958  0.2473  0.06443  \n",
       "544  0.06845  0.2249  0.08492  \n",
       "545  0.07174  0.2642  0.06953  \n",
       "546  0.02381  0.2681  0.07399  \n",
       "547  0.08333  0.2691  0.09479  \n",
       "548  0.03846  0.2552  0.07920  \n",
       "549  0.03264  0.3059  0.07626  \n",
       "550  0.00000  0.2458  0.06592  \n",
       "551  0.06413  0.3169  0.08032  \n",
       "552  0.06498  0.2407  0.06484  \n",
       "553  0.02564  0.2435  0.07393  \n",
       "554  0.06493  0.2372  0.07242  \n",
       "555  0.09127  0.2226  0.08283  \n",
       "556  0.02232  0.2262  0.06742  \n",
       "557  0.00000  0.2475  0.06969  \n",
       "558  0.11050  0.2258  0.08004  \n",
       "559  0.09653  0.2112  0.08732  \n",
       "560  0.10480  0.2250  0.08321  \n",
       "561  0.00000  0.1566  0.05905  \n",
       "562  0.23560  0.4089  0.14090  \n",
       "563  0.25420  0.2929  0.09873  \n",
       "564  0.22160  0.2060  0.07115  \n",
       "565  0.16280  0.2572  0.06637  \n",
       "566  0.14180  0.2218  0.07820  \n",
       "567  0.26500  0.4087  0.12400  \n",
       "568  0.00000  0.2871  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_M = round(len(df[(df[1]==0)])*0.2)\n",
    "num_B = round(len(df[(df[1]==1)])*0.2)\n",
    "df_M = df[(df[1]==-1)]\n",
    "#df_M.reset_index()\n",
    "df_B = df[(df[1]==1)]\n",
    "test_df = df_M.iloc[:num_M,:]\n",
    "train_df =  df_M.iloc[num_M:,:]\n",
    "test_df = pd.concat([test_df, df_B.iloc[:num_B,:]])\n",
    "train_df = pd.concat([train_df, df_B.iloc[num_B:,:]])                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data set (71, 31)\n",
      "train dataset (286, 31)\n"
     ]
    }
   ],
   "source": [
    "print('test data set', test_df.shape)\n",
    "print('train dataset', train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Monte-Carlo Simulation: Repeat the following procedures for supervised, un-supervised, and semi-supervised learing M=30 times, and use randomly selected train and test data (make sure you us 20% of both the positive and negative classes as the test set). Then compare the average scores (accuracy, precision, recall, F-score, and AUC) that you obtain from each algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i. Supervised Learning: Train L1-penalized SVM to classify the data. Use 5 fold cross validation to choose the penalty parameter. Use normalized data. Report the average accuray, percision, recall, F-score, and AUC, for both training and test sets over your M runs. Plot the ROC and report the confusion matrix for training and testing in one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.svm import LinearSVC\n",
    "\n",
    "iterNum = 30\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]} \n",
    "result_svm_L1 = []\n",
    "pred = []\n",
    "pred_proba = []\n",
    "y_true = []\n",
    "num = 0\n",
    "\n",
    "for i in range(iterNum):\n",
    "    # Splitting data (test:train = 20:80)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df.loc[:,2:], df.loc[:,1], test_size=0.2, stratify=df.loc[:,1])\n",
    "    \n",
    "    # Normalization (Standardization)\n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    x_test = StandardScaler().fit_transform(x_test)\n",
    "    \n",
    "    l1_svm = LinearSVC(loss='l2', penalty='l1', dual=False)\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    grid = GridSearchCV(l1_svm,param_grid,cv=StratifiedKFold(n_splits=5))\n",
    "    grid.fit(x_train,y_train)\n",
    "    \n",
    "    #best C\n",
    "    best_params = grid.best_params_\n",
    "    \n",
    "    #prediction\n",
    "    train_pred = grid.predict(x_train)\n",
    "    test_pred = grid.predict(x_test)\n",
    "    train_pred_decision = grid.decision_function(x_train)\n",
    "    test_pred_decision = grid.decision_function(x_test)\n",
    "\n",
    "    #accuracy score\n",
    "    train_accuracy = accuracy_score(y_train, train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, test_pred)\n",
    "    \n",
    "    #precision, recall, F-score\n",
    "    train_precision, train_recall, train_fscore, train_support = precision_recall_fscore_support(y_train, train_pred, average='binary')\n",
    "    test_precision, test_recall, test_fscore, test_support = precision_recall_fscore_support(y_test, test_pred, average='binary')\n",
    "    \n",
    "    #auc\n",
    "    train_auc = roc_auc_score(y_train, train_pred_decision)\n",
    "    test_auc = roc_auc_score(y_test, test_pred_decision)\n",
    "    \n",
    "    result_svm_L1.append([i+1, best_params['C'], train_accuracy, train_precision, train_recall, train_fscore, train_auc,\n",
    "                         test_accuracy, test_precision, test_recall, test_fscore, test_auc])\n",
    "    \n",
    "    if i == iterNum-1:\n",
    "        y_true = [y_train, y_test]\n",
    "        pred = [train_pred, test_pred]\n",
    "        pred_decision = [train_pred_decision, test_pred_decision]\n",
    "        num = i+1\n",
    "        \n",
    "result_svm_L1 = pd.DataFrame(result_svm_L1, columns = ['','best C', 'train accuracy', 'train precision', 'train recall',\n",
    "                                                       'train F-score', 'train AUC','test accuracy', 'test precision',\n",
    "                                                       'test recall', 'test F-score', 'test AUC'])\n",
    "result_svm_L1.set_index('', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best C</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>train precision</th>\n",
       "      <th>train recall</th>\n",
       "      <th>train F-score</th>\n",
       "      <th>train AUC</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>test precision</th>\n",
       "      <th>test recall</th>\n",
       "      <th>test F-score</th>\n",
       "      <th>test AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.980220</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>0.984293</td>\n",
       "      <td>0.996409</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.996032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.991209</td>\n",
       "      <td>0.986159</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993031</td>\n",
       "      <td>0.997461</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.997354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.982639</td>\n",
       "      <td>0.992982</td>\n",
       "      <td>0.987784</td>\n",
       "      <td>0.997358</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.995040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.982418</td>\n",
       "      <td>0.979239</td>\n",
       "      <td>0.992982</td>\n",
       "      <td>0.986063</td>\n",
       "      <td>0.995645</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.989087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.989510</td>\n",
       "      <td>0.992982</td>\n",
       "      <td>0.991243</td>\n",
       "      <td>0.997234</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.996693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.996491</td>\n",
       "      <td>0.991274</td>\n",
       "      <td>0.996388</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.992725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.986813</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>0.999051</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.984458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.982418</td>\n",
       "      <td>0.982578</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.996904</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.997354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.986813</td>\n",
       "      <td>0.982699</td>\n",
       "      <td>0.996491</td>\n",
       "      <td>0.989547</td>\n",
       "      <td>0.998246</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.985450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.980220</td>\n",
       "      <td>0.975862</td>\n",
       "      <td>0.992982</td>\n",
       "      <td>0.984348</td>\n",
       "      <td>0.996925</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.999669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.986813</td>\n",
       "      <td>0.982699</td>\n",
       "      <td>0.996491</td>\n",
       "      <td>0.989547</td>\n",
       "      <td>0.997131</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.965986</td>\n",
       "      <td>0.997024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.972125</td>\n",
       "      <td>0.978947</td>\n",
       "      <td>0.975524</td>\n",
       "      <td>0.992054</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.965035</td>\n",
       "      <td>0.992725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.982418</td>\n",
       "      <td>0.979239</td>\n",
       "      <td>0.992982</td>\n",
       "      <td>0.986063</td>\n",
       "      <td>0.996574</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986301</td>\n",
       "      <td>0.999008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.980220</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>0.984293</td>\n",
       "      <td>0.995769</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.986301</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993103</td>\n",
       "      <td>0.991733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.995604</td>\n",
       "      <td>0.993031</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996503</td>\n",
       "      <td>0.999752</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.989418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.982418</td>\n",
       "      <td>0.982578</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.996760</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.999339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.986813</td>\n",
       "      <td>0.986063</td>\n",
       "      <td>0.992982</td>\n",
       "      <td>0.989510</td>\n",
       "      <td>0.997461</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.994709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.991209</td>\n",
       "      <td>0.989547</td>\n",
       "      <td>0.996491</td>\n",
       "      <td>0.993007</td>\n",
       "      <td>0.997420</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.994709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.996491</td>\n",
       "      <td>0.987826</td>\n",
       "      <td>0.997007</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.957746</td>\n",
       "      <td>0.996693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.986813</td>\n",
       "      <td>0.982699</td>\n",
       "      <td>0.996491</td>\n",
       "      <td>0.989547</td>\n",
       "      <td>0.996409</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.995370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.991209</td>\n",
       "      <td>0.992982</td>\n",
       "      <td>0.992982</td>\n",
       "      <td>0.992982</td>\n",
       "      <td>0.996925</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.989087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.995604</td>\n",
       "      <td>0.993031</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996503</td>\n",
       "      <td>0.999876</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.963624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.982639</td>\n",
       "      <td>0.992982</td>\n",
       "      <td>0.987784</td>\n",
       "      <td>0.996945</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.997685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.989510</td>\n",
       "      <td>0.992982</td>\n",
       "      <td>0.991243</td>\n",
       "      <td>0.998658</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.987765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.991209</td>\n",
       "      <td>0.986159</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993031</td>\n",
       "      <td>0.996388</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.965035</td>\n",
       "      <td>0.989418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.982639</td>\n",
       "      <td>0.992982</td>\n",
       "      <td>0.987784</td>\n",
       "      <td>0.996429</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.991071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.996491</td>\n",
       "      <td>0.991274</td>\n",
       "      <td>0.997234</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.996362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.982418</td>\n",
       "      <td>0.975945</td>\n",
       "      <td>0.996491</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.996615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.997007</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.998677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.982418</td>\n",
       "      <td>0.975945</td>\n",
       "      <td>0.996491</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.997049</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.996032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    best C  train accuracy  train precision  train recall  train F-score  \\\n",
       "                                                                           \n",
       "1     0.10        0.980220         0.979167      0.989474       0.984293   \n",
       "2     0.10        0.991209         0.986159      1.000000       0.993031   \n",
       "3     0.10        0.984615         0.982639      0.992982       0.987784   \n",
       "4     0.10        0.982418         0.979239      0.992982       0.986063   \n",
       "5     0.10        0.989011         0.989510      0.992982       0.991243   \n",
       "6     0.10        0.989011         0.986111      0.996491       0.991274   \n",
       "7     0.10        0.986813         0.989474      0.989474       0.989474   \n",
       "8     0.10        0.982418         0.982578      0.989474       0.986014   \n",
       "9     0.10        0.986813         0.982699      0.996491       0.989547   \n",
       "10    0.10        0.980220         0.975862      0.992982       0.984348   \n",
       "11    0.10        0.986813         0.982699      0.996491       0.989547   \n",
       "12    0.01        0.969231         0.972125      0.978947       0.975524   \n",
       "13    0.10        0.982418         0.979239      0.992982       0.986063   \n",
       "14    0.10        0.980220         0.979167      0.989474       0.984293   \n",
       "15    1.00        0.995604         0.993031      1.000000       0.996503   \n",
       "16    0.10        0.982418         0.982578      0.989474       0.986014   \n",
       "17    0.10        0.986813         0.986063      0.992982       0.989510   \n",
       "18    0.10        0.991209         0.989547      0.996491       0.993007   \n",
       "19    0.10        0.984615         0.979310      0.996491       0.987826   \n",
       "20    0.10        0.986813         0.982699      0.996491       0.989547   \n",
       "21    0.10        0.991209         0.992982      0.992982       0.992982   \n",
       "22    1.00        0.995604         0.993031      1.000000       0.996503   \n",
       "23    0.10        0.984615         0.982639      0.992982       0.987784   \n",
       "24    0.10        0.989011         0.989510      0.992982       0.991243   \n",
       "25    0.10        0.991209         0.986159      1.000000       0.993031   \n",
       "26    0.10        0.984615         0.982639      0.992982       0.987784   \n",
       "27    0.10        0.989011         0.986111      0.996491       0.991274   \n",
       "28    0.10        0.982418         0.975945      0.996491       0.986111   \n",
       "29    0.10        0.989011         0.982759      1.000000       0.991304   \n",
       "30    0.10        0.982418         0.975945      0.996491       0.986111   \n",
       "\n",
       "    train AUC  test accuracy  test precision  test recall  test F-score  \\\n",
       "                                                                          \n",
       "1    0.996409       0.973684        0.972603     0.986111      0.979310   \n",
       "2    0.997461       0.973684        0.972603     0.986111      0.979310   \n",
       "3    0.997358       0.964912        0.959459     0.986111      0.972603   \n",
       "4    0.995645       0.964912        0.959459     0.986111      0.972603   \n",
       "5    0.997234       0.964912        0.947368     1.000000      0.972973   \n",
       "6    0.996388       0.973684        0.972603     0.986111      0.979310   \n",
       "7    0.999051       0.964912        0.959459     0.986111      0.972603   \n",
       "8    0.996904       0.973684        0.972603     0.986111      0.979310   \n",
       "9    0.998246       0.973684        0.985915     0.972222      0.979021   \n",
       "10   0.996925       0.982456        1.000000     0.972222      0.985915   \n",
       "11   0.997131       0.956140        0.946667     0.986111      0.965986   \n",
       "12   0.992054       0.956140        0.971831     0.958333      0.965035   \n",
       "13   0.996574       0.982456        0.972973     1.000000      0.986301   \n",
       "14   0.995769       0.991228        0.986301     1.000000      0.993103   \n",
       "15   0.999752       0.973684        0.972603     0.986111      0.979310   \n",
       "16   0.996760       0.973684        0.960000     1.000000      0.979592   \n",
       "17   0.997461       0.964912        0.947368     1.000000      0.972973   \n",
       "18   0.997420       0.956140        0.958904     0.972222      0.965517   \n",
       "19   0.997007       0.947368        0.971429     0.944444      0.957746   \n",
       "20   0.996409       0.973684        0.985915     0.972222      0.979021   \n",
       "21   0.996925       0.964912        0.947368     1.000000      0.972973   \n",
       "22   0.999876       0.956140        0.958904     0.972222      0.965517   \n",
       "23   0.996945       0.982456        0.986111     0.986111      0.986111   \n",
       "24   0.998658       0.973684        0.960000     1.000000      0.979592   \n",
       "25   0.996388       0.956140        0.971831     0.958333      0.965035   \n",
       "26   0.996429       0.982456        0.986111     0.986111      0.986111   \n",
       "27   0.997234       0.964912        0.959459     0.986111      0.972603   \n",
       "28   0.996615       1.000000        1.000000     1.000000      1.000000   \n",
       "29   0.997007       0.964912        0.972222     0.972222      0.972222   \n",
       "30   0.997049       0.956140        0.958904     0.972222      0.965517   \n",
       "\n",
       "    test AUC  \n",
       "              \n",
       "1   0.996032  \n",
       "2   0.997354  \n",
       "3   0.995040  \n",
       "4   0.989087  \n",
       "5   0.996693  \n",
       "6   0.992725  \n",
       "7   0.984458  \n",
       "8   0.997354  \n",
       "9   0.985450  \n",
       "10  0.999669  \n",
       "11  0.997024  \n",
       "12  0.992725  \n",
       "13  0.999008  \n",
       "14  0.991733  \n",
       "15  0.989418  \n",
       "16  0.999339  \n",
       "17  0.994709  \n",
       "18  0.994709  \n",
       "19  0.996693  \n",
       "20  0.995370  \n",
       "21  0.989087  \n",
       "22  0.963624  \n",
       "23  0.997685  \n",
       "24  0.987765  \n",
       "25  0.989418  \n",
       "26  0.991071  \n",
       "27  0.996362  \n",
       "28  1.000000  \n",
       "29  0.998677  \n",
       "30  0.996032  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_svm_L1\n",
    "\n",
    "#i, best_params, train_accuracy, train_precision, train_recall, train_fscore, train_auc,\n",
    "#                         test_accuracy, test_precision, test_recall, test_fscore, test_auc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>train precision</th>\n",
       "      <th>train recall</th>\n",
       "      <th>train F-score</th>\n",
       "      <th>train AUC</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>test precision</th>\n",
       "      <th>test recall</th>\n",
       "      <th>test F-score</th>\n",
       "      <th>test AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.985934</td>\n",
       "      <td>0.983587</td>\n",
       "      <td>0.994152</td>\n",
       "      <td>0.988834</td>\n",
       "      <td>0.997036</td>\n",
       "      <td>0.969591</td>\n",
       "      <td>0.969233</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.976108</td>\n",
       "      <td>0.993144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train accuracy  train precision  train recall  train F-score  train AUC  \\\n",
       "0        0.985934         0.983587      0.994152       0.988834   0.997036   \n",
       "\n",
       "   test accuracy  test precision  test recall  test F-score  test AUC  \n",
       "0       0.969591        0.969233     0.983333      0.976108  0.993144  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_svm_L1_avg = pd.DataFrame(result_svm_L1.mean(axis=0)).T\n",
    "part1_result = pd.DataFrame(result_svm_L1_avg.loc[:, 'train accuracy':])\n",
    "result_svm_L1_avg.loc[:, 'train accuracy':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration index number:  30 \n",
      "\n",
      "train data\n",
      "\n",
      "[[163   7]\n",
      " [  1 284]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data\n",
      "\n",
      "[[39  3]\n",
      " [ 2 70]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('iteration index number: ', num, '\\n')\n",
    "\n",
    "# Plot confusin matrix & ROC\n",
    "for i in range(len(y_true)):\n",
    "    cm = confusion_matrix(y_true[i], pd.DataFrame(pred[i]))\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_true[i], pred_decision[i])\n",
    "    if i == 0:\n",
    "        print('train data\\n')\n",
    "    else :\n",
    "        print('test data\\n')\n",
    "    print(cm, '\\n')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ii. Semi-Supervised Learning/ Self-training : select 50% of the positive class along with 50% of the negative class in the training set as labeled data and the rest as unlabelled data. You can selet them randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.loc[:,2:], df.loc[:,1], test_size=0.2, stratify=df.loc[:,1])\n",
    "x_labeled, x_unlabeled, y_labeled,_ = train_test_split(x_train, y_train, test_size=0.5, stratify=y_train)\n",
    "## set unlabeled index -> ignore\n",
    "## or\n",
    "## unlabeled index -> decision function index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_svm_semi=[]\n",
    "iterNum = 30\n",
    "y_true = []\n",
    "pred = []\n",
    "pred_decision = []\n",
    "num = 0\n",
    "\n",
    "# randomly choose test data and train data set (test:train = 2:8)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.loc[:,2:], df.loc[:,1], test_size=0.2, stratify=df.loc[:,1])\n",
    "#x_train = MinMaxScaler().fit_transform(x_train)\n",
    "#x_test = MinMaxScaler().fit_transform(x_test)\n",
    "# standardization(Normalization)\n",
    "x_train = StandardScaler().fit_transform(x_train)\n",
    "x_test = StandardScaler().fit_transform(x_test)\n",
    "\n",
    "# Create my estimator and prepare the parameter grid dictionary\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]} \n",
    "svm_l1_semi = LinearSVC(loss='l2', penalty='l1', dual=False)\n",
    "\n",
    "#iterating 30 times\n",
    "for i in range(iterNum):\n",
    "    # split data as labeled set and unlabeled set(50:50)\n",
    "    x_labeled, x_unlabeled, y_labeled,_ = train_test_split(x_train, y_train, test_size=0.5, stratify=y_train)\n",
    "    \n",
    "    #find C using cross validation\n",
    "    grid = GridSearchCV(svm_l1_semi,param_grid,cv=StratifiedKFold(n_splits=5))\n",
    "    grid.fit(x_labeled,y_labeled)\n",
    "    #best C\n",
    "    best_params = grid.best_params_\n",
    "    \n",
    "    # capture and fit the best estimator from across the grid search\n",
    "    best_svm = grid.best_estimator_\n",
    "    num_unlabeled = len(x_unlabeled)\n",
    "    while num_unlabeled > 0:\n",
    "        # train svm with labeled data\n",
    "        best_svm.fit(x_labeled, y_labeled)\n",
    "\n",
    "        # find the distance of unlabeled data form the decision boundary\n",
    "        decision = best_svm.decision_function(x_unlabeled)\n",
    "        coef = best_svm.coef_\n",
    "        norms = np.linalg.norm(coef)\n",
    "        distance = decision/norms\n",
    "        distance = np.array(distance)\n",
    "        \n",
    "        # change to absolute values\n",
    "        distance = np.absolute(distance)\n",
    "        \n",
    "        # find the index of farthest one from unlabeled set\n",
    "        idx = np.where(distance == np.max(distance))\n",
    "        \n",
    "        # labeling the farthest unlabeled one with svm\n",
    "        unlabeled_test = x_unlabeled[idx]\n",
    "        pred = best_svm.predict(unlabeled_test)\n",
    "        \n",
    "        # add newly labeled one to labeled data\n",
    "        x_labeled = np.append(x_labeled, unlabeled_test, axis=0)\n",
    "        y_labeled = y_labeled.append(pd.Series(pred), ignore_index=True)\n",
    "        \n",
    "        # delete labeled one from unlabeled set\n",
    "        x_unlabeled = np.delete(x_unlabeled, idx, 0)\n",
    "        \n",
    "        # Test the final SVM on the test data\n",
    "        if num_unlabeled==1:\n",
    "            #prediction\n",
    "            train_pred = grid.predict(x_labeled)\n",
    "            test_pred = grid.predict(x_test)\n",
    "            train_pred_decision = grid.decision_function(x_labeled)\n",
    "            test_pred_decision = grid.decision_function(x_test)\n",
    "\n",
    "            #accuracy score\n",
    "            train_accuracy = accuracy_score(y_labeled, train_pred)\n",
    "            test_accuracy = accuracy_score(y_test, test_pred)\n",
    "\n",
    "            #precision, recall, F-score\n",
    "            train_precision, train_recall, train_fscore, train_support = precision_recall_fscore_support(y_labeled, train_pred, average='binary')\n",
    "            test_precision, test_recall, test_fscore, test_support = precision_recall_fscore_support(y_test, test_pred, average='binary')\n",
    "\n",
    "            #auc\n",
    "            train_auc = roc_auc_score(y_labeled, train_pred_decision)\n",
    "            test_auc = roc_auc_score(y_test, test_pred_decision)\n",
    "            \n",
    "            #save results\n",
    "            result_svm_semi.append([i+1, best_params['C'], train_accuracy, train_precision, train_recall, train_fscore, train_auc,\n",
    "                                 test_accuracy, test_precision, test_recall, test_fscore, test_auc])      \n",
    "        \n",
    "        # compute the number of unlabeled points\n",
    "        num_unlabeled = len(x_unlabeled)\n",
    "        \n",
    "    if i == iterNum-1:\n",
    "        y_true = [y_labeled, y_test]\n",
    "        pred = [train_pred, test_pred]\n",
    "        pred_decision = [train_pred_decision, test_pred_decision]\n",
    "        num = i+1\n",
    "\n",
    "#save\n",
    "result_svm_semi = pd.DataFrame(result_svm_semi, columns = ['','best C', 'train accuracy', 'train precision', 'train recall',\n",
    "                                                   'train F-score', 'train AUC','test accuracy', 'test precision',\n",
    "                                                   'test recall', 'test F-score', 'test AUC'])\n",
    "result_svm_semi.set_index('', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Train an L1-penalized SVM to classify the labeled data. Use normalized data. Choose the penalty parameter using 5 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best C</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     best C\n",
       "           \n",
       "1       0.1\n",
       "2       0.1\n",
       "3   10000.0\n",
       "4       0.1\n",
       "5       0.1\n",
       "6       0.1\n",
       "7       1.0\n",
       "8       0.1\n",
       "9       0.1\n",
       "10      0.1\n",
       "11      0.1\n",
       "12      0.1\n",
       "13      0.1\n",
       "14      0.1\n",
       "15      0.1\n",
       "16      0.1\n",
       "17      0.1\n",
       "18      0.1\n",
       "19      0.1\n",
       "20      0.1\n",
       "21      1.0\n",
       "22      0.1\n",
       "23      0.1\n",
       "24      0.1\n",
       "25      0.1\n",
       "26   1000.0\n",
       "27      0.1\n",
       "28      0.1\n",
       "29      0.1\n",
       "30      1.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_svm_semi.loc[:, :'best C']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Find the unlabeled data point that is the farthest to the decision boundary of the SVM. Let the SVM label it (ignore its true label), and add it to the labeled data, and retrain the SVM. Continue this process until all unlabeled data are used. Test the final SVM on the test data and the average accracy, precision, recall, F-score, and AUC, for both training and test sets over your M runs. Plot the ROC and report the confusion matrix for training and testing in one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best C</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>train precision</th>\n",
       "      <th>train recall</th>\n",
       "      <th>train F-score</th>\n",
       "      <th>train AUC</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>test precision</th>\n",
       "      <th>test recall</th>\n",
       "      <th>test F-score</th>\n",
       "      <th>test AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.995604</td>\n",
       "      <td>0.993174</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996575</td>\n",
       "      <td>0.996354</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.997024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.995604</td>\n",
       "      <td>0.993007</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996491</td>\n",
       "      <td>0.996891</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.992725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.997354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.991209</td>\n",
       "      <td>0.992982</td>\n",
       "      <td>0.992982</td>\n",
       "      <td>0.992982</td>\n",
       "      <td>0.998947</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.986442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.989583</td>\n",
       "      <td>0.993031</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.999191</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.986111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.991209</td>\n",
       "      <td>0.993056</td>\n",
       "      <td>0.993056</td>\n",
       "      <td>0.993056</td>\n",
       "      <td>0.999626</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.985780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.995604</td>\n",
       "      <td>0.996516</td>\n",
       "      <td>0.996516</td>\n",
       "      <td>0.996516</td>\n",
       "      <td>0.999834</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.997354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.986813</td>\n",
       "      <td>0.986348</td>\n",
       "      <td>0.993127</td>\n",
       "      <td>0.989726</td>\n",
       "      <td>0.996207</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.965986</td>\n",
       "      <td>0.997685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.997802</td>\n",
       "      <td>0.996610</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998302</td>\n",
       "      <td>0.998204</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.991209</td>\n",
       "      <td>0.989655</td>\n",
       "      <td>0.996528</td>\n",
       "      <td>0.993080</td>\n",
       "      <td>0.996798</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.997685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>0.992982</td>\n",
       "      <td>0.996479</td>\n",
       "      <td>0.994728</td>\n",
       "      <td>0.996623</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.991733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.991209</td>\n",
       "      <td>0.989726</td>\n",
       "      <td>0.996552</td>\n",
       "      <td>0.993127</td>\n",
       "      <td>0.999561</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986301</td>\n",
       "      <td>0.989418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.991209</td>\n",
       "      <td>0.986348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993127</td>\n",
       "      <td>0.997749</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.997685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.991209</td>\n",
       "      <td>0.989761</td>\n",
       "      <td>0.996564</td>\n",
       "      <td>0.993151</td>\n",
       "      <td>0.998910</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.984788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.986813</td>\n",
       "      <td>0.982818</td>\n",
       "      <td>0.996516</td>\n",
       "      <td>0.989619</td>\n",
       "      <td>0.996453</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986301</td>\n",
       "      <td>0.994378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.995604</td>\n",
       "      <td>0.996587</td>\n",
       "      <td>0.996587</td>\n",
       "      <td>0.996587</td>\n",
       "      <td>0.999684</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.985780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.991209</td>\n",
       "      <td>0.986395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993151</td>\n",
       "      <td>0.999018</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.990410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.986348</td>\n",
       "      <td>0.996552</td>\n",
       "      <td>0.991424</td>\n",
       "      <td>0.999666</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.989418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>0.989399</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994671</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.986442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.982877</td>\n",
       "      <td>0.993080</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.996581</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.986301</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993103</td>\n",
       "      <td>0.997024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>0.989796</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994872</td>\n",
       "      <td>0.999644</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.986301</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993103</td>\n",
       "      <td>0.999008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.997802</td>\n",
       "      <td>0.996610</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998302</td>\n",
       "      <td>0.998056</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.965986</td>\n",
       "      <td>0.997354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.991209</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.996540</td>\n",
       "      <td>0.993103</td>\n",
       "      <td>0.996957</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.996693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994709</td>\n",
       "      <td>0.996905</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.992063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.989510</td>\n",
       "      <td>0.992982</td>\n",
       "      <td>0.991243</td>\n",
       "      <td>0.999422</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986301</td>\n",
       "      <td>0.984788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.999008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.991209</td>\n",
       "      <td>0.986441</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993174</td>\n",
       "      <td>0.996962</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.997685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.982877</td>\n",
       "      <td>0.993080</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.996290</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986301</td>\n",
       "      <td>0.997685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>0.989619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994783</td>\n",
       "      <td>0.996772</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.993056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>0.989547</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994746</td>\n",
       "      <td>0.999815</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.995701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     best C  train accuracy  train precision  train recall  train F-score  \\\n",
       "                                                                            \n",
       "1       0.1        0.995604         0.993174      1.000000       0.996575   \n",
       "2       0.1        0.995604         0.993007      1.000000       0.996491   \n",
       "3   10000.0        1.000000         1.000000      1.000000       1.000000   \n",
       "4       0.1        0.991209         0.992982      0.992982       0.992982   \n",
       "5       0.1        0.989011         0.989583      0.993031       0.991304   \n",
       "6       0.1        0.991209         0.993056      0.993056       0.993056   \n",
       "7       1.0        0.995604         0.996516      0.996516       0.996516   \n",
       "8       0.1        0.986813         0.986348      0.993127       0.989726   \n",
       "9       0.1        0.997802         0.996610      1.000000       0.998302   \n",
       "10      0.1        0.991209         0.989655      0.996528       0.993080   \n",
       "11      0.1        0.993407         0.992982      0.996479       0.994728   \n",
       "12      0.1        0.991209         0.989726      0.996552       0.993127   \n",
       "13      0.1        0.991209         0.986348      1.000000       0.993127   \n",
       "14      0.1        0.991209         0.989761      0.996564       0.993151   \n",
       "15      0.1        0.986813         0.982818      0.996516       0.989619   \n",
       "16      0.1        0.995604         0.996587      0.996587       0.996587   \n",
       "17      0.1        0.991209         0.986395      1.000000       0.993151   \n",
       "18      0.1        0.989011         0.986348      0.996552       0.991424   \n",
       "19      0.1        0.993407         0.989399      1.000000       0.994671   \n",
       "20      0.1        0.984615         0.982877      0.993080       0.987952   \n",
       "21      1.0        0.993407         0.989796      1.000000       0.994872   \n",
       "22      0.1        0.997802         0.996610      1.000000       0.998302   \n",
       "23      0.1        0.991209         0.989691      0.996540       0.993103   \n",
       "24      0.1        0.993407         0.989474      1.000000       0.994709   \n",
       "25      0.1        0.989011         0.989510      0.992982       0.991243   \n",
       "26   1000.0        1.000000         1.000000      1.000000       1.000000   \n",
       "27      0.1        0.991209         0.986441      1.000000       0.993174   \n",
       "28      0.1        0.984615         0.982877      0.993080       0.987952   \n",
       "29      0.1        0.993407         0.989619      1.000000       0.994783   \n",
       "30      1.0        0.993407         0.989547      1.000000       0.994746   \n",
       "\n",
       "    train AUC  test accuracy  test precision  test recall  test F-score  \\\n",
       "                                                                          \n",
       "1    0.996354       0.964912        0.947368     1.000000      0.972973   \n",
       "2    0.996891       0.964912        0.959459     0.986111      0.972603   \n",
       "3    1.000000       0.964912        0.947368     1.000000      0.972973   \n",
       "4    0.998947       0.973684        0.972603     0.986111      0.979310   \n",
       "5    0.999191       0.973684        0.960000     1.000000      0.979592   \n",
       "6    0.999626       0.973684        0.960000     1.000000      0.979592   \n",
       "7    0.999834       0.956140        0.958904     0.972222      0.965517   \n",
       "8    0.996207       0.956140        0.946667     0.986111      0.965986   \n",
       "9    0.998204       0.973684        0.960000     1.000000      0.979592   \n",
       "10   0.996798       0.982456        0.986111     0.986111      0.986111   \n",
       "11   0.996623       0.964912        0.959459     0.986111      0.972603   \n",
       "12   0.999561       0.982456        0.972973     1.000000      0.986301   \n",
       "13   0.997749       0.964912        0.959459     0.986111      0.972603   \n",
       "14   0.998910       0.973684        0.960000     1.000000      0.979592   \n",
       "15   0.996453       0.982456        0.972973     1.000000      0.986301   \n",
       "16   0.999684       0.973684        0.960000     1.000000      0.979592   \n",
       "17   0.999018       0.982456        0.986111     0.986111      0.986111   \n",
       "18   0.999666       0.982456        0.986111     0.986111      0.986111   \n",
       "19   0.999327       0.964912        0.972222     0.972222      0.972222   \n",
       "20   0.996581       0.991228        0.986301     1.000000      0.993103   \n",
       "21   0.999644       0.991228        0.986301     1.000000      0.993103   \n",
       "22   0.998056       0.956140        0.946667     0.986111      0.965986   \n",
       "23   0.996957       0.956140        0.958904     0.972222      0.965517   \n",
       "24   0.996905       0.956140        0.958904     0.972222      0.965517   \n",
       "25   0.999422       0.982456        0.972973     1.000000      0.986301   \n",
       "26   1.000000       0.973684        0.985915     0.972222      0.979021   \n",
       "27   0.996962       0.973684        0.960000     1.000000      0.979592   \n",
       "28   0.996290       0.982456        0.972973     1.000000      0.986301   \n",
       "29   0.996772       0.956140        0.958904     0.972222      0.965517   \n",
       "30   0.999815       0.964912        0.985714     0.958333      0.971831   \n",
       "\n",
       "    test AUC  \n",
       "              \n",
       "1   0.997024  \n",
       "2   0.992725  \n",
       "3   0.997354  \n",
       "4   0.986442  \n",
       "5   0.986111  \n",
       "6   0.985780  \n",
       "7   0.997354  \n",
       "8   0.997685  \n",
       "9   1.000000  \n",
       "10  0.997685  \n",
       "11  0.991733  \n",
       "12  0.989418  \n",
       "13  0.997685  \n",
       "14  0.984788  \n",
       "15  0.994378  \n",
       "16  0.985780  \n",
       "17  0.990410  \n",
       "18  0.989418  \n",
       "19  0.986442  \n",
       "20  0.997024  \n",
       "21  0.999008  \n",
       "22  0.997354  \n",
       "23  0.996693  \n",
       "24  0.992063  \n",
       "25  0.984788  \n",
       "26  0.999008  \n",
       "27  0.997685  \n",
       "28  0.997685  \n",
       "29  0.993056  \n",
       "30  0.995701  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_svm_semi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>train precision</th>\n",
       "      <th>train recall</th>\n",
       "      <th>train F-score</th>\n",
       "      <th>train AUC</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>test precision</th>\n",
       "      <th>test recall</th>\n",
       "      <th>test F-score</th>\n",
       "      <th>test AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.992308</td>\n",
       "      <td>0.990591</td>\n",
       "      <td>0.997339</td>\n",
       "      <td>0.993948</td>\n",
       "      <td>0.998215</td>\n",
       "      <td>0.971345</td>\n",
       "      <td>0.966712</td>\n",
       "      <td>0.988889</td>\n",
       "      <td>0.977583</td>\n",
       "      <td>0.993276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train accuracy  train precision  train recall  train F-score  train AUC  \\\n",
       "0        0.992308         0.990591      0.997339       0.993948   0.998215   \n",
       "\n",
       "   test accuracy  test precision  test recall  test F-score  test AUC  \n",
       "0       0.971345        0.966712     0.988889      0.977583  0.993276  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_svm_semi_avg = pd.DataFrame(result_svm_semi.mean(axis=0)).T\n",
    "part1_result = pd.concat([part1_result,result_svm_semi_avg.loc[:, 'train accuracy':]], ignore_index=True )\n",
    "result_svm_semi_avg.loc[:, 'train accuracy':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration index number:  30 \n",
      "\n",
      "train data\n",
      "\n",
      "[[168   3]\n",
      " [  0 284]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data\n",
      "\n",
      "[[41  1]\n",
      " [ 3 69]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('iteration index number: ', num, '\\n')\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    cm = confusion_matrix(y_true[i], pd.DataFrame(pred[i]))\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_true[i], pred_decision[i])\n",
    "    #print('fpr', fpr)\n",
    "    #print('tpr', tpr)\n",
    "    #print('threshold', threshold)\n",
    "    if i == 0:\n",
    "        print('train data\\n')\n",
    "    else :\n",
    "        print('test data\\n')\n",
    "    print(cm, '\\n')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iii. Unsupervised Learning: Run k-means algorithm on the whole training set. Ignore the labels of the data, and assume k=2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Run the k-means algorithm multiple times. Make sure that you initialize the algorithm randomly. How do you make sure that the algorithm was not trapped in a local minimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer : Whenever I train it, I set initial point randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeProbaScore(transformed_x, changed):\n",
    "    sum_arr = np.sum(transformed_x, axis=1)\n",
    "    for i in range(len(transformed_x)):\n",
    "        transformed_x[i][0] = transformed_x[i][0]/sum_arr[i]\n",
    "        transformed_x[i][1] = transformed_x[i][1]/sum_arr[i]\n",
    "    #print('original:', transformed_x)\n",
    "    if changed :\n",
    "        transformed_x = transformed_x[:,1]\n",
    "    else :\n",
    "        transformed_x = transformed_x[:,0]\n",
    "    #print('swithced?', changed)\n",
    "    #print('final proba score:', transformed_x)\n",
    "    return transformed_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTrueLabel(kmeans, trueLabel):\n",
    "    switch = 0\n",
    "    \n",
    "    # find the closest one from each center\n",
    "    transformed_distance = kmeans.transform(x_train)\n",
    "    transformed_distance = pd.DataFrame(transformed_distance)\n",
    "    transformed_distance['pred label'] = kmeans.labels_\n",
    "    transformed_distance['true label'] = trueLabel\n",
    "    \n",
    "    # closest 30\n",
    "    closest_30_0 = transformed_distance.sort_values(by=0).iloc[0:30]\n",
    "    closest_30_1 = transformed_distance.sort_values(by=1).iloc[0:30]\n",
    "    \n",
    "    # compare with the true label\n",
    "    true_max_value_0 = closest_30_0.groupby('true label').count().iloc[:,1].index[0]\n",
    "    true_max_value_1 = closest_30_1.groupby('true label').count().iloc[:,1].index[0]\n",
    "    \n",
    "    # check if the label names should be changed\n",
    "    if true_max_value_1 != 1:\n",
    "        switched = 1\n",
    "\n",
    "    return transformed_distance, switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterNum = 30\n",
    "result_kmeans = []\n",
    "y_true = []\n",
    "pred = []\n",
    "pred_decision = []\n",
    "accuracy = 0.8\n",
    "num = 0\n",
    "\n",
    "for i in range(iterNum):\n",
    "    #split training and test set(train:test = 8:2)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df.loc[:,2:], df.loc[:,1], test_size=0.2, stratify=df.loc[:,1])\n",
    "    # Normalization(Standardization)\n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    x_test = StandardScaler().fit_transform(x_test)\n",
    "    # train with training set\n",
    "    kmeans = KMeans(n_clusters=2, init='random').fit(x_train)\n",
    "    # label clusters with true value\n",
    "    train_transformed, switched = findTrueLabel(kmeans, y_train.values)\n",
    "    \n",
    "    #prediction\n",
    "    train_pred = train_transformed['pred label']\n",
    "    test_pred = kmeans.predict(x_test)\n",
    "    \n",
    "    #label with true value\n",
    "    if switched :\n",
    "        train_transformed['pred label'].replace({0: max_value_0, 1: max_value_1}, inplace = True)\n",
    "        test_pred = np.where(test_pred== 1, 0, 1)\n",
    "        \n",
    "    #make probability prediction\n",
    "    train_pred_prob = computeProbaScore(kmeans.transform(x_train), switched)\n",
    "    test_pred_prob = computeProbaScore(kmeans.transform(x_test), switched)\n",
    "    \n",
    "\n",
    "    #accuracy score (##work on test accuracy)\n",
    "    train_accuracy = accuracy_score(y_train, train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, test_pred)\n",
    "    \n",
    "    #precision, recall, F-score\n",
    "    train_precision, train_recall, train_fscore, train_support = precision_recall_fscore_support(y_train, train_pred, average='binary')\n",
    "    test_precision, test_recall, test_fscore, test_support = precision_recall_fscore_support(y_test, test_pred, average='binary')\n",
    "    \n",
    "    #auc\n",
    "    train_auc = roc_auc_score(y_train, train_pred_prob)\n",
    "    test_auc = roc_auc_score(y_test, test_pred_prob)\n",
    "    \n",
    "    result_kmeans.append([i+1, train_accuracy, train_precision, train_recall, train_fscore, train_auc,\n",
    "                         test_accuracy, test_precision, test_recall, test_fscore, test_auc])\n",
    "    \n",
    "    if accuracy < train_accuracy:\n",
    "        accuracy = train_accuracy\n",
    "        y_true = [y_train, y_test]\n",
    "        pred = [train_pred, test_pred]\n",
    "        pred_decision = [train_pred_prob, test_pred_prob]\n",
    "        num = i+1\n",
    "        \n",
    "result_kmeans = pd.DataFrame(result_kmeans, columns = ['', 'train accuracy', 'train precision', 'train recall',\n",
    "                                                       'train F-score', 'train AUC','test accuracy', 'test precision',\n",
    "                                                       'test recall', 'test F-score', 'test AUC'])\n",
    "result_kmeans.set_index('', inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Compute the centers of the two clusters and find the closest 30 data points to each center. Read the true labels of those 30 data points and take a majority poll within them. The majority poll becomes the label predicted by k-means for the members of each cluster. Then compare the labels provided by k-means with the true labels of the training data and report the average accuracy, precision, recall, F-score, and AUC over M runs, and ROC and the confusion matrix for one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_kmeans_avg = pd.DataFrame(result_kmeans.mean(axis=0)).T\n",
    "part1_result = pd.concat([part1_result,result_kmeans_avg], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>train precision</th>\n",
       "      <th>train recall</th>\n",
       "      <th>train F-score</th>\n",
       "      <th>train AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.896703</td>\n",
       "      <td>0.896667</td>\n",
       "      <td>0.943860</td>\n",
       "      <td>0.919658</td>\n",
       "      <td>0.964974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.912088</td>\n",
       "      <td>0.918089</td>\n",
       "      <td>0.943860</td>\n",
       "      <td>0.930796</td>\n",
       "      <td>0.971393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.912088</td>\n",
       "      <td>0.899023</td>\n",
       "      <td>0.968421</td>\n",
       "      <td>0.932432</td>\n",
       "      <td>0.970093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.101099</td>\n",
       "      <td>0.097403</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.068337</td>\n",
       "      <td>0.031001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.912088</td>\n",
       "      <td>0.899023</td>\n",
       "      <td>0.968421</td>\n",
       "      <td>0.932432</td>\n",
       "      <td>0.973602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.083516</td>\n",
       "      <td>0.047945</td>\n",
       "      <td>0.024561</td>\n",
       "      <td>0.032483</td>\n",
       "      <td>0.030815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.899351</td>\n",
       "      <td>0.971930</td>\n",
       "      <td>0.934233</td>\n",
       "      <td>0.970423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.903297</td>\n",
       "      <td>0.911263</td>\n",
       "      <td>0.936842</td>\n",
       "      <td>0.923875</td>\n",
       "      <td>0.974819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.885621</td>\n",
       "      <td>0.950877</td>\n",
       "      <td>0.917090</td>\n",
       "      <td>0.962105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.898901</td>\n",
       "      <td>0.902357</td>\n",
       "      <td>0.940351</td>\n",
       "      <td>0.920962</td>\n",
       "      <td>0.968524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.905495</td>\n",
       "      <td>0.908784</td>\n",
       "      <td>0.943860</td>\n",
       "      <td>0.925990</td>\n",
       "      <td>0.971992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.903297</td>\n",
       "      <td>0.908475</td>\n",
       "      <td>0.940351</td>\n",
       "      <td>0.924138</td>\n",
       "      <td>0.970691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.090110</td>\n",
       "      <td>0.089172</td>\n",
       "      <td>0.049123</td>\n",
       "      <td>0.063348</td>\n",
       "      <td>0.031146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.909890</td>\n",
       "      <td>0.903974</td>\n",
       "      <td>0.957895</td>\n",
       "      <td>0.930153</td>\n",
       "      <td>0.968566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.046980</td>\n",
       "      <td>0.024561</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.916484</td>\n",
       "      <td>0.902280</td>\n",
       "      <td>0.971930</td>\n",
       "      <td>0.935811</td>\n",
       "      <td>0.971228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.098901</td>\n",
       "      <td>0.086093</td>\n",
       "      <td>0.045614</td>\n",
       "      <td>0.059633</td>\n",
       "      <td>0.029329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.105495</td>\n",
       "      <td>0.098684</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.068650</td>\n",
       "      <td>0.032673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.905495</td>\n",
       "      <td>0.895425</td>\n",
       "      <td>0.961404</td>\n",
       "      <td>0.927242</td>\n",
       "      <td>0.970175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.905495</td>\n",
       "      <td>0.906040</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.926244</td>\n",
       "      <td>0.968586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.906355</td>\n",
       "      <td>0.950877</td>\n",
       "      <td>0.928082</td>\n",
       "      <td>0.969701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.918681</td>\n",
       "      <td>0.905229</td>\n",
       "      <td>0.971930</td>\n",
       "      <td>0.937394</td>\n",
       "      <td>0.969577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.901099</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.936842</td>\n",
       "      <td>0.922280</td>\n",
       "      <td>0.970939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.083516</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.028070</td>\n",
       "      <td>0.036952</td>\n",
       "      <td>0.023282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.909890</td>\n",
       "      <td>0.898693</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.930626</td>\n",
       "      <td>0.973292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.898901</td>\n",
       "      <td>0.902357</td>\n",
       "      <td>0.940351</td>\n",
       "      <td>0.920962</td>\n",
       "      <td>0.967843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.081319</td>\n",
       "      <td>0.081761</td>\n",
       "      <td>0.045614</td>\n",
       "      <td>0.058559</td>\n",
       "      <td>0.023653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.911864</td>\n",
       "      <td>0.943860</td>\n",
       "      <td>0.927586</td>\n",
       "      <td>0.970196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>0.049123</td>\n",
       "      <td>0.063205</td>\n",
       "      <td>0.031166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.909890</td>\n",
       "      <td>0.914966</td>\n",
       "      <td>0.943860</td>\n",
       "      <td>0.929188</td>\n",
       "      <td>0.972817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train accuracy  train precision  train recall  train F-score  train AUC\n",
       "                                                                           \n",
       "1         0.896703         0.896667      0.943860       0.919658   0.964974\n",
       "2         0.912088         0.918089      0.943860       0.930796   0.971393\n",
       "3         0.912088         0.899023      0.968421       0.932432   0.970093\n",
       "4         0.101099         0.097403      0.052632       0.068337   0.031001\n",
       "5         0.912088         0.899023      0.968421       0.932432   0.973602\n",
       "6         0.083516         0.047945      0.024561       0.032483   0.030815\n",
       "7         0.914286         0.899351      0.971930       0.934233   0.970423\n",
       "8         0.903297         0.911263      0.936842       0.923875   0.974819\n",
       "9         0.892308         0.885621      0.950877       0.917090   0.962105\n",
       "10        0.898901         0.902357      0.940351       0.920962   0.968524\n",
       "11        0.905495         0.908784      0.943860       0.925990   0.971992\n",
       "12        0.903297         0.908475      0.940351       0.924138   0.970691\n",
       "13        0.090110         0.089172      0.049123       0.063348   0.031146\n",
       "14        0.909890         0.903974      0.957895       0.930153   0.968566\n",
       "15        0.076923         0.046980      0.024561       0.032258   0.025800\n",
       "16        0.916484         0.902280      0.971930       0.935811   0.971228\n",
       "17        0.098901         0.086093      0.045614       0.059633   0.029329\n",
       "18        0.105495         0.098684      0.052632       0.068650   0.032673\n",
       "19        0.905495         0.895425      0.961404       0.927242   0.970175\n",
       "20        0.905495         0.906040      0.947368       0.926244   0.968586\n",
       "21        0.907692         0.906355      0.950877       0.928082   0.969701\n",
       "22        0.918681         0.905229      0.971930       0.937394   0.969577\n",
       "23        0.901099         0.908163      0.936842       0.922280   0.970939\n",
       "24        0.083516         0.054054      0.028070       0.036952   0.023282\n",
       "25        0.909890         0.898693      0.964912       0.930626   0.973292\n",
       "26        0.898901         0.902357      0.940351       0.920962   0.967843\n",
       "27        0.081319         0.081761      0.045614       0.058559   0.023653\n",
       "28        0.907692         0.911864      0.943860       0.927586   0.970196\n",
       "29        0.087912         0.088608      0.049123       0.063205   0.031166\n",
       "30        0.909890         0.914966      0.943860       0.929188   0.972817"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_kmeans.loc[:, :'train AUC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>train precision</th>\n",
       "      <th>train recall</th>\n",
       "      <th>train F-score</th>\n",
       "      <th>train AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.661685</td>\n",
       "      <td>0.655823</td>\n",
       "      <td>0.679064</td>\n",
       "      <td>0.665353</td>\n",
       "      <td>0.68768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train accuracy  train precision  train recall  train F-score  train AUC\n",
       "0        0.661685         0.655823      0.679064       0.665353    0.68768"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_kmeans_avg.loc[:, :'train AUC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration index number:  22 \n",
      "\n",
      "train data\n",
      "\n",
      "[[141  29]\n",
      " [  8 277]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl4lOW5x/HvnbAm7CRsISFA2AMKxuAGIiCyCdVqi1ul9ZRuHnv0VMG1LrX12KO0PbUqtiraWm1RNAJqW6sQFQyoGCCKsidsYQ1L9sxz/phEQgxkCJNZf5/ryuW8M08m92uSH0/e5X7MOYeIiESWmGAXICIi/qdwFxGJQAp3EZEIpHAXEYlACncRkQikcBcRiUAKdxGRCKRwFxGJQAp3EZEI1CxYXzghIcGlpqYG68uLiISljz76aK9zLrGhcUEL99TUVFatWhWsLy8iEpbMbKsv43RYRkQkAincRUQikMJdRCQCKdxFRCKQwl1EJAI1GO5m9rSZFZrZ2hO8bmb2OzPbYGa5ZjbC/2WKiMip8GXm/iww8SSvTwL6VX/MAh4//bLEL/JzIPsR739FJDQE6PeywevcnXPLzCz1JEOmA88573p9K8ysg5l1d87t9FONcjL5ObAlG1JHQXLm8c8/Mwk8lYBBt6HQsl3QyhQRqCotIqZwHeYcNGsF12cd/3vrR/64iSkJyK+1XVD93NfC3cxm4Z3dk5KS4ocvHSUaE+D7NlQ/D+DgSKHCXSSIikoqOLpnN92dx/tEVbn39zqEw93qea7eVbedc/OAeQAZGRlamdsX+TnwzGTwVIDFQNf0xgX4wEkw9TcBKVlEjikqqeBXSz7jxfX5TO6wjf+ruJdYTwXEtvBO2JqIP8K9AEiutd0T2OGH941ONbP00kOwKxeax3mDHcB5oLTItwDPz4Fnp0BVBcQ2hzOuDkz9IvKVKo/jm49/wKY9R/jBhX24efxEYnedVf9f4n7mj3DPAm40sxeBkUCRjrefgtqHXKDWYZYTuOAWyJh57HNPFODJmTBzcUB+iETkeAeOltMhrjmxMcbPJgygR4dWDOvZwfticmZAfh8bDHcz+yswBkgwswLg50BzAOfcE8ASYDKwASgGvttUxUacuodcWnU4ebATAyX7jm02FOAB+iESES/nHK+u3s59r+cxe+JArspMYWJ6t6DU4svVMlc18LoDfuK3iqLJluzjD7lUldc/Lqa59/X6jtEpwEVCwo6DJdy5cA3vrN/D8JQOZPTqGNR6gtbyN+rl50BRPt7z0Q5iW8KEB+GNW72HWSwGepwJw78DXQfr8IpICHtt9XbuXLiWKo/jnqmDuf68VGJj6rvWJHAU7sGw6llYfLN3Nv4V5w3xEx1mUaiLhKz2rZtzZnIHfnX5UJI7xQW7HEDhHlj5OfDpC95wr3u1qKfKG+qj/ltBLhLiKqs8/Om9zVRUebhxbD/GDOjChf0TMQvubL02hXsg1IT6R8+Bq6pngDX5Na8i4h95Ow4x++Vc1mwvYsqw7jjnMLOQCnZQuDetBkMdwCDju3DGVZqxi4Swssoqfv/vDTz+7kY6xDXnD9eMYFJ6t5AL9RoKd3+p2yLguNYAJ2CxMOXRY9eti0jI2rK3mCeWbmTamT24e8pgOsa3CHZJJ6Vw94f6WgQcKTxxsMc0gxHf0WxdJMQdLavkn3m7+cbwJAZ0a8vbt4whpXNonDBtiMK9sWrP1Oter15aVP/nKNRFwkb2l3u4/ZU1bD9YQnpSO9K6tA2bYAeFe+PUnam36X786xfc4r2ssaY1QEysQl0kTBQVV/Dgkjz+tqqAPgnxvDTrXNK6tA12WadM4d4YdWfqFUdrvVjdIkC9XUTCTpXH8c0nPmDz3qP8eExfbhrXj1bNY4NdVqMo3H1R92Rp6iiOu7N0/H3w5hxv+4DalzSqNYBIWNh/tJwOrb2Nvm69ZABJHVqTntQ+2GWdFoV7Q+o7WQocuwmp+s7S67M0SxcJM845Xvl4O/cv8jb6unpkCpcMCU6jL39TuJ9Mfg68+6uTnyzVnaUiYangQDF3LFzLsi/2cFavjmT27hTskvxK4V6fk918VHOydP60rx+GEZGwsPCTAu5auBYH3DdtCNed04uYIDf68jeFe131NvWqUetkqQ7DiIStTvEtOSu1E7+8LJ2eHcPn8sZToXCvLT8HFv0X9S8Ba9CspU6WioShiioPT2VvorLKcdO4flzYP5HR/RJCtnWAPyjca9uSTb3BrpuPRMLW2u1FzH45l3U7DnHpGT1CttGXvynca6t9iSN4e7+cdb1CXSQMlVZU8bu3v+TJZZvoGNeCJ64dwcT07g1/YoRQuNeWnAndhnr7wgycrFAXCWNb9xXzVPYmLh+exF1TBtM+rnmwSwoohXtdLdt5P6bODXYlInKKjpZV8ta6XVw+oicDurXl3/89JmRWRgo0hbuIRISlX+zhjlfWsKOohGE925PWpW3UBjso3I+paTFweKe3VW9+jg7JiISBA0fLeWBxHq98vJ2+ifH8/Qfh2ejL3xTucHyLgRrzp3mvZVfAi4SsmkZfW/cVc+NFadw4Ni1sG335W/SG+4n6sdeoKvc+r3AXCTn7jpTRMa4FsTHGnIkDSerYmiE9wrvRl79FZ7g31I+dGLUVEAlBzjn+/lEBv1iUx+xJA7lmZC8mREijL3+LvnCvrxnYcf3YDfqOgTG3a9YuEkLy9xdzx8I1ZH+5l8zUTpzbp3OwSwpp0RXu9R1bBzhrJnz45LFGYAp2kZDyyscF3PXqWgx44BvpXJOZEnGNvvwtusK9vmPrxECrdmoEJhLCEtq0JLN3Jx68bChJHVoHu5ywEF3hnjrKe4y9puOjxXhXUqoJdIW6SEioqPLw5NKNVHngp+P7Mbp/IqP7Jwa7rLASXeGenOldSam0yNuXvWSfZuoiIWbt9iJuXZDLZzsPMf3MY42+5NT4FO5mNhH4LRAL/NE591Cd11OA+UCH6jFznHNL/Fzr6cvP8faNAe+CGwp1kZBRWlHFb/71JU9lb6JTfAuevO6siFnyLhgaDHcziwUeAy4GCoCVZpblnMurNewu4G/OucfNbDCwBEhtgnobp2ZlpVXzgepDMs9OhZmLFPAiIWLb/mL+9N4mrhjRkzsmD4q6Rl/+5svMPRPY4JzbBGBmLwLTgdrh7oB21Y/bAzv8WWSjnWy5PN2kJBJ0h0sreHPtLq7MSKZ/17a887MxEbsyUqD5Eu5JQH6t7QJgZJ0x9wL/MLP/BOKB8X6p7nSc6LLHGhajm5REguidzwu5c+Eadh0qZXhKB9K6tFWw+1GMD2PqO5NRd7miq4BnnXM9gcnA82b2tfc2s1lmtsrMVu3Zs+fUq/VV3RuVvlZILEx5VLN2kSDYf7Scm19azXefXUl8y2Ys+NF5avTVBHyZuRcAybW2e/L1wy43ABMBnHPLzawVkAAU1h7knJsHzAPIyMiob6HS03fCGbtBTKyWyxMJoiqP44rHP2Db/mJuGtePn1zUl5bN1OirKfgS7iuBfmbWG9gOzACurjNmGzAOeNbMBgGtgCacmp9EvTcqGfS9SHeeigTJnsNldI73Nvq6Y/Igkjq2ZlD3dg1/ojRag4dlnHOVwI3AW8BneK+KWWdm95vZtOph/w1838w+Bf4KzHTONc3MvCE1NyrVsBho1krBLhIEzjleWrmNsY+8yws52wAYP7irgj0AfLrOvfqa9SV1nrun1uM84Hz/ltZIulFJJCRs21fMnFdy+WDjPkb27sQFaQnBLimqROYdqjXroGbMDHYlIlFpwUcF3P3qWmJjjAcvS+eqs9XoK9AiM9xFJKi6tmvJeX0784vL0uneXo2+gkHhLiKnrbzSw+PvbsTjHDdf3J9R/RIZ1U+NvoJJ4S4ip+XT/IPctiCX9bsPc/nwJDX6ChEKdxFplJLyKh7953r+9N5murRtxR+/k8H4wV2DXZZUU7iLSKPkHyhm/gdbmZGZwpxJA2nXSo2+QonCXUR8dqi60de3qht9vXvrGHpoZaSQpHAXEZ/8+/Pd3PHKWgoPlzIipSNpXdoo2EOYwl1ETmrfkTLuX5THa6t3MKBrW5647izSurQJdlnSAIW7iJxQlcdx5RPLyT9QzM3j+/OjMX1p0cyXZrISbJEV7vk53sZhh3eCp9K7rbYDIqes8HApCfEtiY0x7pwyiJ4d4xjQTW15w0nk/BNc0+r37fth/0Y4uBXmT/M+LyI+8Xgcf/lwK2P/dyl/qW70NW5QVwV7GIqcmXt9rX61lJ6Iz7bsPcqcV3JZsWk/5/XtzIW6wzSsRU6417T6ddULYFsMxLbQUnoiPvjbqnzufnUtLWJjeOjyoXz77GTdZRrmIifc1epXpNGSOrRmdP9EHpieTrf2rYJdjvhB5IQ7qNWviI/KKqv4wzsbcc5xy4QBnJ+WwPnqtx5RIivcRaRBn2w7wOyXc/li9xG+OaKnGn1FKIW7SJQoLq/kkX98wdPvb6Zbu1Y8PTODsQPV6CtSKdxFosT2AyU8v2Ir14xMYfbEgbRVo6+IpnAXiWBFJRW8sWYnMzJT6Ne1LUtvHaOVkaKEwl0kQv1j3S7uenUt+46Wk5HaibQubRTsUUThLhJh9h4p496sdSzK3cnAbm354/UZavQVhRTuIhGkyuO44vEP2HGwlJ9N6M8PLuxL89jI6TIivlO4i0SA3YdKSWzjbfT180uH0LNja/p1VT+YaKZ/0kXCmMfjeH7FVsY9spS/fLgVgIsGdlGwi2buIuFq054jzHllDTmb93NBWgJjBnQJdkkSQhTuImHopZXbuOe1dbRsFsPDVwzjyrN66i5TOY7CXSQM9ewYx5gB3kZfXdqp0Zd8ncJdJAyUVVbxf29vAOBnl6jRlzQsck6o5ufAvg3eD62+JBHko637mfzbbH7/zgYKD5finAt2SRIGImPmnp8DT08EV+XdfnYqzFykXu4S1o6WVfLrt9Yzf/kWerRvzfzvZXJhf62OJL7xaeZuZhPNbL2ZbTCzOScY8y0zyzOzdWb2gn/LbMCW7GPBDseW1xMJYzsOlvBCzja+c04v3rp5tIJdTkmDM3cziwUeAy4GCoCVZpblnMurNaYfcDtwvnPugJkF9pqs1FGAAdV/rmp5PQlTRcUVLF6zk6tHeht9Zd92EV11wlQawZfDMpnABufcJgAzexGYDuTVGvN94DHn3AEA51yhvws9qeRM6DYUjhTCwMlwxlU6JCNh5821u7j7tbXsP1rOyD6d6JvYRsEujeZLuCcB+bW2C4CRdcb0BzCz94FY4F7n3Jt138jMZgGzAFJSUhpT74nVLLE3da5/31ekiRUeLuXerHUsWbOLwd3b8czMs+mbqEZfcnp8Cff67oyoe7q+GdAPGAP0BLLNLN05d/C4T3JuHjAPICMjQ6f8JepVeRzfemI5O4pKufWSAcwa3UeNvsQvfAn3AiC51nZPYEc9Y1Y45yqAzWa2Hm/Yr/RLlSIRZmdRCV3btvI2+po2hOSOcWrLK37lyxRhJdDPzHqbWQtgBpBVZ8yrwEUAZpaA9zDNJn8WKhIJPB7Hs+9vZtwjS/lzTaOvAV0U7OJ3Dc7cnXOVZnYj8Bbe4+lPO+fWmdn9wCrnXFb1axPMLA+oAm51zu1rysJFws2GwiPMeTmXVVsPMLp/ImMHqtGXNB2fbmJyzi0BltR57p5ajx1wS/VHYOXneK9pP7wTPJXebV0pIyHmxZxt3JO1jtbNY3nkyjO4fESSGn1JkwrvO1Tzc+CZyeCpOPbc/GlwfZYCXkJKSuc4xg/qwn3T0kls2zLY5UgUCO9w35J9fLDDsbtTFe4SRKUVVfzu7S8BuG3iQM7rm8B5fdXoSwInvK+5Sh0FVmsXLEZ3p0rQrdqyn8m/y+YP725k/9FyNfqSoAjvmXtyJnRNh9IiuOAWKNnnDXbN2iUIjpRV8us3P+e5FVtJ6tCa576XyWj1g5EgCe9wh2N3pmbMDHYlEuV2FZXw4sp8rj83lVsvGUB8y/D/9ZLwpZ8+kdNw4Gg5i9bs5LpzepHWxdvoSysjSShQuIs0gnOON9bu4p7X1nKwuILz+namb2IbBbuEDIW7yCkqPFTK3a+t5a11uxma1J7nvjdSjb4k5CjcRU5Blcdx5ZPL2VVUyu2TBnLDBb1ppkZfEoIU7iI+2HGwhG7tvI2+7p+eTnLH1vTRbF1CmKYcIidR5XE8U6fR14X9ExXsEvI0cxc5gQ2Fh7ltQS4fbzvImAGJjBvUNdglifhM4S5Sjxc+3Ma9WeuIbxnL3G+fwTfOVKMvCS8Kd5F6pCbEMWFIV+6dNoSENmr0JeFH4S6Ct9HX3H99gWHMmaRGXxL+dEJVot6Hm/Yx6bfZPLl0E4dLK9ToSyKCZu4StQ6XVvA/b37On1dsI6VTHC/8x0jOS9NsXSKDwl2i1u5DZSz4qID/uKA3t0zoT1wL/TpI5NBPs0SV/UfLWZy7g+vOTSWtSxuybxurlZEkIincJSo451iUu5N7s9ZxqLSC89MS6JPYRsEuEUvhLhFv96FS7ly4ln99tpthPdvzlytG6g5TiXjhG+75Od61Ug/vBE+ld1srMEkdVR7Ht6obfd05eRDfPT9Vjb4kKoRnuOfnwDOTj18ce/40uD5LAS8AFBwopnv71sTGGA9MTyelUxypCfHBLkskYMJzCrMl+/hgB6gq9z4vUa3K4/hj9ibGP7qUP6/wNvoa3T9RwS5RJzxn7qmjwGLAebzbFgOxLbzPS9Rav+swt72cy6f5Bxk3sAsThqjRl0Sv8Az35Ezomg6lRXDBLVCyzxvsOiQTtf68Yiv3vb6Otq2a89sZZzLtjB5q9CVRLTzDHaBlO+9HxsxgVyJB5JzDzEjr0obJQ7tzz9TBdFajL5EwDneJaiXlVTz6z/XExBi3TxrEOX06c06fzsEuSyRkhOcJVYlqyzfuY+Jvl/FU9maKy6rU6EukHpq5S9g4VFrBr5Z8zl9zttGrcxwvfH+k2vKKnIBPM3czm2hm681sg5nNOcm4K8zMmVmG/0oU8So8VMarn2xn1ug+vPnT0Qp2kZNocOZuZrHAY8DFQAGw0syynHN5dca1BW4CPmyKQiU67TtSxuuf7mDm+b1J69KG92ZfpBOmIj7wZeaeCWxwzm1yzpUDLwLT6xn3APAwUOrH+iRKOed4bfV2xj+6lAeXfMamPUcAFOwiPvIl3JOA/FrbBdXPfcXMhgPJzrlFfqxNotSOgyXcMH8VP31xNb06x7P4plFq9CVyinw5oVrfnSBfXZ5gZjHAXGBmg29kNguYBZCSkuJbhRJVKqs8zJi3gj2Hy7h76mBmnpdKbIxuRhI5Vb6EewGQXGu7J7Cj1nZbIB14t/qOwG5AlplNc86tqv1Gzrl5wDyAjIwMXb8mX8nfX0yPDq1pFhvDLy8bSkqnOFI6xwW7LJGw5cthmZVAPzPrbWYtgBlAVs2Lzrki51yCcy7VOZcKrAC+Fuwi9ams8jBv2UbGP7qU55dvAeCCfgkKdpHT1ODM3TlXaWY3Am8BscDTzrl1ZnY/sMo5l3XydxCp32c7DzH75VxyC4q4eHBXJg3tHuySRCKGTzcxOeeWAEvqPHfPCcaOOf2yJNI9v3wL972eR/vWzfn91cOZMrS7Gn2J+JHuUJWAqmn01b9rWy49owd3Tx1Mp/gWwS5LJOIo3CUgissr+d+3vqBZrHHH5EGM7NOZkWr0JdJk1DhMmtz7G/ZyyW+W8fT7mymv9KjRl0gAaOYuTaaopIJfLv6Ml1bl0zshnr/94Fwye3cKdlkiUUHhLk1m75EyXs/dwQ8v7Mt/je9Hq+axwS5JJGoo3MWv9hz2Nvr63gW96ZvYhvdmj9UJU5EgULiLXzjneHX1du57PY/isiouGtiF3gnxCnaRIFG4y2nbfrCEOxeu4d31exiR0oGHrxhG74T4YJclEtUU7nJavI2+lrPvSDn3XjqY685Voy+RUKBwl0bZtq+YpI7eRl8PXT6MlE5xJHdSPxiRUKHr3OWUVFZ5ePzdjYyfu5Tnlm8B4Py0BAW7SIjRzF18tm5HEbNfzmXt9kNcMqQrU9ToSyRkKdzFJ/M/2MIDi/LoENeCx68ZoQ6OIiFO4S4nVdPoa2C3tkw/M4m7pw6iQ5wubxQJdQp3qdfRskp+/dZ6mscad04ZrEZfImFGJ1Tla5Z9sYcJc5cxf/kWKqqcGn2JhCHN3OUrRcUVPLA4jwUfFdAn0dvo6+xUNfoSCUcKd/nK3qNlvLFmJz8e05ebxqnRl0g4U7hHucLDpWSt3sF/jOrzVaOvjuoHIxL2FO5RyjnHyx9v54FFeZRUVDFuUFd6J8Qr2EUihMI9CuXvL+aOhWvI/nIvGb068tA31ehLJNIo3KNMZZWHq55awYGj5TwwfQjXjOxFjBp9iUQchXuU2LL3KMmd4mgWG8PDV3gbffXsqH4wIpFK17lHuIoqD4+9s4EJc5d91ejrvL4JCnaRCKeZewRbu72I2xbkkrfzEFOGdmfqsB7BLklEAkThHqGeeX8zv1j8GZ3iW/DEtWcxMb1bsEsSkQBSuEeYmkZfQ3q05/LhSdw1ZTDt45oHuywRCTCFe4Q4UlbJw29+TovYGO6aOpjM3p3I7K3WASLRSidUI8C76wu5ZO4ynl+xFQdq9CUiYThzz8+BLdlweCd4Kr3byZnBriooDhwt54HFebzy8XbSurRhwQ/P46xeHYNdloiEgPAK9/wceGYyeCqOPTd/GlyfFZUBf6C4nH+s281NY9P4ydg0WjZToy8R8fLpsIyZTTSz9Wa2wczm1PP6LWaWZ2a5Zva2mfXyf6l4Z+y1gx2gqtz7fJQoPFTKvGUbcc7RJ7EN788eyy0TBijYReQ4DYa7mcUCjwGTgMHAVWY2uM6wT4AM59wwYAHwsL8LBSB1FFitki0GYlt4n49wzjn+tjKfcY8u5ZF/fMGWfcUAuhJGROrly2GZTGCDc24TgJm9CEwH8moGOOfeqTV+BXCtP4v8SnImdE2H0iK44BYo2ecN9gg/JJO/v5jbX1nDexv2ktm7Ew9dPlSNvkTkpHwJ9yQgv9Z2ATDyJONvAN6o7wUzmwXMAkhJSfGxxDpatvN+ZMxs3OeHmZpGXweLK/jFN9K5OjNFjb5EpEG+hHt9SVLvtXZmdi2QAVxY3+vOuXnAPICMjAxdr3cSm/ceJaW60devrziDXp3j6NGhdbDLEpEw4csJ1QIgudZ2T2BH3UFmNh64E5jmnCvzT3nRp6LKw/+9/SWXzF3G/A+2AHBu384KdhE5Jb7M3FcC/cysN7AdmAFcXXuAmQ0HngQmOucK/V5llMgtOMhtC3L5fNdhLj2jB9POVKMvEWmcBsPdOVdpZjcCbwGxwNPOuXVmdj+wyjmXBfwaaAP83cwAtjnnpjVh3RHn6fc284vFeSS2bclT38ng4sFdg12SiIQxn25ics4tAZbUee6eWo/H+7muqFHT6GtYz/Z8++xk5kwaRPvWurxRRE5PeN2hGkEOl1bw0Buf07JZLPdcOpiM1E5kpKrRl4j4hxqHBcE7nxcyYe4y/pqzjWaxpkZfIuJ3mrkH0P6j5dz/+jpeXb2D/l3b8IdrzmN4ihp9iYj/KdwDqKikgrc/K+Sn4/rxk4vSaNFMfziJSNNQuDexXUWlvLp6Oz8Y3YfeCfG8N2esTpiKSJNTuDcR5xwvrsznl4s/o8LjYeKQbqQmxCvYRSQgFO5NYOu+o8x5eQ3LN+3jnD6deOjyYaSq0ZeIBJDC3c8qqzxc/dSHFJVU8MvLhjLj7GQ1+hKRgFO4+8nGPUfoVd3o65FveRt9dW+vfjAiEhy6XOM0lVd6+M2/vmDib5bx3PKtAJzTp7OCXUSCSjP307A6/yCzF+Syfvdhpp/Zg28MTwp2SSIigMK90f703mYeXJxHl7at+NP1GYwbpEZfIhI6FO6nqKbR15nJ7ZmRmcKcSQNp10qXN4pIaFG4++hQaQW/WvI5rZrH8PNLh3BWr06c1UuNvkQkNOmEqg/+lbebix9dyksrt9GiWYwafYlIyNPM/ST2HSnjvtfzyPp0BwO7tWXedRmckdwh2GWJiDRI4X4Sh0sreWd9ITeP78+PxvRVoy8RCRsK9zp2HCxh4Sfb+fGYvqQmxPP+nLE6YSoiYUfhXs3jcbyQs42H3vicKo9jytDupCbEK9hFJCwp3IHNe48y5+VcPty8n/PTOvOry4aR0jku2GWJiDRa1Id7ZZWHa//4IYdKK3j4m8O4MqMnZmr0JSLhLWrDfUPhYVI7x9MsNoa53z6TXp3j6NquVbDLEhHxi6i7/KOssopH//kFE3+TzfzqRl+ZvTsp2EUkokTVzP3jbQeYvSCXLwuPcPnwJC5Xoy8RiVBRE+5PLdvEL9/4jO7tWvHMd8/mogFdgl2SiEiTifhw93gcMTHGiF4duGZkCrMnDqStLm8UkQgXseFeVFLBg4vzaN08lvump6vRl4hElYg8ofrWul1c/OhSXv54O/Etm6nRl4hEnYiaue89UsbPX1vH4jU7Gdy9HU/PPJv0pPbBLktEJOAiKtyPlFaS/eUebr1kALNG96F5bET+YSIi0iCf0s/MJprZejPbYGZz6nm9pZm9VP36h2aW6u9CT2T7wRJ+/+8vcc6RmhDPB7eP4ycXpSnYRSSqNThzN7NY4DHgYqAAWGlmWc65vFrDbgAOOOfSzGwG8D/At5uiYMoOQWkRnq0f8pcdXXnojc/xOJg6rAepCfG0aRlRf4yIiDSKL9PbTGCDc26Tc64ceBGYXmfMdGB+9eMFwDhrigYt+Tmwaw3u4FYqn5nCwqyFjOjVkX/cPJrUhHi/fzkRkXDlyzQ3CcivtV0AjDzRGOdcpZkVAZ2Bvf4o8iuf/hWHw4DmVPBw/zz6zvwvNfoSEanDl5l7fclZ99pCX8ZgZrPMbJWZrdqzZ48v9X3tLWt/obTEeAW7iEg9fAn3AiC51nZPYMeJxphZM6A9sL/uGznn5jnnMpxzGYmJiade7RlXQ2wLwLDYFt5tERH5Gl8Oy6wE+plZb2A7MAOom6pZwPXAcuAK4N+uKe4cSs6EmYthSzakjvJui4jI1zQY7tXH0G8E3gJigaedc+vM7H5glXMAGcm9AAAD9UlEQVQuC/gT8LyZbcA7Y5/RZBUnZyrURUQa4NN1g865JcCSOs/dU+txKXClf0sTEZHG0p0+IiIRSOEuIhKBFO4iIhFI4S4iEoEU7iIiEciCtZCFme0Btjby0xPwd2uD0Kd9jg7a5+hwOvvcyznX4F2gQQv302Fmq5xzGcGuI5C0z9FB+xwdArHPOiwjIhKBFO4iIhEoXMN9XrALCALtc3TQPkeHJt/nsDzmLiIiJxeuM3cRETmJkA73UF6Yu6n4sM+3mFmemeWa2dtm1isYdfpTQ/tca9wVZubMLOyvrPBln83sW9Xf63Vm9kKga/Q3H362U8zsHTP7pPrne3Iw6vQXM3vazArNbO0JXjcz+131/49cMxvh1wKccyH5gbe98EagD9AC+BQYXGfMj4Enqh/PAF4Kdt0B2OeLgLjqxz+Khn2uHtcWWAasADKCXXcAvs/9gE+AjtXbXYJddwD2eR7wo+rHg4Etwa77NPd5NDACWHuC1ycDb+Bdye4c4EN/fv1QnrmHzsLcgdPgPjvn3nHOFVdvrsC7MlY48+X7DPAA8DBQGsjimogv+/x94DHn3AEA51xhgGv0N1/22QHtqh+35+srvoUV59wy6lmRrpbpwHPOawXQwcy6++vrh3K417cwd9KJxjjnKoGahbnDlS/7XNsNeP/lD2cN7rOZDQeSnXOLAllYE/Ll+9wf6G9m75vZCjObGLDqmoYv+3wvcK2ZFeBdP+I/A1Na0Jzq7/sp8WmxjiDx28LcYcTn/TGza4EM4MImrajpnXSfzSwGmAvMDFRBAeDL97kZ3kMzY/D+dZZtZunOuYNNXFtT8WWfrwKedc49Ymbn4l3dLd0552n68oKiSfMrlGfufluYO4z4ss+Y2XjgTmCac64sQLU1lYb2uS2QDrxrZlvwHpvMCvOTqr7+bL/mnKtwzm0G1uMN+3Dlyz7fAPwNwDm3HGiFtwdLpPLp972xQjncv1qY28xa4D1hmlVnTM3C3NCUC3MHToP7XH2I4km8wR7ux2GhgX12zhU55xKcc6nOuVS85xmmOedWBadcv/DlZ/tVvCfPMbMEvIdpNgW0Sv/yZZ+3AeMAzGwQ3nDfE9AqAysL+E71VTPnAEXOuZ1+e/dgn1Fu4GzzZOALvGfZ76x+7n68v9zg/eb/HdgA5AB9gl1zAPb5X8BuYHX1R1awa27qfa4z9l3C/GoZH7/PBjwK5AFrgBnBrjkA+zwYeB/vlTSrgQnBrvk09/evwE6gAu8s/Qbgh8APa32PH6v+/7HG3z/XukNVRCQChfJhGRERaSSFu4hIBFK4i4hEIIW7iEgEUriLiEQghbuISARSuIuIRCCFu4hIBPp/M1Ji0UqSH8oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('iteration index number: ', num, '\\n')\n",
    "i=0\n",
    "#for i in range(len(y_true)):\n",
    "cm = confusion_matrix(y_true[i], pd.DataFrame(pred[i]))\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_true[i], pred_decision[i])\n",
    "#print('fpr', fpr)\n",
    "#print('tpr', tpr)\n",
    "#print('threshold', threshold)\n",
    "if i == 0:\n",
    "    print('train data\\n')\n",
    "else :\n",
    "    print('test data\\n')\n",
    "print(cm, '\\n')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Classify test data based on their proximity to the centers of the clusters. Report the average accuracy, precision, recall, F-score, and AUC over M runs, and ROC and the confusion matrix for one of the runs for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>test precision</th>\n",
       "      <th>test recall</th>\n",
       "      <th>test F-score</th>\n",
       "      <th>test AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.983466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.962302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.981151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.061404</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.036036</td>\n",
       "      <td>0.027447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.903509</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.927152</td>\n",
       "      <td>0.962632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.023810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.931507</td>\n",
       "      <td>0.976190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.952712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.988757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.979167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.903509</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.927152</td>\n",
       "      <td>0.963294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.934211</td>\n",
       "      <td>0.964616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.023810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.885965</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.911565</td>\n",
       "      <td>0.969907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.026786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.971892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.096491</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.055046</td>\n",
       "      <td>0.043981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.035088</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.018188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.934211</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.976521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.907895</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.932432</td>\n",
       "      <td>0.975860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.903509</td>\n",
       "      <td>0.896104</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.926174</td>\n",
       "      <td>0.970899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.885965</td>\n",
       "      <td>0.883117</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.912752</td>\n",
       "      <td>0.976190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.911392</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953642</td>\n",
       "      <td>0.964616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.087719</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.043320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.964947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.922078</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.953020</td>\n",
       "      <td>0.978175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.114035</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.056075</td>\n",
       "      <td>0.041336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.903509</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.927152</td>\n",
       "      <td>0.976521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.026786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.903509</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.927152</td>\n",
       "      <td>0.960317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    test accuracy  test precision  test recall  test F-score  test AUC\n",
       "                                                                      \n",
       "1        0.938596        0.933333     0.972222      0.952381  0.983466\n",
       "2        0.894737        0.875000     0.972222      0.921053  0.962302\n",
       "3        0.912281        0.897436     0.972222      0.933333  0.981151\n",
       "4        0.061404        0.051282     0.027778      0.036036  0.027447\n",
       "5        0.903509        0.886076     0.972222      0.927152  0.962632\n",
       "6        0.122807        0.166667     0.097222      0.122807  0.023810\n",
       "7        0.912281        0.918919     0.944444      0.931507  0.976190\n",
       "8        0.921053        0.888889     1.000000      0.941176  0.952712\n",
       "9        0.964912        0.959459     0.986111      0.972603  0.988757\n",
       "10       0.929825        0.910256     0.986111      0.946667  0.979167\n",
       "11       0.903509        0.886076     0.972222      0.927152  0.963294\n",
       "12       0.912281        0.887500     0.986111      0.934211  0.964616\n",
       "13       0.105263        0.083333     0.041667      0.055556  0.023810\n",
       "14       0.885965        0.893333     0.930556      0.911565  0.969907\n",
       "15       0.105263        0.083333     0.041667      0.055556  0.026786\n",
       "16       0.894737        0.894737     0.944444      0.918919  0.971892\n",
       "17       0.096491        0.081081     0.041667      0.055046  0.043981\n",
       "18       0.035088        0.025000     0.013889      0.017857  0.018188\n",
       "19       0.947368        0.934211     0.986111      0.959459  0.976521\n",
       "20       0.912281        0.907895     0.958333      0.932432  0.975860\n",
       "21       0.903509        0.896104     0.958333      0.926174  0.970899\n",
       "22       0.885965        0.883117     0.944444      0.912752  0.976190\n",
       "23       0.938596        0.911392     1.000000      0.953642  0.964616\n",
       "24       0.087719        0.078947     0.041667      0.054545  0.043320\n",
       "25       0.894737        0.894737     0.944444      0.918919  0.964947\n",
       "26       0.938596        0.922078     0.986111      0.953020  0.978175\n",
       "27       0.114035        0.085714     0.041667      0.056075  0.041336\n",
       "28       0.903509        0.886076     0.972222      0.927152  0.976521\n",
       "29       0.105263        0.083333     0.041667      0.055556  0.026786\n",
       "30       0.903509        0.886076     0.972222      0.927152  0.960317"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_kmeans.loc[:, 'test accuracy':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>test precision</th>\n",
       "      <th>test recall</th>\n",
       "      <th>test F-score</th>\n",
       "      <th>test AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.667836</td>\n",
       "      <td>0.65638</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.671249</td>\n",
       "      <td>0.689187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test accuracy  test precision  test recall  test F-score  test AUC\n",
       "0       0.667836         0.65638     0.691667      0.671249  0.689187"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_kmeans_avg.loc[:, 'test accuracy':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration index number:  22 \n",
      "\n",
      "test data\n",
      "\n",
      "[[33  9]\n",
      " [ 4 68]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('iteration index number: ', num, '\\n')\n",
    "\n",
    "i=1\n",
    "#for i in range(len(y_true)):\n",
    "cm = confusion_matrix(y_true[i], pd.DataFrame(pred[i]))\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_true[i], pred_decision[i])\n",
    "#print('fpr', fpr)\n",
    "#print('tpr', tpr)\n",
    "#print('threshold', threshold)\n",
    "if i == 0:\n",
    "    print('train data\\n')\n",
    "else :\n",
    "    print('test data\\n')\n",
    "print(cm, '\\n')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iv. Spectral Clustering: Repeat 1(b)iii using spectral clustering, which is clustering based on kernels. Research what spectral clustering is. Use RBF kernel with gamma=1 or find a gamma for which the two clusters have the same balance as the one in original data set (if the positive class has p and the negative class has n sample, the two clusters must have p and n members). Do not label data based on their proximity to cluster center, because spectral clustering may give you non-convex clusters. Instead, use fit-predict method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer : Spectral clustering is a technique with roots in graph theory, where the approach is used to identify communities of nodes in a graph based on the edges connecting them. The method is flexible and allows us to cluster non graph data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findClass(pred, trueLabel):\n",
    "    temp = pd.DataFrame(list(zip(pred, trueLabel.values)), columns=['pred', 'true'])\n",
    "    temp_0 = temp[(temp['pred']==0)]\n",
    "    temp_1 = temp[(temp['pred']==1)]\n",
    "        \n",
    "    if len(temp_0) != 0:\n",
    "        lebel_0 = temp_0['true'].value_counts().nlargest(n=1).index[0]\n",
    "    else :\n",
    "        label_0 = -1\n",
    "        \n",
    "    if len(temp_1) != 0:\n",
    "        label_1 = temp_1['true'].value_counts().nlargest(n=1).index[0]\n",
    "    else :\n",
    "        label_1 = -1\n",
    "    \n",
    "    #print(temp_0['true'].value_counts().nlargest(n=1).index[0])\n",
    "    #print(temp_0['true'].value_counts().nlargest(n=1).index[0])\n",
    "    \n",
    "    return lebel_0, label_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probaScore(matrix):\n",
    "    total_dist = 0\n",
    "    for row in matrix:\n",
    "        total_dist += sum(row)\n",
    "    dist_row_02 = []\n",
    "    for row in matrix:\n",
    "        dist_row_02.append(1 - (sum(row)/total_dist))\n",
    "    return np.array(dist_row_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## spectral clustering\n",
    "\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "iterNum = 30\n",
    "result_spectral = []\n",
    "y_true = []\n",
    "pred = []\n",
    "pred_decision = []\n",
    "accuracy = 0.0\n",
    "num = 0\n",
    "switch = [0,0]\n",
    "\n",
    "for i in range(iterNum):\n",
    "\n",
    "    #split training and test set(train:test = 8:2)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df.loc[:,2:], df.loc[:,1], test_size=0.2, stratify=df.loc[:,1])\n",
    "\n",
    "    # Normalization(Standardization)\n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    x_test = StandardScaler().fit_transform(x_test)\n",
    "\n",
    "    # train with training set\n",
    "    clustering = SpectralClustering(n_clusters=2, n_init=1000, gamma=1, eigen_tol=1e-9)\n",
    "\n",
    "    # train prediction\n",
    "    train_pred = clustering.fit_predict(x_train)\n",
    "    # train  probability prediction\n",
    "    train_pred_prob = probaScore(clustering.affinity_matrix_)\n",
    "\n",
    "    # test prediction\n",
    "    test_pred = clustering.fit_predict(x_test)\n",
    "    # train  probability prediction\n",
    "    test_pred_prob = probaScore(clustering.affinity_matrix_)\n",
    "\n",
    "    # make sure the results has both classes\n",
    "    #train_exist_0 = np.any(train_pred == 0)\n",
    "    #train_exist_1 = np.any(train_pred == 1)\n",
    "    #if exist_0 == True and exist_1 == True:\n",
    "    #    iterNum = iterNum -1 \n",
    "    #else:\n",
    "    #    continue\n",
    "\n",
    "    #get label\n",
    "    train_label_0, train_label_1 = findClass(train_pred, y_train)\n",
    "    test_label_0, test_label_1 = findClass(test_pred, y_test)\n",
    "    if train_label_0 != 0 or train_label_1 !=0 :\n",
    "        switch[0] = 1\n",
    "    if test_label_0 != 0 or test_label_1 !=0 :\n",
    "        switch[1] = 1    \n",
    "\n",
    "    #label with true value\n",
    "    train_pred = np.where(train_pred==0, train_label_0, train_label_1)\n",
    "    test_pred = np.where(test_pred==0, test_label_0, test_label_1)\n",
    "\n",
    "    #accuracy score (##work on test accuracy)\n",
    "    train_accuracy = accuracy_score(y_train, train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, test_pred)\n",
    "\n",
    "    #precision, recall, F-score\n",
    "    train_precision, train_recall, train_fscore, train_support = precision_recall_fscore_support(y_train, train_pred, average='binary')\n",
    "    test_precision, test_recall, test_fscore, test_support = precision_recall_fscore_support(y_test, test_pred, average='binary')\n",
    "\n",
    "    #auc\n",
    "    train_auc = roc_auc_score(y_train, train_pred_prob)\n",
    "    test_auc = roc_auc_score(y_test, test_pred_prob)\n",
    "\n",
    "    result_spectral.append([i+1, train_accuracy, train_precision, train_recall, train_fscore, train_auc,\n",
    "                         test_accuracy, test_precision, test_recall, test_fscore, test_auc])\n",
    "\n",
    "    if accuracy < train_accuracy:\n",
    "        accuracy = train_accuracy\n",
    "        y_true = [y_train, y_test]\n",
    "        pred = [train_pred, test_pred]\n",
    "        pred_decision = [train_pred_prob, test_pred_prob]\n",
    "      \n",
    "result_spectral = pd.DataFrame(result_spectral, columns = ['', 'train accuracy', 'train precision', 'train recall',\n",
    "                                                           'train F-score', 'train AUC','test accuracy', 'test precision',\n",
    "                                                           'test recall', 'test F-score', 'test AUC'])\n",
    "\n",
    "result_spectral.set_index('', inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>train precision</th>\n",
       "      <th>train recall</th>\n",
       "      <th>train F-score</th>\n",
       "      <th>train AUC</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>test precision</th>\n",
       "      <th>test recall</th>\n",
       "      <th>test F-score</th>\n",
       "      <th>test AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.627753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.771313</td>\n",
       "      <td>0.227214</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778378</td>\n",
       "      <td>0.197751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.627753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.771313</td>\n",
       "      <td>0.232074</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778378</td>\n",
       "      <td>0.229993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.627753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.771313</td>\n",
       "      <td>0.215294</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778378</td>\n",
       "      <td>0.274471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.629139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.213333</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.236772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.626374</td>\n",
       "      <td>0.626374</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.770270</td>\n",
       "      <td>0.234035</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.205688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.626374</td>\n",
       "      <td>0.626374</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.770270</td>\n",
       "      <td>0.210031</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.286045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.629139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.231249</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778378</td>\n",
       "      <td>0.156746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.626374</td>\n",
       "      <td>0.626374</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.770270</td>\n",
       "      <td>0.231290</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.189815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.629139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.224830</td>\n",
       "      <td>0.649123</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.177579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.629139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.211889</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.229167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.627753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.771313</td>\n",
       "      <td>0.209236</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778378</td>\n",
       "      <td>0.308201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.627753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.771313</td>\n",
       "      <td>0.228029</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778378</td>\n",
       "      <td>0.218089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.627753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.771313</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778378</td>\n",
       "      <td>0.252811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.627753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.771313</td>\n",
       "      <td>0.196068</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778378</td>\n",
       "      <td>0.353175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.629139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.235243</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.200231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.629139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.214902</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778378</td>\n",
       "      <td>0.303241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.629139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.215418</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778378</td>\n",
       "      <td>0.259921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.626374</td>\n",
       "      <td>0.626374</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.770270</td>\n",
       "      <td>0.225098</td>\n",
       "      <td>0.649123</td>\n",
       "      <td>0.645455</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.780220</td>\n",
       "      <td>0.226521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.629139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.224396</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.230820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.629139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.223282</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778378</td>\n",
       "      <td>0.245205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.626374</td>\n",
       "      <td>0.626374</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.770270</td>\n",
       "      <td>0.239835</td>\n",
       "      <td>0.649123</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.629139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.189804</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.347222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.629139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.228070</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778378</td>\n",
       "      <td>0.180225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.627753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.771313</td>\n",
       "      <td>0.233705</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778378</td>\n",
       "      <td>0.195767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.629139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.214861</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.232474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.627753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.771313</td>\n",
       "      <td>0.233437</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778378</td>\n",
       "      <td>0.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.626374</td>\n",
       "      <td>0.626374</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.770270</td>\n",
       "      <td>0.223715</td>\n",
       "      <td>0.649123</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.193783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.629139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.219154</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.239749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.627753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.771313</td>\n",
       "      <td>0.211703</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778378</td>\n",
       "      <td>0.253968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.629139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.231703</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778378</td>\n",
       "      <td>0.181878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train accuracy  train precision  train recall  train F-score  train AUC  \\\n",
       "                                                                              \n",
       "1         0.628571         0.627753           1.0       0.771313   0.227214   \n",
       "2         0.628571         0.627753           1.0       0.771313   0.232074   \n",
       "3         0.628571         0.627753           1.0       0.771313   0.215294   \n",
       "4         0.630769         0.629139           1.0       0.772358   0.213333   \n",
       "5         0.626374         0.626374           1.0       0.770270   0.234035   \n",
       "6         0.626374         0.626374           1.0       0.770270   0.210031   \n",
       "7         0.630769         0.629139           1.0       0.772358   0.231249   \n",
       "8         0.626374         0.626374           1.0       0.770270   0.231290   \n",
       "9         0.630769         0.629139           1.0       0.772358   0.224830   \n",
       "10        0.630769         0.629139           1.0       0.772358   0.211889   \n",
       "11        0.628571         0.627753           1.0       0.771313   0.209236   \n",
       "12        0.628571         0.627753           1.0       0.771313   0.228029   \n",
       "13        0.628571         0.627753           1.0       0.771313   0.220000   \n",
       "14        0.628571         0.627753           1.0       0.771313   0.196068   \n",
       "15        0.630769         0.629139           1.0       0.772358   0.235243   \n",
       "16        0.630769         0.629139           1.0       0.772358   0.214902   \n",
       "17        0.630769         0.629139           1.0       0.772358   0.215418   \n",
       "18        0.626374         0.626374           1.0       0.770270   0.225098   \n",
       "19        0.630769         0.629139           1.0       0.772358   0.224396   \n",
       "20        0.630769         0.629139           1.0       0.772358   0.223282   \n",
       "21        0.626374         0.626374           1.0       0.770270   0.239835   \n",
       "22        0.630769         0.629139           1.0       0.772358   0.189804   \n",
       "23        0.630769         0.629139           1.0       0.772358   0.228070   \n",
       "24        0.628571         0.627753           1.0       0.771313   0.233705   \n",
       "25        0.630769         0.629139           1.0       0.772358   0.214861   \n",
       "26        0.628571         0.627753           1.0       0.771313   0.233437   \n",
       "27        0.626374         0.626374           1.0       0.770270   0.223715   \n",
       "28        0.630769         0.629139           1.0       0.772358   0.219154   \n",
       "29        0.628571         0.627753           1.0       0.771313   0.211703   \n",
       "30        0.630769         0.629139           1.0       0.772358   0.231703   \n",
       "\n",
       "    test accuracy  test precision  test recall  test F-score  test AUC  \n",
       "                                                                        \n",
       "1        0.640351        0.637168     1.000000      0.778378  0.197751  \n",
       "2        0.640351        0.637168     1.000000      0.778378  0.229993  \n",
       "3        0.640351        0.637168     1.000000      0.778378  0.274471  \n",
       "4        0.631579        0.631579     1.000000      0.774194  0.236772  \n",
       "5        0.631579        0.631579     1.000000      0.774194  0.205688  \n",
       "6        0.631579        0.631579     1.000000      0.774194  0.286045  \n",
       "7        0.640351        0.637168     1.000000      0.778378  0.156746  \n",
       "8        0.631579        0.631579     1.000000      0.774194  0.189815  \n",
       "9        0.649123        0.642857     1.000000      0.782609  0.177579  \n",
       "10       0.631579        0.631579     1.000000      0.774194  0.229167  \n",
       "11       0.640351        0.637168     1.000000      0.778378  0.308201  \n",
       "12       0.640351        0.637168     1.000000      0.778378  0.218089  \n",
       "13       0.640351        0.637168     1.000000      0.778378  0.252811  \n",
       "14       0.640351        0.637168     1.000000      0.778378  0.353175  \n",
       "15       0.771930        0.734694     1.000000      0.847059  0.200231  \n",
       "16       0.640351        0.637168     1.000000      0.778378  0.303241  \n",
       "17       0.640351        0.637168     1.000000      0.778378  0.259921  \n",
       "18       0.649123        0.645455     0.986111      0.780220  0.226521  \n",
       "19       0.631579        0.631579     1.000000      0.774194  0.230820  \n",
       "20       0.640351        0.637168     1.000000      0.778378  0.245205  \n",
       "21       0.649123        0.642857     1.000000      0.782609  0.208333  \n",
       "22       0.956140        0.958904     0.972222      0.965517  0.347222  \n",
       "23       0.640351        0.637168     1.000000      0.778378  0.180225  \n",
       "24       0.640351        0.637168     1.000000      0.778378  0.195767  \n",
       "25       0.631579        0.631579     1.000000      0.774194  0.232474  \n",
       "26       0.640351        0.637168     1.000000      0.778378  0.190476  \n",
       "27       0.649123        0.642857     1.000000      0.782609  0.193783  \n",
       "28       0.631579        0.631579     1.000000      0.774194  0.239749  \n",
       "29       0.640351        0.637168     1.000000      0.778378  0.253968  \n",
       "30       0.640351        0.637168     1.000000      0.778378  0.181878  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_spectral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>train precision</th>\n",
       "      <th>train recall</th>\n",
       "      <th>train F-score</th>\n",
       "      <th>train AUC</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>test precision</th>\n",
       "      <th>test recall</th>\n",
       "      <th>test F-score</th>\n",
       "      <th>test AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.629158</td>\n",
       "      <td>0.628124</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.771592</td>\n",
       "      <td>0.22163</td>\n",
       "      <td>0.654094</td>\n",
       "      <td>0.650498</td>\n",
       "      <td>0.998611</td>\n",
       "      <td>0.786274</td>\n",
       "      <td>0.233537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train accuracy  train precision  train recall  train F-score  train AUC  \\\n",
       "0        0.629158         0.628124           1.0       0.771592    0.22163   \n",
       "\n",
       "   test accuracy  test precision  test recall  test F-score  test AUC  \n",
       "0       0.654094        0.650498     0.998611      0.786274  0.233537  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectral_avg = pd.DataFrame(result_spectral.mean(axis=0)).T\n",
    "\n",
    "part1_result = pd.concat([part1_result,spectral_avg], ignore_index=True)\n",
    "\n",
    "spectral_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration index number:  0 \n",
      "\n",
      "switch 1\n",
      "train data\n",
      "\n",
      "[[  2 168]\n",
      " [  0 285]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "switch 1\n",
      "test data\n",
      "\n",
      "[[ 0 42]\n",
      " [ 0 72]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('iteration index number: ', num, '\\n')\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    cm = confusion_matrix(y_true[i], pd.DataFrame(pred[i]))\n",
    "    print('switch', switch[i])\n",
    "    if switch[i] == 1 :\n",
    "            fpr, tpr, threshold = metrics.roc_curve(1-y_true[i], pred_decision[i])\n",
    "    else:\n",
    "        fpr, tpr, threshold = metrics.roc_curve(y_true[i], pred_decision[i])\n",
    "    #print('fpr', fpr)\n",
    "    #print('tpr', tpr)\n",
    "    #print('threshold', threshold)\n",
    "    if i == 0:\n",
    "        print('train data\\n')\n",
    "    else :\n",
    "        print('test data\\n')\n",
    "    print(cm, '\\n')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v. One can expect that supervised learning on the full data set works better than semi-supervised learning with half of the data set labeled. One can expect that unsupervised learning underperforms in such situations. Compare the results you obtained by those methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>train precision</th>\n",
       "      <th>train recall</th>\n",
       "      <th>train F-score</th>\n",
       "      <th>train AUC</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>test precision</th>\n",
       "      <th>test recall</th>\n",
       "      <th>test F-score</th>\n",
       "      <th>test AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.985934</td>\n",
       "      <td>0.983587</td>\n",
       "      <td>0.994152</td>\n",
       "      <td>0.988834</td>\n",
       "      <td>0.997036</td>\n",
       "      <td>0.969591</td>\n",
       "      <td>0.969233</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.976108</td>\n",
       "      <td>0.993144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.992308</td>\n",
       "      <td>0.990591</td>\n",
       "      <td>0.997339</td>\n",
       "      <td>0.993948</td>\n",
       "      <td>0.998215</td>\n",
       "      <td>0.971345</td>\n",
       "      <td>0.966712</td>\n",
       "      <td>0.988889</td>\n",
       "      <td>0.977583</td>\n",
       "      <td>0.993276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.661685</td>\n",
       "      <td>0.655823</td>\n",
       "      <td>0.679064</td>\n",
       "      <td>0.665353</td>\n",
       "      <td>0.687680</td>\n",
       "      <td>0.667836</td>\n",
       "      <td>0.656380</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.671249</td>\n",
       "      <td>0.689187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.629158</td>\n",
       "      <td>0.628124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.771592</td>\n",
       "      <td>0.221630</td>\n",
       "      <td>0.654094</td>\n",
       "      <td>0.650498</td>\n",
       "      <td>0.998611</td>\n",
       "      <td>0.786274</td>\n",
       "      <td>0.233537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train accuracy  train precision  train recall  train F-score  train AUC  \\\n",
       "0        0.985934         0.983587      0.994152       0.988834   0.997036   \n",
       "1        0.992308         0.990591      0.997339       0.993948   0.998215   \n",
       "2        0.661685         0.655823      0.679064       0.665353   0.687680   \n",
       "3        0.629158         0.628124      1.000000       0.771592   0.221630   \n",
       "\n",
       "   test accuracy  test precision  test recall  test F-score  test AUC  \n",
       "0       0.969591        0.969233     0.983333      0.976108  0.993144  \n",
       "1       0.971345        0.966712     0.988889      0.977583  0.993276  \n",
       "2       0.667836        0.656380     0.691667      0.671249  0.689187  \n",
       "3       0.654094        0.650498     0.998611      0.786274  0.233537  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part1_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer : \n",
    "### 0: supervised, 1: semi-supervised, 2:k_means clustering, 3: spectralclustering\n",
    "\n",
    "### In terms of test accuracy, supervised and semi-supervised has higher accuracy, but k-means clustering  and spectral clustering has very lower accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Active Learning Using Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Download the banknote authentication Data Set. choose 472 data points randomly as the test set, and the remaining 900 points as the training set. This is a binary classification problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of test set :  472\n",
      "the number of training set :  900\n"
     ]
    }
   ],
   "source": [
    "filepath2 = 'https://raw.githubusercontent.com/seongohr/ML/master/data_banknote_authentication.txt'\n",
    "df2 = pd.read_csv(filepath2, sep=',', header=None)    \n",
    "\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(df2.loc[:,:3], df2.loc[:,4], test_size=0.344, stratify=df2.loc[:,4])\n",
    "\n",
    "print('the number of test set : ', len(x_test2))\n",
    "print('the number of training set : ', len(x_train2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)Repeat each of the following two procedure 50 times. You will have 50 errors for 90 SVMs per each procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i. Train a SVM with a pool of 10 randomly selected data points from the training set using linear kernel and L1 penalty. Select the penalty parameter using 10-fold cross validation. Repeat this process by adding 10 other randomly selected data points to the pool, until you use all the 900 points. Do NOT replace the samples back into the training set at each step. Calculate the test error for each SVM. You will have 90 SVMs that were trained using 10, 20, 30, ..., 900 data points and their 90 test errors. You have implemented passive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeRandomPools(x, y):\n",
    "    num = 90\n",
    "    el_num = 10\n",
    "    pools = []\n",
    "    start_0 = 0\n",
    "    start_1 = 0\n",
    "    concat = []\n",
    "    \n",
    "    df = pd.concat([x,y], axis=1)\n",
    "    df_0 = df[(df[4]==0)].sample(frac=1).reset_index(drop=True)\n",
    "    df_1 = df[(df[4]==1)].sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    q_0 = int(len(df_0)/num)\n",
    "    r_0 = len(df_0) - (q_0*num)\n",
    "    \n",
    "    #print('q_0:', q_0)\n",
    "    #print('r_0:', r_0)\n",
    "    \n",
    "    for i in range(num):\n",
    "        #print('r_0 : ', r_0)\n",
    "        if r_0 > 0:\n",
    "            end_0 = start_0 + (q_0 + 1)\n",
    "            end_1 = start_1 + (el_num - (q_0 + 1))\n",
    "            #print(i, 'end_1: ', end_1)\n",
    "        else: \n",
    "            end_0 = start_0 + q_0\n",
    "            end_1 = start_1 + (el_num - q_0)\n",
    "            #print(i, 'end_1: ', end_1)\n",
    "        \n",
    "        if i == num-1:\n",
    "            one_pool_0 = df_0.iloc[start_0:]\n",
    "            one_pool_1 = df_1.iloc[start_1:]\n",
    "        else:\n",
    "            one_pool_0 = df_0.iloc[start_0:end_0]\n",
    "            one_pool_1 = df_1.iloc[start_1:end_1]\n",
    "        \n",
    "        one_pool = pd.concat([one_pool_0, one_pool_1], )\n",
    "        if i == 0: \n",
    "            concat = one_pool\n",
    "        else:\n",
    "            concat = pd.concat([concat,one_pool])\n",
    "        pools.append([concat.loc[:,:3],concat.loc[:,4:]])\n",
    "\n",
    "        start_0 = end_0\n",
    "        start_1 = end_1\n",
    "        if r_0 >= 1 :\n",
    "            r_0 = r_0 - 1\n",
    "    return pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear kernal\n",
    "iterNum = 50\n",
    "\n",
    "# Create my estimator and prepare the parameter grid dictionary\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]} \n",
    "total_set = []\n",
    "\n",
    "for i in range(iterNum):\n",
    "    best_estimates = []\n",
    "    best_params = []\n",
    "    one_pool_set = []\n",
    "    \n",
    "    x_train2, x_test2, y_train2, y_test2 = train_test_split(df2.loc[:,:3], df2.loc[:,4], test_size=0.344, stratify=df2.loc[:,4])\n",
    "    \n",
    "    # Standardization(Normalization)\n",
    "    x_train2 = StandardScaler().fit_transform(x_train2)\n",
    "    x_test2 = StandardScaler().fit_transform(x_test2)\n",
    "    \n",
    "    y_train2 = pd.DataFrame(y_train2)\n",
    "    y_test2 = pd.DataFrame(y_test2)\n",
    "    x_train2 = pd.DataFrame(x_train2, index = y_train2.index)\n",
    "    x_test2 = pd.DataFrame(x_test2, index = y_test2.index)\n",
    "    \n",
    "    poolsArr = makeRandomPools(x_train2, y_train2)\n",
    "    \n",
    "    for j in range(len(poolsArr)):\n",
    "        x = poolsArr[j][0]\n",
    "        y = poolsArr[j][1]\n",
    "        #print('length x:', len(x))\n",
    "        #print('length y:', len(y))\n",
    "        \n",
    "        # train with linear kernel svm\n",
    "        linear_svm = LinearSVC(loss='l2', penalty='l1', dual=False)\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        #grid = GridSearchCV(linear_svm,param_grid,cv=StratifiedKFold(n_splits=5))\n",
    "        grid = GridSearchCV(linear_svm,param_grid,cv=KFold(n_splits=10))\n",
    "        grid.fit(x,y)\n",
    "        \n",
    "        # best C\n",
    "        best_params = grid.best_params_\n",
    "\n",
    "        #prediction\n",
    "        train_pred = grid.predict(x)\n",
    "        test_pred = grid.predict(x_test2)\n",
    "        train_pred_decision = grid.decision_function(x)\n",
    "        test_pred_decision = grid.decision_function(x_test2)\n",
    "\n",
    "        #accuracy score\n",
    "        train_error = 1 - (accuracy_score(y, train_pred))\n",
    "        test_error = 1- (accuracy_score(y_test2, test_pred))\n",
    "        \n",
    "        #add one pool result\n",
    "        one_pool_set.append([j+1, best_params, train_error, test_error])\n",
    "        #print(one_pool_set)\n",
    "    # make dataframe for svm 90 model\n",
    "    one_pool_set = pd.DataFrame(one_pool_set, columns = ['', 'best C', 'train error', 'test error'])\n",
    "    one_pool_set.set_index('', inplace = True)\n",
    "    # for total 50 iteration\n",
    "    total_set.append(one_pool_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of models in one iteration: 90\n",
      "the number of iteration: 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best C</th>\n",
       "      <th>train error</th>\n",
       "      <th>test error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.021186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.023305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.023305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'C': 0.1}</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.036017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.006154</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.006849</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.006757</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.006579</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.006410</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>0.006329</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>0.006173</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>0.006024</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        best C  train error  test error\n",
       "                                       \n",
       "1     {'C': 1}     0.000000    0.029661\n",
       "2     {'C': 1}     0.000000    0.029661\n",
       "3     {'C': 1}     0.000000    0.031780\n",
       "4     {'C': 1}     0.025000    0.021186\n",
       "5     {'C': 1}     0.020000    0.023305\n",
       "6     {'C': 1}     0.016667    0.023305\n",
       "7     {'C': 1}     0.014286    0.014831\n",
       "8     {'C': 1}     0.012500    0.014831\n",
       "9   {'C': 0.1}     0.033333    0.036017\n",
       "10   {'C': 10}     0.000000    0.012712\n",
       "11  {'C': 100}     0.000000    0.014831\n",
       "12  {'C': 100}     0.000000    0.014831\n",
       "13  {'C': 100}     0.000000    0.012712\n",
       "14   {'C': 10}     0.000000    0.019068\n",
       "15   {'C': 10}     0.000000    0.016949\n",
       "16   {'C': 10}     0.000000    0.012712\n",
       "17   {'C': 10}     0.000000    0.012712\n",
       "18   {'C': 10}     0.000000    0.010593\n",
       "19   {'C': 10}     0.000000    0.012712\n",
       "20   {'C': 10}     0.000000    0.012712\n",
       "21   {'C': 10}     0.000000    0.010593\n",
       "22   {'C': 10}     0.000000    0.012712\n",
       "23   {'C': 10}     0.000000    0.010593\n",
       "24   {'C': 10}     0.000000    0.010593\n",
       "25   {'C': 10}     0.000000    0.010593\n",
       "26   {'C': 10}     0.000000    0.010593\n",
       "27  {'C': 100}     0.000000    0.016949\n",
       "28  {'C': 100}     0.000000    0.016949\n",
       "29   {'C': 10}     0.000000    0.016949\n",
       "30   {'C': 10}     0.000000    0.016949\n",
       "..         ...          ...         ...\n",
       "61    {'C': 1}     0.008197    0.010593\n",
       "62    {'C': 1}     0.008065    0.010593\n",
       "63    {'C': 1}     0.007937    0.010593\n",
       "64   {'C': 10}     0.006250    0.016949\n",
       "65   {'C': 10}     0.006154    0.016949\n",
       "66   {'C': 10}     0.007576    0.012712\n",
       "67   {'C': 10}     0.007463    0.010593\n",
       "68   {'C': 10}     0.007353    0.010593\n",
       "69   {'C': 10}     0.007246    0.010593\n",
       "70  {'C': 100}     0.007143    0.010593\n",
       "71   {'C': 10}     0.007042    0.014831\n",
       "72   {'C': 10}     0.006944    0.010593\n",
       "73   {'C': 10}     0.006849    0.010593\n",
       "74   {'C': 10}     0.006757    0.010593\n",
       "75   {'C': 10}     0.006667    0.010593\n",
       "76   {'C': 10}     0.006579    0.010593\n",
       "77   {'C': 10}     0.006494    0.010593\n",
       "78   {'C': 10}     0.006410    0.014831\n",
       "79  {'C': 100}     0.006329    0.012712\n",
       "80  {'C': 100}     0.006250    0.012712\n",
       "81  {'C': 100}     0.006173    0.012712\n",
       "82  {'C': 100}     0.006098    0.012712\n",
       "83  {'C': 100}     0.006024    0.012712\n",
       "84   {'C': 10}     0.005952    0.014831\n",
       "85   {'C': 10}     0.005882    0.014831\n",
       "86   {'C': 10}     0.005814    0.014831\n",
       "87   {'C': 10}     0.005747    0.012712\n",
       "88   {'C': 10}     0.005682    0.012712\n",
       "89   {'C': 10}     0.005618    0.010593\n",
       "90   {'C': 10}     0.005556    0.010593\n",
       "\n",
       "[90 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('the number of models in one iteration:', len(total_set[0]))\n",
    "print('the number of iteration:', len(total_set))\n",
    "total_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L-1 penalty with pool of 10 randomly selected data\n",
    "iterNum = 50\n",
    "\n",
    "# Create my estimator and prepare the parameter grid dictionary\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]} \n",
    "l1_total_set = []\n",
    "\n",
    "for i in range(iterNum):\n",
    "    best_estimates = []\n",
    "    best_params = []\n",
    "    one_pool_set = []\n",
    "    \n",
    "    x_train2, x_test2, y_train2, y_test2 = train_test_split(df2.loc[:,:3], df2.loc[:,4], test_size=0.344, stratify=df2.loc[:,4])\n",
    "    \n",
    "    # Standardization(Normalization)\n",
    "    x_train2 = StandardScaler().fit_transform(x_train2)\n",
    "    x_test2 = StandardScaler().fit_transform(x_test2)\n",
    "    \n",
    "    y_train2 = pd.DataFrame(y_train2)\n",
    "    y_test2 = pd.DataFrame(y_test2)\n",
    "    x_train2 = pd.DataFrame(x_train2, index = y_train2.index)\n",
    "    x_test2 = pd.DataFrame(x_test2, index = y_test2.index)\n",
    "    \n",
    "    poolsArr = makeRandomPools(x_train2, y_train2)\n",
    "    \n",
    "    for j in range(len(poolsArr)):\n",
    "        x = poolsArr[j][0]\n",
    "        y = poolsArr[j][1]\n",
    "        #print('length x:', len(x))\n",
    "        #print('length y:', len(y))\n",
    "        \n",
    "        # train with linear kernel svm\n",
    "        linear_svm = LinearSVC(loss='l2', penalty='l1', dual=False)\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        #grid = GridSearchCV(linear_svm,param_grid,cv=StratifiedKFold(n_splits=5))\n",
    "        grid = GridSearchCV(linear_svm,param_grid,cv=KFold(n_splits=10))\n",
    "        grid.fit(x,y)\n",
    "        \n",
    "        # best C\n",
    "        best_params = grid.best_params_\n",
    "\n",
    "        #prediction\n",
    "        train_pred = grid.predict(x)\n",
    "        test_pred = grid.predict(x_test2)\n",
    "        train_pred_decision = grid.decision_function(x)\n",
    "        test_pred_decision = grid.decision_function(x_test2)\n",
    "\n",
    "        #accuracy score\n",
    "        train_error = 1 - (accuracy_score(y, train_pred))\n",
    "        test_error = 1- (accuracy_score(y_test2, test_pred))\n",
    "        \n",
    "        #add one pool result\n",
    "        one_pool_set.append([j+1, best_params, train_error, test_error])\n",
    "        #print(one_pool_set)\n",
    "    # make dataframe for svm 90 model\n",
    "    one_pool_set = pd.DataFrame(one_pool_set, columns = ['', 'best C', 'train error', 'test error'])\n",
    "    one_pool_set.set_index('', inplace = True)\n",
    "    # for total 50 iteration\n",
    "    l1_total_set.append(one_pool_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of models in one iteration: 90\n",
      "the number of iteration: 50\n"
     ]
    }
   ],
   "source": [
    "print('the number of models in one iteration:', len(l1_total_set[0]))\n",
    "print('the number of iteration:', len(l1_total_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ii. Train a SVM with a pool of 10 randomly selected data points from the training set using linear kernel and L1 penalty. Select the parameters with 10-fold cross validation. Choose the 10 closest data points in the training set to the hyperplane of the SVM and add them to the pool. Do not replace the samples back into the training set. Train a new SVM using the pool. Repeat this process until all training data is used. You will have 90 SVMs that were trainined using 10, 20, 30, ..., 900 data points and their 90 test errors. You have implemented active learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findClosest(model, x, y, x_rest, y_rest):\n",
    "    num = 10\n",
    "\n",
    "    # find the distance of unlabeled data form the decision boundary\n",
    "    decision = model.decision_function(x_rest)\n",
    "    coef = model.coef_\n",
    "    norms = np.linalg.norm(coef)\n",
    "    distance = decision/norms\n",
    "\n",
    "    distance = pd.DataFrame(distance, columns = ['distance'], index = x_rest.index)\n",
    "    distance['y'] = y_rest.values\n",
    "    distance = np.absolute(distance)\n",
    "    \n",
    "    # sort from closest point\n",
    "    distance = distance.sort_values(by=['distance'])\n",
    "    \n",
    "    # get index of 10 closest point to decision boundary\n",
    "    indexes = distance.iloc[:10, 0].index\n",
    "    \n",
    "    # add 10 closest point to the pool\n",
    "    for i in range(num):\n",
    "        x = pd.concat([x, pd.DataFrame(x_rest.loc[indexes[i]]).T])\n",
    "        y = pd.concat([y, pd.DataFrame(y_rest.loc[indexes[i]]).T])\n",
    "    \n",
    "    # remove 10 closest points from the remained points\n",
    "    for i in range(num):\n",
    "        x_rest = x_rest.drop([indexes[i]])\n",
    "        y_rest = y_rest.drop([indexes[i]])\n",
    "    return x, y, x_rest, y_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear kernal\n",
    "iterNum = 50\n",
    "modelNum = 90\n",
    "\n",
    "\n",
    "# Create my estimator and prepare the parameter grid dictionary\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]} \n",
    "total_set_2 = []\n",
    "\n",
    "for i in range(iterNum):\n",
    "    best_estimates = []\n",
    "    best_params = []\n",
    "    one_pool_set = []\n",
    "    \n",
    "    x_train2, x_test2, y_train2, y_test2 = train_test_split(df2.loc[:,:3], df2.loc[:,4], test_size=0.344, stratify=df2.loc[:,4])\n",
    "    \n",
    "    # Standardization(Normalization)\n",
    "    x_train2 = StandardScaler().fit_transform(x_train2)\n",
    "    x_test2 = StandardScaler().fit_transform(x_test2)\n",
    "    \n",
    "    x_rest, x, y_rest, y = train_test_split(x_train2, y_train2, test_size=0.011, stratify=y_train2)\n",
    "    y = pd.DataFrame(y)\n",
    "    y_rest = pd.DataFrame(y_rest)\n",
    "    x = pd.DataFrame(x, index = y.index)\n",
    "    x_rest = pd.DataFrame(x_rest, index = y_rest.index)\n",
    "    \n",
    "    for j in range(modelNum):                \n",
    "        # train with linear kernel svm\n",
    "        linear_svm = LinearSVC(loss='l2', penalty='l1', dual=False)\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        grid = GridSearchCV(linear_svm,param_grid,cv=KFold(n_splits=10))\n",
    "        grid.fit(x,y)        \n",
    "        \n",
    "        # best C\n",
    "        best_params = grid.best_params_\n",
    "        \n",
    "        # svm with best parameters from cross validation\n",
    "        best_svm = grid.best_estimator_\n",
    "        best_svm.fit(x, y)\n",
    "\n",
    "        #prediction\n",
    "        train_pred = best_svm.predict(x)\n",
    "        test_pred = best_svm.predict(x_test2)\n",
    "        train_pred_decision = best_svm.decision_function(x)\n",
    "        test_pred_decision = best_svm.decision_function(x_test2)\n",
    "\n",
    "        #accuracy score\n",
    "        train_error = 1 - (accuracy_score(y, train_pred))\n",
    "        test_error = 1- (accuracy_score(y_test2, test_pred))\n",
    "        \n",
    "        #choose 10 closest data points in the training set to the hyperplane of the SVM\n",
    "        if len(x_rest) > 0:\n",
    "            x, y, x_rest, y_rest = findClosest(best_svm, x, y, x_rest, y_rest)\n",
    "        \n",
    "        #add one pool result\n",
    "        one_pool_set.append([j+1, best_params, train_error, test_error])\n",
    "        #print(one_pool_set)\n",
    "        \n",
    "    # make dataframe for svm 90 model\n",
    "    one_pool_set = pd.DataFrame(one_pool_set, columns = ['', 'best C', 'train error', 'test error'])\n",
    "    one_pool_set.set_index('', inplace = True)\n",
    "    # for total 50 iteration\n",
    "    total_set_2.append(one_pool_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of models in one iteration: 90\n",
      "the number of iteration: 50\n"
     ]
    }
   ],
   "source": [
    "#len(total_set_2)\n",
    "\n",
    "print('the number of models in one iteration:', len(total_set_2[0]))\n",
    "print('the number of iteration:', len(total_set_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L-1 penalty\n",
    "iterNum = 50\n",
    "modelNum = 90\n",
    "\n",
    "# Create my estimator and prepare the parameter grid dictionary\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]} \n",
    "l1_total_set_2 = []\n",
    "\n",
    "for i in range(iterNum):\n",
    "    best_estimates = []\n",
    "    best_params = []\n",
    "    one_pool_set = []\n",
    "    \n",
    "    # split training set and test set\n",
    "    x_train2, x_test2, y_train2, y_test2 = train_test_split(df2.loc[:,:3], df2.loc[:,4], test_size=0.344, stratify=df2.loc[:,4])\n",
    "\n",
    "    # Standardization(Normalization)\n",
    "    x_train2 = StandardScaler().fit_transform(x_train2)\n",
    "    x_test2 = StandardScaler().fit_transform(x_test2)\n",
    "    \n",
    "    # split pool\n",
    "    x_rest, x, y_rest, y = train_test_split(x_train2, y_train2, test_size=0.011, stratify=y_train2)\n",
    "    y = pd.DataFrame(y)\n",
    "    y_rest = pd.DataFrame(y_rest)\n",
    "    x = pd.DataFrame(x, index = y.index)\n",
    "    x_rest = pd.DataFrame(x_rest, index = y_rest.index)\n",
    "\n",
    "    for j in range(modelNum):\n",
    "        # train with L1-penalty\n",
    "        linear_svm = LinearSVC(loss='l2', penalty='l1', dual=False)\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        grid = GridSearchCV(linear_svm,param_grid,cv=KFold(n_splits=10))\n",
    "        grid.fit(x,y)\n",
    "        \n",
    "        # best C\n",
    "        best_params = grid.best_params_\n",
    "\n",
    "        # svm with best parameters from cross validation\n",
    "        best_svm = grid.best_estimator_\n",
    "        best_svm.fit(x, y)\n",
    "\n",
    "        #prediction\n",
    "        train_pred = best_svm.predict(x)\n",
    "        test_pred = best_svm.predict(x_test2)\n",
    "        train_pred_decision = best_svm.decision_function(x)\n",
    "        test_pred_decision = best_svm.decision_function(x_test2)\n",
    "\n",
    "        #accuracy score\n",
    "        train_error = 1 - (accuracy_score(y, train_pred))\n",
    "        test_error = 1- (accuracy_score(y_test2, test_pred))\n",
    "        \n",
    "        #choose 10 closest data points in the training set to the hyperplane of the SVM\n",
    "        if len(x_rest) > 0:\n",
    "            x, y, x_rest, y_rest = findClosest(best_svm, x, y, x_rest, y_rest)\n",
    "        \n",
    "        #add one pool result\n",
    "        one_pool_set.append([j+1, best_params, train_error, test_error])\n",
    "        #print(one_pool_set)\n",
    "    # make dataframe for svm 90 model\n",
    "    one_pool_set = pd.DataFrame(one_pool_set, columns = ['', 'best C', 'train error', 'test error'])\n",
    "    one_pool_set.set_index('', inplace = True)\n",
    "    # for total 50 iteration\n",
    "    l1_total_set_2.append(one_pool_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of models in one iteration: 90\n",
      "the number of iteration: 50\n"
     ]
    }
   ],
   "source": [
    "#len(l1_total_set_2)\n",
    "\n",
    "print('the number of models in one iteration:', len(l1_total_set_2[0]))\n",
    "print('the number of iteration:', len(l1_total_set_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Average the 50 test errors for each of the incrementally trained 90 SVMs in 2(b)i and 2(b)ii. By doing so, you are performing a Monte Carlo simulation. Plot average test error versus numbers of training instances for both active and passive learners on the same figure and report your conclusion. Here, you are actually obtaining a learning curve by Monte-Carlo simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAverage(dataset):\n",
    "    iNum = len(dataset)\n",
    "    #print('iNum:', iNum)\n",
    "    modelNum = len(dataset[0])\n",
    "    #print('modelNum:', modelNum)\n",
    "    s_train = 0\n",
    "    s_test = 0\n",
    "    avg_train = 0\n",
    "    avg_test = 0\n",
    "    average_dataset = []\n",
    "    \n",
    "    for j in range(1, modelNum+1):\n",
    "        s_train = 0\n",
    "        s_test = 0\n",
    "        avg_train = 0\n",
    "        avg_test = 0\n",
    "        for i in range(iNum):\n",
    "            #print(i, j)\n",
    "            s_train += dataset[i].loc[j, 'train error']\n",
    "            s_test += dataset[i].loc[j, 'test error']\n",
    "            \n",
    "        avg_train = s_train / iNum\n",
    "        avg_test = s_test / iNum\n",
    "        average_dataset.append([avg_train, avg_test])\n",
    "    return average_dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train error</th>\n",
       "      <th>test error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.116695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.068644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.046229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.030466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.025763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.009333</td>\n",
       "      <td>0.025297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.010571</td>\n",
       "      <td>0.024831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.011750</td>\n",
       "      <td>0.024153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.009333</td>\n",
       "      <td>0.023644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.022669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.009273</td>\n",
       "      <td>0.022373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.022712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.008769</td>\n",
       "      <td>0.021822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.007429</td>\n",
       "      <td>0.020847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.008533</td>\n",
       "      <td>0.019068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.008125</td>\n",
       "      <td>0.018136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.007176</td>\n",
       "      <td>0.017458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.007667</td>\n",
       "      <td>0.016864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.007579</td>\n",
       "      <td>0.017797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.016525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.007238</td>\n",
       "      <td>0.016864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.006364</td>\n",
       "      <td>0.016695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.007565</td>\n",
       "      <td>0.016271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.007167</td>\n",
       "      <td>0.015339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.007760</td>\n",
       "      <td>0.015508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.007111</td>\n",
       "      <td>0.015636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.006714</td>\n",
       "      <td>0.015297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.007172</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.007267</td>\n",
       "      <td>0.014364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.008164</td>\n",
       "      <td>0.011695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.008032</td>\n",
       "      <td>0.011356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.008254</td>\n",
       "      <td>0.011229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.008312</td>\n",
       "      <td>0.011229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.008369</td>\n",
       "      <td>0.010975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.008212</td>\n",
       "      <td>0.011314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.008328</td>\n",
       "      <td>0.011102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.008529</td>\n",
       "      <td>0.011186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.008435</td>\n",
       "      <td>0.010975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.008343</td>\n",
       "      <td>0.011059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.008451</td>\n",
       "      <td>0.010932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.008750</td>\n",
       "      <td>0.010805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.008466</td>\n",
       "      <td>0.011186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.008541</td>\n",
       "      <td>0.011059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.008747</td>\n",
       "      <td>0.010805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.008632</td>\n",
       "      <td>0.011144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.008727</td>\n",
       "      <td>0.011229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.008538</td>\n",
       "      <td>0.011229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.008532</td>\n",
       "      <td>0.011483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.008725</td>\n",
       "      <td>0.011229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.008716</td>\n",
       "      <td>0.011483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.008780</td>\n",
       "      <td>0.011525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.008916</td>\n",
       "      <td>0.011314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.008714</td>\n",
       "      <td>0.011398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.008447</td>\n",
       "      <td>0.011398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.008558</td>\n",
       "      <td>0.011441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.008598</td>\n",
       "      <td>0.011441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.008636</td>\n",
       "      <td>0.011483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.008517</td>\n",
       "      <td>0.011483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.008489</td>\n",
       "      <td>0.011441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    train error  test error\n",
       "0      0.024000    0.116695\n",
       "1      0.017000    0.068644\n",
       "2      0.012000    0.046229\n",
       "3      0.009500    0.030466\n",
       "4      0.010000    0.025763\n",
       "5      0.009333    0.025297\n",
       "6      0.010571    0.024831\n",
       "7      0.011750    0.024153\n",
       "8      0.009333    0.023644\n",
       "9      0.009800    0.022669\n",
       "10     0.009273    0.022373\n",
       "11     0.010000    0.022712\n",
       "12     0.008769    0.021822\n",
       "13     0.007429    0.020847\n",
       "14     0.008533    0.019068\n",
       "15     0.008125    0.018136\n",
       "16     0.007176    0.017458\n",
       "17     0.007667    0.016864\n",
       "18     0.007579    0.017797\n",
       "19     0.006800    0.016525\n",
       "20     0.007238    0.016864\n",
       "21     0.006364    0.016695\n",
       "22     0.007565    0.016271\n",
       "23     0.007167    0.015339\n",
       "24     0.007760    0.015508\n",
       "25     0.006923    0.014831\n",
       "26     0.007111    0.015636\n",
       "27     0.006714    0.015297\n",
       "28     0.007172    0.015000\n",
       "29     0.007267    0.014364\n",
       "..          ...         ...\n",
       "60     0.008164    0.011695\n",
       "61     0.008032    0.011356\n",
       "62     0.008254    0.011229\n",
       "63     0.008312    0.011229\n",
       "64     0.008369    0.010975\n",
       "65     0.008212    0.011314\n",
       "66     0.008328    0.011102\n",
       "67     0.008529    0.011186\n",
       "68     0.008435    0.010975\n",
       "69     0.008343    0.011059\n",
       "70     0.008451    0.010932\n",
       "71     0.008750    0.010805\n",
       "72     0.008466    0.011186\n",
       "73     0.008541    0.011059\n",
       "74     0.008747    0.010805\n",
       "75     0.008632    0.011144\n",
       "76     0.008727    0.011229\n",
       "77     0.008538    0.011229\n",
       "78     0.008532    0.011483\n",
       "79     0.008725    0.011229\n",
       "80     0.008716    0.011483\n",
       "81     0.008780    0.011525\n",
       "82     0.008916    0.011314\n",
       "83     0.008714    0.011398\n",
       "84     0.008447    0.011398\n",
       "85     0.008558    0.011441\n",
       "86     0.008598    0.011441\n",
       "87     0.008636    0.011483\n",
       "88     0.008517    0.011483\n",
       "89     0.008489    0.011441\n",
       "\n",
       "[90 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passive_linear_avg = computeAverage(total_set)\n",
    "passive_linear_avg = pd.DataFrame(passive_linear_avg, columns = ['train error', 'test error'])\n",
    "passive_linear_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train error</th>\n",
       "      <th>test error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.118898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.061314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011333</td>\n",
       "      <td>0.041907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.034788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.032246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.029873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.009714</td>\n",
       "      <td>0.026441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.024364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.022373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.022712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.021949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.021992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.006769</td>\n",
       "      <td>0.018814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.007714</td>\n",
       "      <td>0.018390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.007733</td>\n",
       "      <td>0.018729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.017203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.016314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.006556</td>\n",
       "      <td>0.016186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.005789</td>\n",
       "      <td>0.015975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.015593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.006762</td>\n",
       "      <td>0.014788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.014788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.005913</td>\n",
       "      <td>0.014915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.006333</td>\n",
       "      <td>0.014237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.007280</td>\n",
       "      <td>0.014492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.006692</td>\n",
       "      <td>0.013983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.007259</td>\n",
       "      <td>0.014153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.014407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.007379</td>\n",
       "      <td>0.013814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.007467</td>\n",
       "      <td>0.013856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.008426</td>\n",
       "      <td>0.011695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.008161</td>\n",
       "      <td>0.011949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.008159</td>\n",
       "      <td>0.011992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.008406</td>\n",
       "      <td>0.012076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.008585</td>\n",
       "      <td>0.011610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.008879</td>\n",
       "      <td>0.012034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.008687</td>\n",
       "      <td>0.012076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.008471</td>\n",
       "      <td>0.012034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.008464</td>\n",
       "      <td>0.011780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.008314</td>\n",
       "      <td>0.011568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.008648</td>\n",
       "      <td>0.011695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.008639</td>\n",
       "      <td>0.011398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.008575</td>\n",
       "      <td>0.011229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.008459</td>\n",
       "      <td>0.011186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.008747</td>\n",
       "      <td>0.011229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.008921</td>\n",
       "      <td>0.011441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.009013</td>\n",
       "      <td>0.011441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.008744</td>\n",
       "      <td>0.011441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.008911</td>\n",
       "      <td>0.011398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.011483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.008840</td>\n",
       "      <td>0.011653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.008878</td>\n",
       "      <td>0.011737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.008627</td>\n",
       "      <td>0.011822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.008762</td>\n",
       "      <td>0.011737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.008753</td>\n",
       "      <td>0.011653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.008860</td>\n",
       "      <td>0.012034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.008805</td>\n",
       "      <td>0.011737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.008636</td>\n",
       "      <td>0.011271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.008697</td>\n",
       "      <td>0.011483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.008867</td>\n",
       "      <td>0.011949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    train error  test error\n",
       "0      0.022000    0.118898\n",
       "1      0.006000    0.061314\n",
       "2      0.011333    0.041907\n",
       "3      0.010000    0.034788\n",
       "4      0.008400    0.032246\n",
       "5      0.008333    0.029873\n",
       "6      0.009714    0.026441\n",
       "7      0.007500    0.024364\n",
       "8      0.007333    0.022373\n",
       "9      0.007600    0.022712\n",
       "10     0.006000    0.021949\n",
       "11     0.008333    0.021992\n",
       "12     0.006769    0.018814\n",
       "13     0.007714    0.018390\n",
       "14     0.007733    0.018729\n",
       "15     0.006250    0.017203\n",
       "16     0.005882    0.016314\n",
       "17     0.006556    0.016186\n",
       "18     0.005789    0.015975\n",
       "19     0.005700    0.015593\n",
       "20     0.006762    0.014788\n",
       "21     0.005727    0.014788\n",
       "22     0.005913    0.014915\n",
       "23     0.006333    0.014237\n",
       "24     0.007280    0.014492\n",
       "25     0.006692    0.013983\n",
       "26     0.007259    0.014153\n",
       "27     0.008000    0.014407\n",
       "28     0.007379    0.013814\n",
       "29     0.007467    0.013856\n",
       "..          ...         ...\n",
       "60     0.008426    0.011695\n",
       "61     0.008161    0.011949\n",
       "62     0.008159    0.011992\n",
       "63     0.008406    0.012076\n",
       "64     0.008585    0.011610\n",
       "65     0.008879    0.012034\n",
       "66     0.008687    0.012076\n",
       "67     0.008471    0.012034\n",
       "68     0.008464    0.011780\n",
       "69     0.008314    0.011568\n",
       "70     0.008648    0.011695\n",
       "71     0.008639    0.011398\n",
       "72     0.008575    0.011229\n",
       "73     0.008459    0.011186\n",
       "74     0.008747    0.011229\n",
       "75     0.008921    0.011441\n",
       "76     0.009013    0.011441\n",
       "77     0.008744    0.011441\n",
       "78     0.008911    0.011398\n",
       "79     0.008700    0.011483\n",
       "80     0.008840    0.011653\n",
       "81     0.008878    0.011737\n",
       "82     0.008627    0.011822\n",
       "83     0.008762    0.011737\n",
       "84     0.008753    0.011653\n",
       "85     0.008860    0.012034\n",
       "86     0.008805    0.011737\n",
       "87     0.008636    0.011271\n",
       "88     0.008697    0.011483\n",
       "89     0.008867    0.011949\n",
       "\n",
       "[90 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passive_l1_avg = computeAverage(l1_total_set)\n",
    "passive_l1_avg = pd.DataFrame(passive_l1_avg, columns = ['train error', 'test error'])\n",
    "passive_l1_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train error</th>\n",
       "      <th>test error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.113136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.059195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.037924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013500</td>\n",
       "      <td>0.026059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.026800</td>\n",
       "      <td>0.016314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.045333</td>\n",
       "      <td>0.012754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.079714</td>\n",
       "      <td>0.011737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.092750</td>\n",
       "      <td>0.011144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.090667</td>\n",
       "      <td>0.011186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.081600</td>\n",
       "      <td>0.011186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.073455</td>\n",
       "      <td>0.011186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.069167</td>\n",
       "      <td>0.011144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.063538</td>\n",
       "      <td>0.010890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.058571</td>\n",
       "      <td>0.011398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.057733</td>\n",
       "      <td>0.011314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.048588</td>\n",
       "      <td>0.010847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.046778</td>\n",
       "      <td>0.011144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.044316</td>\n",
       "      <td>0.010763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.010975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.040190</td>\n",
       "      <td>0.010805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.038636</td>\n",
       "      <td>0.011441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.037217</td>\n",
       "      <td>0.011441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.035083</td>\n",
       "      <td>0.011271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.034320</td>\n",
       "      <td>0.011653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.033923</td>\n",
       "      <td>0.011695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.033407</td>\n",
       "      <td>0.012034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.032071</td>\n",
       "      <td>0.011695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.031103</td>\n",
       "      <td>0.011822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.029667</td>\n",
       "      <td>0.011653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.015377</td>\n",
       "      <td>0.011992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.014742</td>\n",
       "      <td>0.011780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.014603</td>\n",
       "      <td>0.012119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.014594</td>\n",
       "      <td>0.011992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.014677</td>\n",
       "      <td>0.012669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.013909</td>\n",
       "      <td>0.012076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.013612</td>\n",
       "      <td>0.012288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.013618</td>\n",
       "      <td>0.012542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.013130</td>\n",
       "      <td>0.011864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.012457</td>\n",
       "      <td>0.011695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.012282</td>\n",
       "      <td>0.011949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.011441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.012274</td>\n",
       "      <td>0.011653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.012162</td>\n",
       "      <td>0.011780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.012107</td>\n",
       "      <td>0.011780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.012474</td>\n",
       "      <td>0.012458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.011818</td>\n",
       "      <td>0.011483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.012077</td>\n",
       "      <td>0.012203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.012228</td>\n",
       "      <td>0.012288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.011975</td>\n",
       "      <td>0.012246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.011778</td>\n",
       "      <td>0.012076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.010634</td>\n",
       "      <td>0.011780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.010337</td>\n",
       "      <td>0.011483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.010595</td>\n",
       "      <td>0.011695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.010518</td>\n",
       "      <td>0.011907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.010116</td>\n",
       "      <td>0.011653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.009816</td>\n",
       "      <td>0.011568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.009568</td>\n",
       "      <td>0.011610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.009303</td>\n",
       "      <td>0.011356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.009289</td>\n",
       "      <td>0.011610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    train error  test error\n",
       "0      0.014000    0.113136\n",
       "1      0.005000    0.059195\n",
       "2      0.007333    0.037924\n",
       "3      0.013500    0.026059\n",
       "4      0.026800    0.016314\n",
       "5      0.045333    0.012754\n",
       "6      0.079714    0.011737\n",
       "7      0.092750    0.011144\n",
       "8      0.090667    0.011186\n",
       "9      0.081600    0.011186\n",
       "10     0.073455    0.011186\n",
       "11     0.069167    0.011144\n",
       "12     0.063538    0.010890\n",
       "13     0.058571    0.011398\n",
       "14     0.057733    0.011314\n",
       "15     0.052000    0.010593\n",
       "16     0.048588    0.010847\n",
       "17     0.046778    0.011144\n",
       "18     0.044316    0.010763\n",
       "19     0.042200    0.010975\n",
       "20     0.040190    0.010805\n",
       "21     0.038636    0.011441\n",
       "22     0.037217    0.011441\n",
       "23     0.035083    0.011271\n",
       "24     0.034320    0.011653\n",
       "25     0.033923    0.011695\n",
       "26     0.033407    0.012034\n",
       "27     0.032071    0.011695\n",
       "28     0.031103    0.011822\n",
       "29     0.029667    0.011653\n",
       "..          ...         ...\n",
       "60     0.015377    0.011992\n",
       "61     0.014742    0.011780\n",
       "62     0.014603    0.012119\n",
       "63     0.014594    0.011992\n",
       "64     0.014677    0.012669\n",
       "65     0.013909    0.012076\n",
       "66     0.013612    0.012288\n",
       "67     0.013618    0.012542\n",
       "68     0.013130    0.011864\n",
       "69     0.012457    0.011695\n",
       "70     0.012282    0.011949\n",
       "71     0.011667    0.011441\n",
       "72     0.012274    0.011653\n",
       "73     0.012162    0.011780\n",
       "74     0.012107    0.011780\n",
       "75     0.012474    0.012458\n",
       "76     0.011818    0.011483\n",
       "77     0.012077    0.012203\n",
       "78     0.012228    0.012288\n",
       "79     0.011975    0.012246\n",
       "80     0.011778    0.012076\n",
       "81     0.010634    0.011780\n",
       "82     0.010337    0.011483\n",
       "83     0.010595    0.011695\n",
       "84     0.010518    0.011907\n",
       "85     0.010116    0.011653\n",
       "86     0.009816    0.011568\n",
       "87     0.009568    0.011610\n",
       "88     0.009303    0.011356\n",
       "89     0.009289    0.011610\n",
       "\n",
       "[90 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_linear_avg = computeAverage(total_set_2)\n",
    "active_linear_avg = pd.DataFrame(active_linear_avg, columns = ['train error', 'test error'])\n",
    "#active_linear_avg['n'] = np.arange(10, 901, 10)\n",
    "active_linear_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train error</th>\n",
       "      <th>test error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.114534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.058814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.009333</td>\n",
       "      <td>0.031186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.025381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.020424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.048333</td>\n",
       "      <td>0.013771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.078000</td>\n",
       "      <td>0.012924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.088222</td>\n",
       "      <td>0.012415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.012288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.069455</td>\n",
       "      <td>0.012331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.063667</td>\n",
       "      <td>0.012331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.059692</td>\n",
       "      <td>0.012203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.055714</td>\n",
       "      <td>0.012458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.051067</td>\n",
       "      <td>0.012246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.049125</td>\n",
       "      <td>0.012288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.045647</td>\n",
       "      <td>0.012246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.043778</td>\n",
       "      <td>0.012669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.012331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.040900</td>\n",
       "      <td>0.012542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.038286</td>\n",
       "      <td>0.012415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.035545</td>\n",
       "      <td>0.012754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.034870</td>\n",
       "      <td>0.012458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.012203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.032400</td>\n",
       "      <td>0.012076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.031385</td>\n",
       "      <td>0.012754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.030667</td>\n",
       "      <td>0.012585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.029143</td>\n",
       "      <td>0.012458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.028759</td>\n",
       "      <td>0.012415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.027667</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.014197</td>\n",
       "      <td>0.012415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.013839</td>\n",
       "      <td>0.012458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.013524</td>\n",
       "      <td>0.012373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.013406</td>\n",
       "      <td>0.012627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.013354</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.012119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.012448</td>\n",
       "      <td>0.012585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.012176</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.011971</td>\n",
       "      <td>0.012754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.011543</td>\n",
       "      <td>0.012627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.011746</td>\n",
       "      <td>0.012797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.011694</td>\n",
       "      <td>0.013305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.011836</td>\n",
       "      <td>0.012881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.011568</td>\n",
       "      <td>0.012331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.010693</td>\n",
       "      <td>0.012585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.010368</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.010234</td>\n",
       "      <td>0.012754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.010154</td>\n",
       "      <td>0.012797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.012542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.010175</td>\n",
       "      <td>0.012881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.013051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.009805</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.009783</td>\n",
       "      <td>0.013093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.009667</td>\n",
       "      <td>0.012797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.009741</td>\n",
       "      <td>0.012797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.009581</td>\n",
       "      <td>0.012754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.009448</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.009341</td>\n",
       "      <td>0.012542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.009551</td>\n",
       "      <td>0.012797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.009467</td>\n",
       "      <td>0.012797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    train error  test error\n",
       "0      0.020000    0.114534\n",
       "1      0.011000    0.058814\n",
       "2      0.009333    0.031186\n",
       "3      0.020000    0.025381\n",
       "4      0.035200    0.020424\n",
       "5      0.048333    0.013771\n",
       "6      0.078000    0.012924\n",
       "7      0.091000    0.012712\n",
       "8      0.088222    0.012415\n",
       "9      0.081000    0.012288\n",
       "10     0.069455    0.012331\n",
       "11     0.063667    0.012331\n",
       "12     0.059692    0.012203\n",
       "13     0.055714    0.012458\n",
       "14     0.051067    0.012246\n",
       "15     0.049125    0.012288\n",
       "16     0.045647    0.012246\n",
       "17     0.043778    0.012669\n",
       "18     0.042105    0.012331\n",
       "19     0.040900    0.012542\n",
       "20     0.038286    0.012415\n",
       "21     0.035545    0.012754\n",
       "22     0.034870    0.012458\n",
       "23     0.034000    0.012203\n",
       "24     0.032400    0.012076\n",
       "25     0.031385    0.012754\n",
       "26     0.030667    0.012585\n",
       "27     0.029143    0.012458\n",
       "28     0.028759    0.012415\n",
       "29     0.027667    0.012712\n",
       "..          ...         ...\n",
       "60     0.014197    0.012415\n",
       "61     0.013839    0.012458\n",
       "62     0.013524    0.012373\n",
       "63     0.013406    0.012627\n",
       "64     0.013354    0.012500\n",
       "65     0.013000    0.012119\n",
       "66     0.012448    0.012585\n",
       "67     0.012176    0.012500\n",
       "68     0.011971    0.012754\n",
       "69     0.011543    0.012627\n",
       "70     0.011746    0.012797\n",
       "71     0.011694    0.013305\n",
       "72     0.011836    0.012881\n",
       "73     0.011568    0.012331\n",
       "74     0.010693    0.012585\n",
       "75     0.010368    0.012712\n",
       "76     0.010234    0.012754\n",
       "77     0.010154    0.012797\n",
       "78     0.010101    0.012542\n",
       "79     0.010175    0.012881\n",
       "80     0.009901    0.013051\n",
       "81     0.009805    0.012712\n",
       "82     0.009783    0.013093\n",
       "83     0.009667    0.012797\n",
       "84     0.009741    0.012797\n",
       "85     0.009581    0.012754\n",
       "86     0.009448    0.012712\n",
       "87     0.009341    0.012542\n",
       "88     0.009551    0.012797\n",
       "89     0.009467    0.012797\n",
       "\n",
       "[90 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_l1_avg = computeAverage(l1_total_set_2)\n",
    "active_l1_avg = pd.DataFrame(active_l1_avg, columns = ['train error', 'test error'])\n",
    "active_l1_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2_test_error = pd.DataFrame({'active linear':active_linear_avg['test error'], \n",
    "                             'passive linear':passive_linear_avg['test error'], \n",
    "                             'active L1':active_l1_avg['test error'], \n",
    "                             'passive L1':passive_l1_avg['test error']})\n",
    "part2_test_error['n'] = np.arange(10, 901, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>active linear</th>\n",
       "      <th>passive linear</th>\n",
       "      <th>active L1</th>\n",
       "      <th>passive L1</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.113136</td>\n",
       "      <td>0.116695</td>\n",
       "      <td>0.114534</td>\n",
       "      <td>0.118898</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.059195</td>\n",
       "      <td>0.068644</td>\n",
       "      <td>0.058814</td>\n",
       "      <td>0.061314</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.037924</td>\n",
       "      <td>0.046229</td>\n",
       "      <td>0.031186</td>\n",
       "      <td>0.041907</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.026059</td>\n",
       "      <td>0.030466</td>\n",
       "      <td>0.025381</td>\n",
       "      <td>0.034788</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016314</td>\n",
       "      <td>0.025763</td>\n",
       "      <td>0.020424</td>\n",
       "      <td>0.032246</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.012754</td>\n",
       "      <td>0.025297</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.029873</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.011737</td>\n",
       "      <td>0.024831</td>\n",
       "      <td>0.012924</td>\n",
       "      <td>0.026441</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.011144</td>\n",
       "      <td>0.024153</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.024364</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.011186</td>\n",
       "      <td>0.023644</td>\n",
       "      <td>0.012415</td>\n",
       "      <td>0.022373</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.011186</td>\n",
       "      <td>0.022669</td>\n",
       "      <td>0.012288</td>\n",
       "      <td>0.022712</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.011186</td>\n",
       "      <td>0.022373</td>\n",
       "      <td>0.012331</td>\n",
       "      <td>0.021949</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.011144</td>\n",
       "      <td>0.022712</td>\n",
       "      <td>0.012331</td>\n",
       "      <td>0.021992</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.010890</td>\n",
       "      <td>0.021822</td>\n",
       "      <td>0.012203</td>\n",
       "      <td>0.018814</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.011398</td>\n",
       "      <td>0.020847</td>\n",
       "      <td>0.012458</td>\n",
       "      <td>0.018390</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.011314</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012246</td>\n",
       "      <td>0.018729</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.018136</td>\n",
       "      <td>0.012288</td>\n",
       "      <td>0.017203</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.010847</td>\n",
       "      <td>0.017458</td>\n",
       "      <td>0.012246</td>\n",
       "      <td>0.016314</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.011144</td>\n",
       "      <td>0.016864</td>\n",
       "      <td>0.012669</td>\n",
       "      <td>0.016186</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.010763</td>\n",
       "      <td>0.017797</td>\n",
       "      <td>0.012331</td>\n",
       "      <td>0.015975</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.010975</td>\n",
       "      <td>0.016525</td>\n",
       "      <td>0.012542</td>\n",
       "      <td>0.015593</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.010805</td>\n",
       "      <td>0.016864</td>\n",
       "      <td>0.012415</td>\n",
       "      <td>0.014788</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.011441</td>\n",
       "      <td>0.016695</td>\n",
       "      <td>0.012754</td>\n",
       "      <td>0.014788</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.011441</td>\n",
       "      <td>0.016271</td>\n",
       "      <td>0.012458</td>\n",
       "      <td>0.014915</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.011271</td>\n",
       "      <td>0.015339</td>\n",
       "      <td>0.012203</td>\n",
       "      <td>0.014237</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.011653</td>\n",
       "      <td>0.015508</td>\n",
       "      <td>0.012076</td>\n",
       "      <td>0.014492</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.011695</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012754</td>\n",
       "      <td>0.013983</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.012034</td>\n",
       "      <td>0.015636</td>\n",
       "      <td>0.012585</td>\n",
       "      <td>0.014153</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.011695</td>\n",
       "      <td>0.015297</td>\n",
       "      <td>0.012458</td>\n",
       "      <td>0.014407</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.011822</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.012415</td>\n",
       "      <td>0.013814</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.011653</td>\n",
       "      <td>0.014364</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.013856</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.011992</td>\n",
       "      <td>0.011695</td>\n",
       "      <td>0.012415</td>\n",
       "      <td>0.011695</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.011780</td>\n",
       "      <td>0.011356</td>\n",
       "      <td>0.012458</td>\n",
       "      <td>0.011949</td>\n",
       "      <td>620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.012119</td>\n",
       "      <td>0.011229</td>\n",
       "      <td>0.012373</td>\n",
       "      <td>0.011992</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.011992</td>\n",
       "      <td>0.011229</td>\n",
       "      <td>0.012627</td>\n",
       "      <td>0.012076</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.012669</td>\n",
       "      <td>0.010975</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.011610</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.012076</td>\n",
       "      <td>0.011314</td>\n",
       "      <td>0.012119</td>\n",
       "      <td>0.012034</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.012288</td>\n",
       "      <td>0.011102</td>\n",
       "      <td>0.012585</td>\n",
       "      <td>0.012076</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.012542</td>\n",
       "      <td>0.011186</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.012034</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.011864</td>\n",
       "      <td>0.010975</td>\n",
       "      <td>0.012754</td>\n",
       "      <td>0.011780</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.011695</td>\n",
       "      <td>0.011059</td>\n",
       "      <td>0.012627</td>\n",
       "      <td>0.011568</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.011949</td>\n",
       "      <td>0.010932</td>\n",
       "      <td>0.012797</td>\n",
       "      <td>0.011695</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.011441</td>\n",
       "      <td>0.010805</td>\n",
       "      <td>0.013305</td>\n",
       "      <td>0.011398</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.011653</td>\n",
       "      <td>0.011186</td>\n",
       "      <td>0.012881</td>\n",
       "      <td>0.011229</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.011780</td>\n",
       "      <td>0.011059</td>\n",
       "      <td>0.012331</td>\n",
       "      <td>0.011186</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.011780</td>\n",
       "      <td>0.010805</td>\n",
       "      <td>0.012585</td>\n",
       "      <td>0.011229</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.012458</td>\n",
       "      <td>0.011144</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.011441</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.011483</td>\n",
       "      <td>0.011229</td>\n",
       "      <td>0.012754</td>\n",
       "      <td>0.011441</td>\n",
       "      <td>770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.012203</td>\n",
       "      <td>0.011229</td>\n",
       "      <td>0.012797</td>\n",
       "      <td>0.011441</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.012288</td>\n",
       "      <td>0.011483</td>\n",
       "      <td>0.012542</td>\n",
       "      <td>0.011398</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.012246</td>\n",
       "      <td>0.011229</td>\n",
       "      <td>0.012881</td>\n",
       "      <td>0.011483</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.012076</td>\n",
       "      <td>0.011483</td>\n",
       "      <td>0.013051</td>\n",
       "      <td>0.011653</td>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.011780</td>\n",
       "      <td>0.011525</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.011737</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.011483</td>\n",
       "      <td>0.011314</td>\n",
       "      <td>0.013093</td>\n",
       "      <td>0.011822</td>\n",
       "      <td>830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.011695</td>\n",
       "      <td>0.011398</td>\n",
       "      <td>0.012797</td>\n",
       "      <td>0.011737</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.011907</td>\n",
       "      <td>0.011398</td>\n",
       "      <td>0.012797</td>\n",
       "      <td>0.011653</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.011653</td>\n",
       "      <td>0.011441</td>\n",
       "      <td>0.012754</td>\n",
       "      <td>0.012034</td>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.011568</td>\n",
       "      <td>0.011441</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.011737</td>\n",
       "      <td>870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.011610</td>\n",
       "      <td>0.011483</td>\n",
       "      <td>0.012542</td>\n",
       "      <td>0.011271</td>\n",
       "      <td>880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.011356</td>\n",
       "      <td>0.011483</td>\n",
       "      <td>0.012797</td>\n",
       "      <td>0.011483</td>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.011610</td>\n",
       "      <td>0.011441</td>\n",
       "      <td>0.012797</td>\n",
       "      <td>0.011949</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    active linear  passive linear  active L1  passive L1    n\n",
       "0        0.113136        0.116695   0.114534    0.118898   10\n",
       "1        0.059195        0.068644   0.058814    0.061314   20\n",
       "2        0.037924        0.046229   0.031186    0.041907   30\n",
       "3        0.026059        0.030466   0.025381    0.034788   40\n",
       "4        0.016314        0.025763   0.020424    0.032246   50\n",
       "5        0.012754        0.025297   0.013771    0.029873   60\n",
       "6        0.011737        0.024831   0.012924    0.026441   70\n",
       "7        0.011144        0.024153   0.012712    0.024364   80\n",
       "8        0.011186        0.023644   0.012415    0.022373   90\n",
       "9        0.011186        0.022669   0.012288    0.022712  100\n",
       "10       0.011186        0.022373   0.012331    0.021949  110\n",
       "11       0.011144        0.022712   0.012331    0.021992  120\n",
       "12       0.010890        0.021822   0.012203    0.018814  130\n",
       "13       0.011398        0.020847   0.012458    0.018390  140\n",
       "14       0.011314        0.019068   0.012246    0.018729  150\n",
       "15       0.010593        0.018136   0.012288    0.017203  160\n",
       "16       0.010847        0.017458   0.012246    0.016314  170\n",
       "17       0.011144        0.016864   0.012669    0.016186  180\n",
       "18       0.010763        0.017797   0.012331    0.015975  190\n",
       "19       0.010975        0.016525   0.012542    0.015593  200\n",
       "20       0.010805        0.016864   0.012415    0.014788  210\n",
       "21       0.011441        0.016695   0.012754    0.014788  220\n",
       "22       0.011441        0.016271   0.012458    0.014915  230\n",
       "23       0.011271        0.015339   0.012203    0.014237  240\n",
       "24       0.011653        0.015508   0.012076    0.014492  250\n",
       "25       0.011695        0.014831   0.012754    0.013983  260\n",
       "26       0.012034        0.015636   0.012585    0.014153  270\n",
       "27       0.011695        0.015297   0.012458    0.014407  280\n",
       "28       0.011822        0.015000   0.012415    0.013814  290\n",
       "29       0.011653        0.014364   0.012712    0.013856  300\n",
       "..            ...             ...        ...         ...  ...\n",
       "60       0.011992        0.011695   0.012415    0.011695  610\n",
       "61       0.011780        0.011356   0.012458    0.011949  620\n",
       "62       0.012119        0.011229   0.012373    0.011992  630\n",
       "63       0.011992        0.011229   0.012627    0.012076  640\n",
       "64       0.012669        0.010975   0.012500    0.011610  650\n",
       "65       0.012076        0.011314   0.012119    0.012034  660\n",
       "66       0.012288        0.011102   0.012585    0.012076  670\n",
       "67       0.012542        0.011186   0.012500    0.012034  680\n",
       "68       0.011864        0.010975   0.012754    0.011780  690\n",
       "69       0.011695        0.011059   0.012627    0.011568  700\n",
       "70       0.011949        0.010932   0.012797    0.011695  710\n",
       "71       0.011441        0.010805   0.013305    0.011398  720\n",
       "72       0.011653        0.011186   0.012881    0.011229  730\n",
       "73       0.011780        0.011059   0.012331    0.011186  740\n",
       "74       0.011780        0.010805   0.012585    0.011229  750\n",
       "75       0.012458        0.011144   0.012712    0.011441  760\n",
       "76       0.011483        0.011229   0.012754    0.011441  770\n",
       "77       0.012203        0.011229   0.012797    0.011441  780\n",
       "78       0.012288        0.011483   0.012542    0.011398  790\n",
       "79       0.012246        0.011229   0.012881    0.011483  800\n",
       "80       0.012076        0.011483   0.013051    0.011653  810\n",
       "81       0.011780        0.011525   0.012712    0.011737  820\n",
       "82       0.011483        0.011314   0.013093    0.011822  830\n",
       "83       0.011695        0.011398   0.012797    0.011737  840\n",
       "84       0.011907        0.011398   0.012797    0.011653  850\n",
       "85       0.011653        0.011441   0.012754    0.012034  860\n",
       "86       0.011568        0.011441   0.012712    0.011737  870\n",
       "87       0.011610        0.011483   0.012542    0.011271  880\n",
       "88       0.011356        0.011483   0.012797    0.011483  890\n",
       "89       0.011610        0.011441   0.012797    0.011949  900\n",
       "\n",
       "[90 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part2_test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XecXNV5//HPM32byhY1JFDDqEuABNhgqjEQF4hNTXDwz46xnWAnLr+A7UAAOzH2jwSCQ4wVg2ObxEhgCIqNDaabjkSVEAJ1repqV1pt352Z5/fHvSvNrlbaVZkdSft9v17zYu65ZZ57Gc2z55x7zjV3R0REZG8ihQ5AREQOfUoWIiLSKyULERHplZKFiIj0SslCRER6pWQhIiK9ihU6AJHDgZmtBv4SOAkY7+5/WdiIRPqXkoXIPnD3fyp0DCKFoGYokcOAmUULHYMMbEoWIvvAzG40s3vD92PNzM3sKjNba2Zbzew7OdtGzOw6M1thZrVmNt/MynPW329mm8ys3syeNbOpOev+08x+bGaPmFkTcFa/nqhIN0oWIgfuNOA44BzgBjObHJZ/FbgIOAMYBWwD7szZ73fAscAw4DXgv7od98+AfwTKgOfyFbxIXyhZiBy4m9y9xd3fBN4EZoblXwS+4+7V7t4G3AhcbGYxAHe/x90bctbNNLPBOcd92N2fd/esu7f229mI9EAd3CIHblPO+2agNHx/DPCQmWVz1meA4Wa2iaDWcAlQBXRuUwnUh+/X5S1ikX2kmoVI/qwDLnD3ITmvlLuvJ2hiuhD4CDAYGBvuYzn7a0poOWQoWYjkz13AP5rZMQBmVmVmF4bryoA2oBYoBnRLrhzSlCxE8udfgQXAY2bWALwEnByu+wWwBlgPvBOuEzlkmR5+JCIivVHNQkREeqVkISIivVKyEBGRXilZiIhIr46YQXmVlZU+duzYQochInJYWbRo0VZ3r+ptu7wmCzM7n+D2wSjwU3e/pdv604HbgRnA5e7+QFg+C/gxMIhgxOs/uvu8vX3W2LFjWbhw4cE/CRGRI5iZrenLdnlrhgqnVL4TuACYAlxhZlO6bbYW+Czw393Km4G/cPepwPnA7WY2JF+xiojI3uWzZnESsNzdVwKY2X0E0xu807mBu68O1+XOnYO7v5fzfoOZbSGYP2d7HuMVEZE9yGcH91F0nQitOizbJ2Z2EpAAVvSw7mozW2hmC2tqavY7UBER2bt81iysh7J9Gi5uZiOBXwJXuXu2+3p3nwvMBZg9e7aGoosMYB0dHVRXV9Paqtnce5JKpRg9ejTxeHy/9s9nsqgGxuQsjwY29HVnMxsE/Bb4e3fXvDkislfV1dWUlZUxduxYzHr6W3Xgcndqa2uprq5m3Lhx+3WMfDZDvQoca2bjzCwBXE4wqVqvwu0fAn7h7vfnMUYROUK0trZSUVGhRNEDM6OiouKAal15SxbungauAR4FlgLz3X2Jmd1sZp8EMLM5ZlZN8ACYn5jZknD3S4HTgc+a2Rvha1a+YhWRI4MSxZ4d6LXJ6zgLd38EeKRb2Q05718laJ7qvt+9wL35jG2nlU/DMz+EsafBWd/ul48UETncaLqPlu2w5nnY8k7v24qIHARPP/00L7zwws7lu+66i1/84hcFjKh3R8x0H/stXhz8t6OlsHGIyIDx9NNPU1payoc+9CEAvvSlLxU4ot6pZhEvCv6rZCEiB+iiiy7ixBNPZOrUqcydOxeA3//+95xwwgnMnDmTc845h9WrV3PXXXdx2223MWvWLP74xz9y4403cuutt7J06VJOOumkncdbvXo1M2bMAGDRokWcccYZnHjiiZx33nls3LixX89NNYudNYvmwsYhIgfN2Ot+m5fjrr7lY3tdf88991BeXk5LSwtz5szhwgsv5Atf+ALPPvss48aNo66ujvLycr70pS9RWlrKN7/5TQCeeOIJACZPnkx7ezsrV65k/PjxzJs3j0svvZSOjg6+8pWv8PDDD1NVVcW8efP4zne+wz333JOX8+yJkkU8Ffy3QwN5ROTA3HHHHTz00EMArFu3jrlz53L66afvHNtQXl7e6zEuvfRS5s+fz3XXXce8efOYN28ey5YtY/HixZx77rkAZDIZRo4cmb8T6cGATxatJEkBmfYmooUORkQOit5qAPnw9NNP8/jjj/Piiy9SXFzMmWeeycyZM1m2bNk+Heeyyy7jkksu4VOf+hRmxrHHHsvbb7/N1KlTefHFF/MUfe8GfJ/F06saAWhqbChwJCJyOKuvr2fo0KEUFxfz7rvv8tJLL9HW1sYzzzzDqlWrAKirqwOgrKyMhoaef3MmTJhANBrlu9/9LpdddhkAxx13HDU1NTuTRUdHB0uWLOlx/3wZ8MkiliwBIJ5tK3AkInI4O//880mn08yYMYPrr7+eU045haqqKubOncunPvUpZs6cufPH/xOf+AQPPfTQzg7u7i677DLuvfdeLr30UgASiQQPPPAA1157LTNnzmTWrFldbr3tD+Z+ZMy/N3v2bN+fhx89s3Q9Z8ybQpoosRvrDn5gItIvli5dyuTJkwsdxiGtp2tkZovcfXZv+w74mkUikSLtEWJkINNR6HBERA5JAz5ZpOIRWkgGC7p9VkSkR0oW8SitJIIFDcwTEemRkkU8Sot3JgvVLEREejLgk0UyltsMpYF5IiI9GfDJQs1QIiK9U7KIR3KShZqhROTQsmHDBi6++OJCh6FkkYxFafGgGcqVLETkEDNq1CgeeOCBQoehZBGNGG0WJIt0W1OBoxGRw9Xq1auZNGkSV111FTNmzODiiy+mubmZm2++mTlz5jBt2jSuvvpqOgdC33HHHUyZMoUZM2Zw+eWXA/DMM88wa9YsZs2axfHHH09DQwOrV69m2rRpAJx88sldpvk488wzWbRoEU1NTXzuc59jzpw5HH/88Tz88MMH/fwG/ESCAB0WNEN1tLYQL3AsInIQ3Dg4T8et3+vqZcuWcffdd3Pqqafyuc99jn//93/nmmuu4YYbgqdJf+Yzn+E3v/kNn/jEJ7jllltYtWoVyWSS7du3A3Drrbdy5513cuqpp9LY2Egqlepy/Msvv5z58+dz0003sXHjRjZs2MCJJ57It7/9bc4++2zuuecetm/fzkknncRHPvIRSkpKDtqpD/iaBUB7JPgfopqFiByIMWPGcOqppwJw5ZVX8txzz/HUU09x8sknM336dJ588smdNYMZM2bw53/+59x7773EYsHf7aeeeipf//rXueOOO9i+ffvO8k6XXnop999/PwDz58/nkksuAeCxxx7jlltuYdasWZx55pm0traydu3ag3puqlkA6UgKMpBpVbIQOSL0UgPIFzPbbfmv/uqvWLhwIWPGjOHGG2+ktTW4Rf+3v/0tzz77LAsWLOC73/0uS5Ys4brrruNjH/sYjzzyCKeccgqPP/54l9rFUUcdRUVFBW+99Rbz5s3jJz/5CQDuzq9//WuOO+64vJ2bahZAOhr8z8i0q4NbRPbf2rVrd04j/qtf/YrTTjsNgMrKShobG3d2VGezWdatW8dZZ53FD3/4Q7Zv305jYyMrVqxg+vTpXHvttcyePZt33313t8+4/PLL+eEPf0h9fT3Tp08H4LzzzuNHP/rRzv6Q119//aCfm5IFu5JFtl3jLERk/02ePJmf//znzJgxg7q6Or785S/zhS98genTp3PRRRcxZ84cIHjS3ZVXXsn06dM5/vjj+drXvsaQIUO4/fbbmTZtGjNnzqSoqIgLLrhgt8+4+OKLue+++3ZOXw5w/fXX09HRwYwZM5g2bRrXX3/9QT83NUMB2Z3JQjULEdl/kUiEu+66q0vZ9773Pb73ve/ttu1zzz23W9mPfvSj3crGjh3L4sWLdy4PHz6cdDrdZZuioqKdTVL5opoFu5KFK1mIiPRIyQLIxouDNxqUJyL7qXsN4EijZAF4LLzbQHNDiRzWjpQnf+bDgV4bJQuAWFizSGvWWZHDVSqVora2VgmjB+5ObW3tboP89kVeO7jN7HzgX4Eo8FN3v6Xb+tOB24EZwOXu/kDOuquAvw8Xv+fuP89boIngAlpaNQuRw9Xo0aOprq6mpqam0KEcklKpFKNHj97v/fOWLMwsCtwJnAtUA6+a2QJ3fydns7XAZ4Fvdtu3HPgHYDbgwKJw3215iTXss4goWYgctuLxOOPGjSt0GEesfDZDnQQsd/eV7t4O3AdcmLuBu69297eAbLd9zwP+4O51YYL4A3B+vgK1RJAsohklCxGRnuQzWRwFrMtZrg7LDtq+Zna1mS00s4UHUvXsrFlEM237fQwRkSNZPpOF9VDW156nPu3r7nPdfba7z66qqtqn4HLFkp3JQh3cIiI9yWeyqAbG5CyPBjb0w777LBomi1hWyUJEpCf5TBavAsea2TgzSwCXAwv6uO+jwEfNbKiZDQU+GpblRTQRzPkeV7IQEelR3pKFu6eBawh+5JcC8919iZndbGafBDCzOWZWDVwC/MTMloT71gHfJUg4rwI3h2V5kUgmybgR8zRk0r3vICIywOR1nIW7PwI80q3shpz3rxI0MfW07z3APfmMr1MqEaOFJKW0QroFomX98bEiIocNjeAGUvEIrQSPVtWUHyIiu1OyAJKxaE6y0GSCIiLdKVkQ1CxaPBksqGYhIrIbJQuCmkWLmqFERPZIyYKwZoFqFiIie6JkQVCzaPN4sKBkISKyGyULIBWP5tQs1MEtItKdkgWdzVDqsxAR2RMlC8IO7s67ofRMCxGR3ShZAPGo0RbWLDJtaoYSEelOyQIwMzqiwaNV021NBY5GROTQo2QRSkeCZijVLEREdqdkEUpHgppFpl3JQkSkOyWLUCZWBEBWyUJEZDdKFqFMWLNQshAR2Z2SRSgbC5IF7bp1VkSkOyWLUDZshnKN4BYR2Y2SRad4kCysQ8/hFhHpTski5GHNwtKqWYiIdKdkEbLOmkVaNQsRke6ULEKWKAYgormhRER2o2QRinQmi4xqFiIi3SlZhCwRNENFlSxERHajZBGKhTWLmJKFiMhulCxCsWSYLLwdspkCRyMicmhRsggl41GaOx+ApKfliYh0oWQRSsajux6tqttnRUS6ULIIpWIRWuisWWhgnohIrrwmCzM738yWmdlyM7uuh/VJM5sXrn/ZzMaG5XEz+7mZvW1mS83sW/mMEyAVj9Lm8WBBzVAiIl3kLVmYWRS4E7gAmAJcYWZTum32eWCbu08EbgN+EJZfAiTdfTpwIvDFzkSSL0nVLERE9iifNYuTgOXuvtLd24H7gAu7bXMh8PPw/QPAOWZmgAMlZhYDioB2YEceYyWV22ehyQRFRLrIZ7I4CliXs1wdlvW4jbungXqggiBxNAEbgbXAre5el8dYg2ThqlmIiPQkn8nCeijzPm5zEpABRgHjgG+Y2fjdPsDsajNbaGYLa2pqDijYZCxC686ahfosRERy5TNZVANjcpZHAxv2tE3Y5DQYqAP+DPi9u3e4+xbgeWB29w9w97nuPtvdZ1dVVR1QsKl4VMlCRGQP8pksXgWONbNxZpYALgcWdNtmAXBV+P5i4El3d4Kmp7MtUAKcArybx1hJxSNqhhIR2YO8JYuwD+Ia4FFgKTDf3ZeY2c1m9slws7uBCjNbDnwd6Ly99k6gFFhMkHR+5u5v5StW6NbBrUF5IiJdxPJ5cHd/BHikW9kNOe9bCW6T7b5fY0/l+RT0WahmISLSE43gDgV9FhqUJyLSEyWLUDK2q8/C21WzEBHJpWQRMjM6IkGyyLSrZiEikmuvycLMomZ2b38FU2jpaAqATHtTgSMRETm07DVZuHsGqApvfT3iZaLBo1WzbWqGEhHJ1Ze7oVYDz5vZAoIpOABw93/JV1CF4tEUZNRnISLSXV+SxYbwFQHK8htOYWViKWgH191QIiJd9Jos3P0mADMrCxa9Me9RFYjHgudw69ZZEZGuer0bysymmdnrBKOpl5jZIjObmv/Q+p/Hgw5uJQsRka76cuvsXODr7n6Mux8DfAP4j/yGVSDxoGZhaSULEZFcfUkWJe7+VOeCuz8NlOQtogKysGYRUbIQEemiLx3cK83seuCX4fKVwKr8hVRAiSAHKlmIiHTVl5rF54Aq4MHwVQn8n3wGVSiRsBkqkmkrcCQiIoeWvdYszCwKfNvdv9pP8RRUNBE0Q8WyrZDNQkSzoYiIQN9GcJ/YT7EUXDIep9XDmWf1TAsRkZ360mfxejh6+366juB+MG9RFUgqHqGFJCk6gmdaJIoLHZKIyCGhL8miHKgFzs4pc4L+iyNKKh6l3ksYao3Qsg1KKgsdkojIIaEvfRZvuftt/RRPQSVjEeooYyyboWkrVB5b6JBERA4Jfemz+OTetjmSpOJRan1QsNC8tbDBiIgcQvrSDPWCmf0bMI+ufRav5S2qAknFI9R1JosmJQsRkU59SRYfCv97c06Z07UP44iQjEXZ2DmxrmoWIiI79WXW2bP6I5BDQSoe2dUM1VRb2GBERA4hfZl1driZ3W1mvwuXp5jZ5/MfWv9LxqPUuWoWIiLd9WWI8n8CjwKjwuX3gL/NV0CFlIpFqUN9FiIi3fUlWVS6+3wgC+DuaSCT16gKJJnbDNWsZigRkU59SRZNZlZB0KmNmZ0C1Oc1qgJJxXKboZQsREQ69eVuqK8DC4AJZvY8wQy0F+c1qgJJxSPU5jZDuYNZYYMSETkE9OVuqNfM7AzgOMCAZe7ekffICiAZj9JKkhaSFGXaoL0RkmWFDktEpOD6UrPo7KdYkudYCi4VC1rltjGIImqC2oWShYhIn/os9puZnW9my8xsuZld18P6pJnNC9e/bGZjc9bNMLMXzWyJmb1tZql8xgrBdB8Ateq3EBHpIm/JIpyE8E7gAmAKcIWZTem22eeBbe4+EbgN+EG4bwy4F/iSu08FzgTy3vSVDGsWtdkwWej2WRERoG+D8p7oS1kPTgKWu/tKd28H7gMu7LbNhcDPw/cPAOeYmQEfJZjt9k0Ad68NJzXMq1g0Qixiuzq5NTBPRATYS7Iws5SZlQOVZjbUzMrD11h2DdDbm6OAdTnL1WFZj9uE/SL1QAXwAcDN7FEze83M/m4PMV5tZgvNbGFNTU0fQupdKncUt2oWIiLA3ju4v0gwUnsUsIjgTiiAHQTNS73p6Z5T7+M2MeA0YA7QDDxhZovcvUuNxt3nAnMBZs+e3f3Y+6UoEaWuWTULEZFce6xZuPu/uvs44JvuPt7dx4Wvme7+b304djUwJmd5NLBhT9uE/RSDgbqw/Bl33+ruzcAjwAl9PqsDUFGSoLZz5llNJigiAvStg3uTmZUBmNnfm9mDZtaXH+5XgWPNbJyZJYDLCQb35VoAXBW+vxh40t2dYC6qGWZWHCaRM4B3+vCZB6yiNLHrmRaqWYiIAH1LFte7e4OZnQacR9Ah/ePedgr7IK4h+OFfCsx39yVmdrOZdT59726gwsyWE4wUvy7cdxvwLwQJ5w3gNXf/7b6d2v6pKEmqz0JEpJu+DMrrvAvpY8CP3f1hM7uxLwd390cImpByy27Ied8KXLKHfe8luH22X1WUJnhTd0OJiHTRl5rFejP7CXAp8IiZJfu432GpsjSZ82hV9VmIiEDffvQvJWhKOt/dtwPlwP/Na1QFVFmaoIEi0haDjiboaCl0SCIiBddrsgjvRtpCcCsrQBp4P59BFVJFSRIwGiKDgwL1W4iI9GkE9z8A1wLfCoviFKAvob9UlCaAYDJBQP0WIiL0rRnqT4FPAk0A7r4BOGKnYq0sTQLsemKe+i1ERPqULNrDsQ+dT8oryW9IhdVZs9iUDk9TNQsRkT4li/nh3VBDzOwLwOPAT/MbVuEUJ2IUxaNs1cyzIiI79eVJebea2bkEc0IdB9zg7n/Ie2QFVFGaoHaH+ixERDr1mizM7Afufi3whx7KjkgVpUnqduQ8i1tEZIDrSzPUuT2UXXCwAzmUVJYk9LQ8EZEce6xZmNmXgb8CxpvZWzmryoDn8x1YIVWUJljlqlmIiHTaWzPUfwO/A75POMFfqMHd6/IaVYFVliZZ1Hl3sPosRET2nCzcvZ7gyXVX9F84h4aK0qTGWYiI5DhiJwQ8EJWlCeopIUME2uoh3V7okERECkrJogcVJUmcCI2RzttnVbsQkYFNyaIHmh9KRKQrJYsedCYLjeIWEQkoWfSgvDhIFlsynfNDqRlKRAY2JYsexKIRhhbHc+6IUs1CRAY2JYs9qChNUtvZZ9FUU9hgREQKTMliDypKEmz0imBhx/rCBiMiUmBKFntQWZZkQ2eyqK8ubDAiIgWmZLEHlSUJJQsRkZCSxR5UlObULHash2y2sAGJiBSQksUeVJQmaCFFU3QwZNrVyS0iA5qSxR5UlCQB2BqtCgrUFCUiA5iSxR5UhqO4N1MZFOxQshCRgUvJYg8qSoOaxbpseVCgmoWIDGB5TRZmdr6ZLTOz5WZ2XQ/rk2Y2L1z/spmN7bb+aDNrNLNv5jPOnnTOD7WqfWhQoGQhIgNY3pKFmUWBOwme1z0FuMLMpnTb7PPANnefCNwG/KDb+tsIntbX78qSMRLRCKvTnTWLdYUIQ0TkkJDPmsVJwHJ3X+nu7cB9wIXdtrkQ+Hn4/gHgHDMzADO7CFgJLMljjHtkZlSUaqyFiAjkN1kcBeT+OV4dlvW4jbunCR7jWmFmJcC1wE17+wAzu9rMFprZwpqag39ra2XuWIt6TfkhIgNXPpOF9VDmfdzmJuA2d2/c2we4+1x3n+3us6uqqvYzzD2rKE2whaFkLQpNW6Cj9aB/hojI4SCWx2NXA2NylkcDG/awTbWZxYDBQB1wMnCxmf0QGAJkzazV3f8tj/HupqIkSZYILanhlLRsCEZyV0zozxBERA4J+axZvAoca2bjzCwBXA4s6LbNAuCq8P3FwJMe+LC7j3X3scDtwD/1d6KAXWMt6hPDgwL1W4jIAJW3ZBH2QVwDPAosBea7+xIzu9nMPhludjdBH8Vy4OvAbrfXFtKIwSkANptGcYvIwJbPZijc/RHgkW5lN+S8bwUu6eUYN+YluD6YUFUKwOqOoRwPShYiMmBpBPdeTBgWJIt3W8In5mnKDxEZoJQs9mLkoBSpeIT3WocEBapZiMgApWSxF5GIMb6yVAPzRGTAU7LoxfiqEjZ4OPNsfTV496EiIiJHPiWLXkyoKqWBYtqiJdDRDC3bCh2SiEi/U7LoRWcn99aIbp8VkYFLyaIX4ytLAFivfgsRGcCULHoxvipIFivadEeUiAxcSha9KE7EGDU4xbpsZ81Cz7UQkYFHyaIPJgwr7XpHlIjIAKNk0QcTqkrZ2NlnsUPPtRCRgUfJog/GV5WwnjBZ1K3SWAsRGXCULPpgQlXQDFUXKQ8egrT2xUKHJCLSr5Qs+mB8VQlZIjzkZwYFr/2ioPGIiPQ3JYs+GDEoRXEiys9bPxwULPkfaK0vbFAiIv1IyaIPzIzxVSWs9eHsGPFBSLfA2w8UOiwRkX6jZNFHnQ9CemfERUGBmqJEZABRsuijzmTxbPwUSA2BjW/AxjcLHJWISP9Qsuijzmk/3tuahhmXBYWv/bKAEYmI9B8liz7qrFmsrGmEE/4iKHxrPuzYWMCoRET6R6zQARwuxlWWEI0Yq2ubaK34MKlRJ8CG1+BfJkHlcTDuwzA2fJVUFDpcEZGDSsmij1LxKBOqSnhvcyPLNjUw86Ifw6PfgjUvwtZlwevVnwYbD5sKE86CGZfCiBlgVtjgRUQOkJLFPpgychDvbW7knY07mHnSJPjMQ5Buh/WLYPUfYdWzsO4V2LIkeL34bzBsCsy8AiZ/HMrHF/oURET2i5LFPpgyahD/88YGlmzIGZAXS8AxHwxeZ/wddLRC9auw9H/h7fthyzvwh+uD19CxMOEcmPJJGHeGahwicthQB/c+mDJyMADvbNix543iqaD/4k9+CN9YBpf/N0z9VHC77bbVsPBu+MWF8J8fD2ohIiKHAdUs9sGUUYMAeHdTA5msE430UjOIJWDSx4JXNgMbXof3HoVX/wPWPAd3nwsfOB8mnA2VH4Cq46B1B2x6Gza/De3NcMJnYOTMfjg7EZE9U7LYB+UlCUYOTrGxvpU1tU2MD2+n7ZNIFEbPDl4f/Gt44Ufw0r/De78PXnvy6n8ECeXD34Qxcw78JERE9oOSxT6aMnIQG+tbWbJhx74li1xFQ+Cc6+Gkq2HJg7BlKWx9L3glSoI7qIZPCyYrXPSfuxLKxI/AOTeopiEi/S6vycLMzgf+FYgCP3X3W7qtTwK/AE4EaoHL3H21mZ0L3AIkgHbg/7r7k/mMta+mjBrEE+9u4Z2NO/jEzFEHdrCy4XDKl/e+zenfDGogL8+F5Y8Hr2mfhrO+AxUTDuzzRUT6KG8d3GYWBe4ELgCmAFeY2ZRum30e2ObuE4HbgB+E5VuBT7j7dOAq4JCZV2PKyKDfYq+d3AdTSWVQm/ibN+GD10A0CYt/DT/+UJA4RET6QT7vhjoJWO7uK929HbgPuLDbNhcCPw/fPwCcY2bm7q+7+4awfAmQCmshBdfZyf3Oxn5KFp1KKuC8f4SvvgZT/xTSrfCrK2DZ7/o3DhEZkPKZLI4C1uUsV4dlPW7j7mmgHug+V8angdfdva37B5jZ1Wa20MwW1tTUHLTA92bM0GLKkjFqGtrY0tDaL5/ZxeDRcPHP4KQvQqYd5l0J7zzc/3GIyICSz2TR032lvi/bmNlUgqapL/b0Ae4+191nu/vsqqqq/Q50X0QixuT+borqzgwu+AF86CuQTcP9/wee+C60bCtMPCJyxMtnsqgGxuQsjwY27GkbM4sBg4G6cHk08BDwF+6+Io9x7rOCNUXlMoNzvwun/x14Bv54K9w+E56+RY98FZGDLp/J4lXgWDMbZ2YJ4HJgQbdtFhB0YANcDDzp7m5mQ4DfAt9y9+fzGON+6fdO7j0xg7O/A597NJg+pK0env4+/PMkePBqWPFkMBiwO3dY9UdY+DNI79a6JyKym7zdOuvuaTO7BniU4NbZe9x9iZndDCx09wXA3cAvzWw5QY3i8nD3a4CJwPVmdn1Y9lF335KvePfFIVGzyHX0KXDVAlj9HDzzg2BCw7fmBa+yUcFcVJMZDKqFAAAViUlEQVQ/GWy39kV46vvBCHIIZsr91FwYPrWw5yAihzRz796NcHiaPXu2L1y4sF8+qy2dYeoNj5JxZ/GN51GSPMTGNtatChLFm78K5qPqlBwEbWGCSw2B1CDYvhaiiV3jNjYths2Lob0RSoZB6TAoGwmjZsGoEyBRXJBTEpH8MLNF7j67t+0OsV+5w0MyFmXisFLe3dTAu5t2cOIx5YUOqavycXDmdXDGtVC9EJYuCF7bVkNyMHzoGjj5i2BReOw7wSjxx/+h9+NGYsHo8UkfgzlfCJKNiAwIqlnsp289+Da/emUtV5x0NN//1PR++9z95g7bVkFJFSTLuq5771F49lZIlgbTjIyYDkXl0LQFGrcEtY/qV2DzEvBssE/RUPjQV4MpS8ygcTM010HFxGA6k1z11cF4kJKq4NhDx0FEEx6LHAr6WrNQsthPy7c08tHbniFixpPfOJOjKwZA80zrDljzPDx3O6x7KSizaHA3VqdoEib9SfDAp9QQePmuYBxI7jbxEhg2OZhlt/IDQX/J2NMgXtS/5yMiShb94Rvz3+TXr1Xz6RNG88+XDqDJ/dxh5VPw1D8FD3qKJoO+jWRZMCli9+E0kVgwc266LegPadi4+zETpUHz1rSLYeSMoB8lGodYEUTVWiqSL0oW/WBtbTNn//PTZN157GtnMHHYfs5Cezhrbw5qBJ1P/du+Dt6eD2/Og9btMOvPgv6NwTmD95u2Qs274eu9oIlrw+s9Hz+agJGz4OiTYfQcyHQEzVo71kN7UzBLb6IkSFTDpgSd8GXD83/eIkcIJYt+8u2H3ua/X17Lx2eM5N/+7IR+//wjRu0KWPwgLH0YGjZDtiNIDO2N+36sQUfBqOPD/pdpQT9Kpj1ILh3NUHEsDD3m4J+DyGFIyaKfbKxv4Yz/9zTt6Sy/+5sP75wKRA6Slu3BHV3rXgpqH/FiGDwmqKkky4KaTUdT0Lm+8U3Y8Aa0N/R+3GFTyB57HumqqSQa1wed//XVQS2ldHhw23DFeDYMOZHnN0VZvL6e1o4sHZksHVlnUCrG2IoSjkvWMiGxjVEzzsKi8X06tdaODEs37uCt6nq2NLTysemjdo7h6fPlac+wcE0dL66oZcmGHRTFowwuijOkOM7RFcXMGVvOxKpSIr091VEGLCWLfnTT/y7hZ8+vZlxlCdd/fDJnHTcMM/3j7C+tHRmWb2lkXGUJJXGDre8Hj6bd9BbZTW+TqVtLLFmEJUohGsc3vI7tQ43l/exRvJydxGofwQavYLMPZVZkBZ+IvsisSDATTTXDeWbYlTDrCmYeM4yJw0pJxaNA8AfFmwufJ770QWjdQTbdTjbdzrq2Yt7MjONNn8BaHwYYp3+gii+dPp5BRXEWrdnGm6s2Edm+ig8nVzAju5ThLSvYPGgajxRfxOO1Q1m8vp6OTNd/w3HSFNFKE0VkiDIoFWPSiEE0d6TZ3tzBjpYOhpYkOLq8mLEVJZSXJGjpyNDUlqalI0PUjHgsQiIaoS2dZUdLB9tb2ulIO6dMqOD8qSOYPLKsz9/xTNZ5YcVWHnxtPS+trGXqqEGcP20k504ezuDiOK0dGTZsb6F6Wwtr6ppZs7WJ9dtbGF9Vwp8ePzqvzbsNrR38fvEmWjoyXHT8UQxKdU347ekstU1tNLVlaG5P09qRJRY1EtEI8WiEwUVxKkoTxKOH7919Shb9qLaxjUt+8iIra5oA+NCECq49fxIzRg8eUEmjtSNDc3sm+Os7k2VLQxtLN+5g6cYdrN7azMjBKSaPHMSUUYMYUhynvrmD7S0dtLRnGFIcp6osSVVpklQiuvOYiWhk549uJ3dnY30rz72/lceXbua55Vtpbs8QixgzRg/mgxMqyGRh0Zo63qyupz2dJR41xlaUMHpoEYtWbmF6ZjEfibzGUbaVaq9itQ9nvVdSTBuVVs8w285UW83s6HsUs+cpUVotRZ2XMYpg1uMNXs4jmZN5m4lsHzKNymwtH2+Yx1nRN/d67doiRezIJGkjRtqjFFkbQ2giaR173OeZzAxe8imcUFLLlNgGqjrWE+toIuLBPi2RYl5hGn9om8or2Um0kgAgQpaRVsfRtoVjbDMjrI4hNDLEGimhlXVexVI/hqXZo3nbx7EuTGS5ZpSnmXr0MKrKyxk5pIhkLMKyTQ28s3EH729uJBoxhhTHGVwUZ2VNE5t27D5DcyxiDCqKU9fUvtdrM3PMEC6YNoKjhhRRVZZkaHGCmoY2Vtc2saa2iUQswqkTK5l9TDmJ2K4f7bZ0hm1NHdS3dLC9uZ2m9vTOdS3tWR57ZxOPLtlEa0dwO/jQ4jh/fdZErjzlGFbWNHHfq2t56PX1NLSmd4upu6HFcYYPSjFhWCkTq0qZOKyUY4eXMq6yhGRs1/e3tSPDxvpWNm5vYf32FjbWtxKPRhhbUczRFcWMHlpMaTJGdB9rgs3taYoT+3cjiJJFP2tLZ/jli2v40ZPLqW8J/rGOryrhgmkj+OiUEQwtTtAe/ohm93LNoxEjHg3+qitKRKkoSexMOO7Oss0N/PG9razf3kJxIkpJMkZZKsaZHxjW6+279c0dFCWiXf5B9bb975ds5IUVtaRz/notTkSDH/ayJNGI8XZ1PW9V1/P+lgayefg6jR5axMRhpYytKGFdXTNvVteztbHrD/iY8iLWb2vp8fMrS5O7bf/hYyv53KnjOGlcOa+squPZ92t4bc02yksSTBwW/GOfMnIwU4aniG58Hda9sqtjvb4ahowJnlh47Hl4NMGWl+eRfOGfGdLY85yXbZZk5VEXYlWTSKWSFKVSDOnYQnLzG7DhNWjqeYr9jMVpLx7O+pKpvGHH8VrLCD7KS5za+Cjx7B6SmEWD5rS2gzMdTWuykoZhJ9JQNpGm9YuprF/CSLaSdWOND+MdP4blPpo6L2O7l1BPCemc8b7NniQzZCxnnTCFj0wZwRtralj8xiu0b3gby2bZEqkgU3oUicHDGT80zrhBzqiSLK+sz3Dfsg7q2/r2pSpORJk1ZgiNbWk2bGsh1byeBB1s91LqKSFDtMf9Th1bypB0LcvXb6aEVsoTaVo7glu9DeeYolbGR7dwjG2i3HdQE6lkvY1gHcPJtjdR0VbN0baF4baNOGnipImRpc5L2UQFTamRpOMldLQ0QkczRbTtTL2GEyUb7GdpYgSfGzUjFoEoWWLeQdTTxCxLKh4lFY+QiEVpy0Zo6ICGdqMlG+WCv3+ISGLfbz9XsiiQ+uYO7nx6OfcvXMe25j3/VdhXxYkoR5cXM3Jwinc27mDzjj3/lXvqxAoun3M0HxhexpraJtbUNrOqtonlWxpZsaWR2qZ2ErEIU0YOYubowRw1tIjNO9rYsL2FzTtaKUnGdv51v6KmiWffq6E9k+1zrNGIUZaK7Ux2g4riTB5RxuSRgxhfVcL67S0s3biDdzbsoKk9w9DiOIOLEhQlomxvbqemoY2ahjba07s+s6UjQ7qHDDC4KM6Jxwzl7EnDOHvSMEYNKaKhtYNXV9fx8so6ohFj9tihnHD0UIYUJ2huT7OypolVW5uYNKKMY4eX7XbMA5bNwupnYe3LZKoXka1ehJsRmfOXxE7+QvAAq564B9PLZ9rDVwfEUsHAx9w7zXI118Eb/wX166FyIlRNCgdEDg3uIDML7kxb8SSseCJolvOc/5elw4PBkeXjg2ekFJfv+rzaFWEz3tuwfhG01O328ZlYMZZpJ+K9/9W9U6I0+Nzta4MbGPrALUJrahj1Xkok00I800w820Z7pIj2+CAyySE0RspY1RRnXUuSNFEm21pmRFZQbl2bGhsjZWyNjWBLbCS1sRGMSzYwPruKRN37XccBHaZq/mYdVUP3vc9UyaLA0pksr6yq43eLN/Hc8q2ks1nikaCdc2+djdms05HJ0p7J0tCa3llL6VRVluT0Y6uYPLKM1o4MTe0Zqre18NiSTbSl9/7DnopHdla5+yJi8MEJFZw/bSRDi4O2XHdobEtT09DG1sY2WjsyTB45iBmjhzB11KDdmowOVEcmy5raZpZvaWRNbRMjhxQxc/Rgji4vHlBNfAXjDrXLYe1LUPs+VB4HR50QDKbMZmDrsiCp1K0MbkZo2Ra8cn98W+uD9blT5w8dF9ypFksFCW9HdXBLdbwoqBXFi4PjNW5m98fg9DH04kosNTiIp3V712TZhQV30CXL8EQJLSRIJuJEO79fyUFBUi0fF9z4sGN9cD7bVgdxlodJd9CocFxQHCJRaKqho24t9ZtWkWlrpKR0MMWlg4kkisFyaveRaDiuKAGRKFk3OjJZ2tJZLBojGk8QiydpzRhbGtrYvKOV2sY2BicjjCiJMLwkwqCEE5n26f2aGUHJ4ghR39zBmromqre1MK6yhEkjeu5YrG/pYMEb63ngtfXsaOkIOy+LObqiJGg/HVbKyMEpdrSmWby+njert7NlRxsjB6cYOaSI4WVJmtsz1DQGf90PKopz3tThDCtLFeCs5YjUXBcMyBxy9O5TzuxJuh0aNgSJJlEa/DjHU8FdcJ2JqbUzSW0Pbo2umhQktMFjdtXKsllorg1+4OtWBrWb0ioYPj2YTWAAT5CpZCEiIr3qa7I4fO/3EhGRfqNkISIivVKyEBGRXilZiIhIr5QsRESkV0oWIiLSKyULERHplZKFiIj06ogZlGdmNcCafdilEtiap3AOV7omXel6dKXrsbsj4Zoc4+5VvW10xCSLfWVmC/syanEg0TXpStejK12P3Q2ka6JmKBER6ZWShYiI9GogJ4u5hQ7gEKRr0pWuR1e6HrsbMNdkwPZZiIhI3w3kmoWIiPSRkoWIiPRqQCYLMzvfzJaZ2XIzu67Q8fQHMxtjZk+Z2VIzW2JmfxOWl5vZH8zs/fC/Q8NyM7M7wmv0lpmdUNgzyA8zi5rZ62b2m3B5nJm9HF6PeWaWCMuT4fLycP3YQsadL2Y2xMweMLN3w+/KBwfyd8TMvhb+e1lsZr8ys9RA/Y4MuGRhZlHgTuACYApwhZlNKWxU/SINfMPdJwOnAH8dnvd1wBPufizwRLgMwfU5NnxdDfy4/0PuF38DLM1Z/gFwW3g9tgGfD8s/D2xz94nAbeF2R6J/BX7v7pOAmQTXZkB+R8zsKOCrwGx3nwZEgcsZqN8Rdx9QL+CDwKM5y98CvlXouApwHR4GzgWWASPDspHAsvD9T4Arcrbfud2R8gJGE/z4nQ38BjCC0bix7t8V4FHgg+H7WLidFfocDvL1GASs6n5eA/U7AhwFrAPKw//nvwHOG6jfkQFXs2DXF6BTdVg2YITV4+OBl4Hh7r4RIPzvsHCzgXCdbgf+DsiGyxXAdndPh8u557zzeoTr68PtjyTjgRrgZ2HT3E/NrIQB+h1x9/XArcBaYCPB//NFDNDvyEBMFtZD2YC5f9jMSoFfA3/r7jv2tmkPZUfMdTKzjwNb3H1RbnEPm3of1h0pYsAJwI/d/XigiV1NTj05oq9J2DdzITAOGAWUEDS9dTcgviMDMVlUA2NylkcDGwoUS78yszhBovgvd38wLN5sZiPD9SOBLWH5kX6dTgU+aWargfsImqJuB4aYWSzcJvecd16PcP1goK4/A+4H1UC1u78cLj9AkDwG6nfkI8Aqd69x9w7gQeBDDNDvyEBMFq8Cx4Z3NCQIOqwWFDimvDMzA+4Glrr7v+SsWgBcFb6/iqAvo7P8L8I7Xk4B6jubIo4E7v4tdx/t7mMJvgNPuvufA08BF4ebdb8endfp4nD7I+avRgB33wSsM7PjwqJzgHcYoN8RguanU8ysOPz303k9BuZ3pNCdJoV4AX8CvAesAL5T6Hj66ZxPI6gSvwW8Eb7+hKBN9Qng/fC/5eH2RnDX2ArgbYI7Qgp+Hnm6NmcCvwnfjwdeAZYD9wPJsDwVLi8P148vdNx5uhazgIXh9+R/gKED+TsC3AS8CywGfgkkB+p3RNN9iIhIrwZiM5SIiOwjJQsREemVkoWIiPRKyUJERHqlZCEiIr1SspABycyeNrPZ/fA5Xw1nb/2vbuWzzeyO/Tzm35pZ8cGJUKRvdOusDEhm9jTwTXdfuB/7xnzX3EC9bfsucIG7r9rXz9nLMVcTjGnYerCOKdIb1SzkkGVmY8O/yv8jfKbAY2ZWFK7bWTMws8rwBxQz+6yZ/Y+Z/a+ZrTKza8zs6+HEeC+ZWXnOR1xpZi+Ezyo4Kdy/xMzuMbNXw30uzDnu/Wb2v8BjPcT69fA4i83sb8OyuwgGcC0ws6912/5M2/UMjRvDz3zazFaa2VdzYvmtmb0ZHveycN0o4Ckzeyrc7sdmtjC8RjflfMZqM7vJzF4zs7fNbFJYXmpmPwvL3jKzT4flHzWzF8Pt7w/nEcPMbjGzd8Jtbz2g/6ly+Cr0qEC99NrTCxhL8ByOWeHyfODK8P3ThCOGgUpgdfj+swQjaMuAKoKZP78UrruNYALFzv3/I3x/OrA4fP9POZ8xhGCkf0l43GrC0cvd4jyRYARzCVAKLAGOD9etBip72OdMdo0avxF4gWB0cCVQC8SBT3fGGG43uKdjsmtEdTQ8rxk5230lfP9XwE/D9z8Abs/Zf2j4uc8CJWHZtcANBNNzL2NXK8SQQn8v9CrMSzULOdStcvc3wveLCBJIb55y9wZ3ryFIFv8blr/dbf9fAbj7s8AgMxsCfBS4zszeIPjhTQFHh9v/wd17mhjuNOAhd29y90aCCec+3LfT2+m37t7mQdPSFmB4GO9HzOwHZvZhd6/fw76XmtlrwOvAVIKHenXqnDAy99p9hGCaDgDcfRvBA7GmAM+H534VcAywA2gFfmpmnwKa9/G85AgR630TkYJqy3mfAYrC92l2NaOm9rJPNmc5S9fvfPcOOyeY7+jT7r4sd4WZnUwwZXdPepqael91P8+Yu79nZicSzOH1fTN7zN1v7hbXOOCbwBx332Zm/0nX69GWe8yceLufuxEkwyu6BxY20Z1DMOHiNQQz9MoAo5qFHK5WEzT/wK4ZQPfVZQBmdhrBjKn1BE87+0o4yyhmdnwfjvMscFE4O2kJ8KfAH/czpp3MbBTQ7O73EjyEp/MZ1w0EzWwQPN2uCag3s+H0/LyF7h4j+NHv/JyhwEvAqWY2MSwrNrMPhP0Wg939EeBvCSYalAFINQs5XN0KzDezzwBP7ucxtpnZCwQ/uJ8Ly75L8FyLt8KEsRr4+N4O4u6vhX/RvxIW/dTdX9/PmHJNB/6fmWWBDuDLYflc4HdmttHdzzKz1wn6SVYCz/fhuN8D7jSzxQQ1jpvc/UEz+yzwKzNLhtv9PUFietjMUgS1j6/1dEA58unWWRER6ZWaoUREpFdKFiIi0islCxER6ZWShYiI9ErJQkREeqVkISIivVKyEBGRXv1/PtQqxsql4d4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(part2_test_error['n'], part2_test_error['active linear'], linewidth=2.0, label='active')\n",
    "plt.plot(part2_test_error['n'], part2_test_error['passive linear'], linewidth = 2.0, label='passive')\n",
    "plt.title('linear')\n",
    "plt.xlabel('number of instances')\n",
    "plt.ylabel('test error')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(part2_test_error['n'], part2_test_error['active L1'], linewidth=2.0, label='active')\n",
    "plt.plot(part2_test_error['n'], part2_test_error['passive L1'], linewidth = 2.0, label='passive')\n",
    "plt.title('L1')\n",
    "plt.xlabel('number of instances')\n",
    "plt.ylabel('test error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In both linear SVM and L1 SVM, active learners test errors are reduced faster than passive learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
